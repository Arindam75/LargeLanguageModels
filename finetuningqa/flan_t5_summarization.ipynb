{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30c648d3-04bf-45db-936a-ac142eb0f81b",
   "metadata": {},
   "source": [
    "[__Source__](https://www.philschmid.de/fine-tune-flan-t5)\n",
    "\n",
    "Model trained on Vastai\n",
    "\n",
    "![title](assets/vastai_flant5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20620889-9d50-42ad-b07d-1066228e8c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM\n",
    "from random import randrange\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77ddc613-c035-4747-9fb7-27448902dbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\arind\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_API_TOKEN\")\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb52f4b4-aa8e-4f8b-93cf-5ef9cd8fd349",
   "metadata": {},
   "source": [
    "### Load Data for pre-processing\n",
    "\n",
    "The dataset used in this exercise already has the train, test validation splits. <br> Each sample has a __dialogue__ followed by a __summary__. For pre-processing ( padding, tokenization etc ) we need to figure out the maximum dialogue length and summary length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "492428c6-3aff-46f0-8319-7b6e45deafca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install py7zr evaluate nltk absl-py rouge_score tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ebff729-f9c6-4c09-a81c-ce5a1f653215",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a84595b-28b4-4994-8218-cfe322bc339b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test', 'validation'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Acquire the training data from Hugging Face\n",
    "data_id= \"samsum\"\n",
    "dataset = load_dataset(data_id, trust_remote_code=True)\n",
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a2ae55-419d-407f-b29d-e336d7041824",
   "metadata": {},
   "source": [
    "Since the data has three partitions already, we can check the size of each partition and also see a random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9e32ecc-9f03-4047-adac-a22ec3127809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 14732\n",
      "Length of val dataset: 818\n",
      "Length of test dataset: 819\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of train dataset: {len(dataset['train'])}\")\n",
    "print(f\"Length of val dataset: {len(dataset['validation'])}\")\n",
    "print(f\"Length of test dataset: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e78f2f33-1dc2-49a0-b540-bfa0ebc78052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialogue: \n",
      "Noe: hey girl, is everything good with you?\n",
      "Laila: hii! Yes I am great! What about you? \n",
      "Noe: good good! So how is your new job? Apartment? Life! Tell me EVERYTHING! üëÄ\n",
      "Laila: oh I freaking love it here in Amsterdam! It is less stressful than Paris, but you still have a lot of career opportunities with all the big brands being based here...\n",
      "Noe: that sounds great, are you satisfied with your new job?\n",
      "Laila: humm ‚Ä¶ I am still discovering all its aspects, and getting to know my boss better (hope she doesn‚Äôt turn out a bitch like the last one) üòÇ‚Ä¶ but so far so good with the colleagues.\n",
      "Laila: and we have people from all over the world!\n",
      "Noe: hahah you cracked me up! She was a real bitch though! Thank God I left at the same time as you, otherwise I would have gone crazy.\n",
      "Laila: tell me me about it!  üò∑\n",
      "Noe: and how is yoru roommate? do you get along?\n",
      "Laila: yes, perfectly! I am so lucky this time thank God. She is German, I also get to practice it with her üòä\n",
      "Laila: when are you coming to visit though?\n",
      "Noe: soon! soon! I am just waiting for my contact to be signed, then I can make travel plans.\n",
      "Laila: ok, let me know ‚ù§Ô∏è\n",
      "---------------\n",
      "summary: \n",
      "Laila loves Amsterdam as it is less stressful than Paris. She is still learning about her new job and getting to know her boss. She hopes she will be better than the previous one, who was \"a real bitch\" in Noe's opinion. Laila also likes her German roommate. Noe will visit her soon.\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "sample = dataset['train'][randrange(len(dataset[\"train\"]))]\n",
    "print(f\"dialogue: \\n{sample['dialogue']}\\n---------------\")\n",
    "print(f\"summary: \\n{sample['summary']}\\n---------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4dea72-70c6-4996-9a10-57b0b92d49db",
   "metadata": {},
   "source": [
    "We concatenate the train and test set to figure out the max source(dialogue) length and max target(summary) length. This will be used for padding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f4db924-06dd-4c67-82dc-caeab57bc719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798b6cb974884c7e98104f3e16540a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target length: 95\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"dialogue\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"summary\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
    "max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n",
    "print(f\"Max target length: {max_target_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e3b71e-fcbc-4c6a-93ea-87bed782f241",
   "metadata": {},
   "source": [
    "### Pre-Processing\n",
    "\n",
    "Now that we have the max length for the dialogues and summary, we take the following steps.\n",
    "\n",
    "- Add a prefix to the dialogue.\n",
    "- Pad the dialogues and the summary to the max length with the eos token.\n",
    "- Tokenize to create __input_ids__ and __label__ fields in the dataset.\n",
    "- Replace the eos token IDs in labels with -100, so that they are taken into account in the loss calculation.\n",
    "- Remove other fields , that aren't needed anymore\n",
    "\n",
    "This is illustrated in the image below\n",
    "\n",
    "![title](assets/Paddingflan.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ded62859-db1e-4ccb-a9d5-d812c83aa847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ba2976b586f4f6bb5bc4c42ade23e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(sample,padding=\"max_length\"):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
    " \n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    " \n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    " \n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "  \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    " \n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d6326a-baad-47a9-8caa-3cb83bebad57",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a871b70-d7ea-41d5-b597-82dcbbcfb210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    " \n",
    "# load model from the hub\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c8a657b-c065-4f24-af7f-ec4c91475126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download(\"punkt\")\n",
    " \n",
    "# Metric\n",
    "metric = evaluate.load(\"rouge\")\n",
    " \n",
    "# helper function to postprocess text\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    " \n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n",
    " \n",
    "    return preds, labels\n",
    " \n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    " \n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    " \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6965ea57-5d51-4198-bae9-8317c1cc970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    " \n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd6e3586-0d68-4fee-a281-3597753757f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    " \n",
    "# Hugging Face repository id\n",
    "repository_id = f\"{model_id.split('/')[1]}-{data_id}\"\n",
    " \n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=repository_id,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, # Overflows with fp16\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=5,\n",
    "    # logging & evaluation strategies\n",
    "    logging_dir=f\"{repository_id}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    # metric_for_best_model=\"overall_f1\",\n",
    "    # push to hub parameters\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=False,\n",
    "    hub_strategy=\"every_save\",\n",
    "    hub_model_id=repository_id,\n",
    "    hub_token=HfFolder.get_token(),\n",
    ")\n",
    " \n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72459bb4-2694-45b1-a4a0-99163f2edb1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9210' max='9210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9210/9210 1:39:34, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.469700</td>\n",
       "      <td>1.396656</td>\n",
       "      <td>46.015300</td>\n",
       "      <td>22.241600</td>\n",
       "      <td>38.074100</td>\n",
       "      <td>42.383600</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.350400</td>\n",
       "      <td>1.388259</td>\n",
       "      <td>45.978800</td>\n",
       "      <td>22.095400</td>\n",
       "      <td>38.219900</td>\n",
       "      <td>42.427500</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.290100</td>\n",
       "      <td>1.383503</td>\n",
       "      <td>46.345600</td>\n",
       "      <td>22.204800</td>\n",
       "      <td>38.373700</td>\n",
       "      <td>42.528700</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.233100</td>\n",
       "      <td>1.388069</td>\n",
       "      <td>46.558400</td>\n",
       "      <td>22.833400</td>\n",
       "      <td>38.882800</td>\n",
       "      <td>43.029500</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.210200</td>\n",
       "      <td>1.389546</td>\n",
       "      <td>46.449200</td>\n",
       "      <td>22.512000</td>\n",
       "      <td>38.613200</td>\n",
       "      <td>42.844900</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/mypy/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/mypy/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/mypy/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/mypy/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/mypy/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9210, training_loss=1.3138273092098007, metrics={'train_runtime': 5975.3743, 'train_samples_per_second': 12.327, 'train_steps_per_second': 1.541, 'total_flos': 5.043922658131968e+16, 'train_loss': 1.3138273092098007, 'epoch': 5.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e241dee1-b36d-4dea-87ac-75f4fd075f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/mypy/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='103' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [103/103 00:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.383502721786499,\n",
       " 'eval_rouge1': 46.3456,\n",
       " 'eval_rouge2': 22.2048,\n",
       " 'eval_rougeL': 38.3737,\n",
       " 'eval_rougeLsum': 42.5287,\n",
       " 'eval_gen_len': 20.0,\n",
       " 'eval_runtime': 44.391,\n",
       " 'eval_samples_per_second': 18.45,\n",
       " 'eval_steps_per_second': 2.32,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb0fbdf7-6a04-4405-9dce-f0c719bfd427",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "events.out.tfevents.1722167089.a8cd146e4c7b:   0%|          | 0.00/12.7k [00:00<?, ?B/s]\n",
      "events.out.tfevents.1722173139.a8cd146e4c7b:   0%|          | 0.00/40.0 [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "training_args.bin:   0%|          | 0.00/5.37k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Upload 5 LFS files:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   0%|          | 3.78M/990M [00:00<00:30, 32.0MB/s]\u001b[A\u001b[A\n",
      "events.out.tfevents.1722167089.a8cd146e4c7b: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12.7k/12.7k [00:00<00:00, 46.4kB/s]\n",
      "\n",
      "training_args.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.37k/5.37k [00:00<00:00, 17.2kB/s][A\u001b[A\n",
      "events.out.tfevents.1722173139.a8cd146e4c7b: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40.0/40.0 [00:00<00:00, 94.9B/s]\n",
      "events.out.tfevents.1722167089.a8cd146e4c7b: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12.7k/12.7k [00:00<00:00, 25.3kB/s]\n",
      "\n",
      "\n",
      "\n",
      "Upload 5 LFS files:  20%|‚ñà‚ñà        | 1/5 [00:00<00:03,  1.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   1%|          | 9.49M/990M [00:00<02:11, 7.49MB/s]\u001b[A\u001b[A\n",
      "\n",
      "spiece.model: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 792k/792k [00:01<00:00, 665kB/s]29MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:   2%|‚ñè         | 15.9M/990M [00:01<01:11, 13.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   2%|‚ñè         | 18.2M/990M [00:01<01:54, 8.52MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   2%|‚ñè         | 22.1M/990M [00:01<01:24, 11.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   3%|‚ñé         | 25.6M/990M [00:02<01:05, 14.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   3%|‚ñé         | 28.0M/990M [00:02<01:43, 9.28MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   3%|‚ñé         | 30.0M/990M [00:02<01:33, 10.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   3%|‚ñé         | 32.0M/990M [00:03<02:34, 6.20MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   4%|‚ñç         | 38.3M/990M [00:03<01:25, 11.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   4%|‚ñç         | 42.9M/990M [00:04<01:27, 10.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   5%|‚ñç         | 44.7M/990M [00:04<01:29, 10.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   5%|‚ñç         | 46.2M/990M [00:04<01:25, 11.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   5%|‚ñç         | 48.0M/990M [00:05<02:31, 6.23MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   5%|‚ñå         | 54.3M/990M [00:05<01:24, 11.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   6%|‚ñå         | 56.5M/990M [00:05<01:16, 12.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   6%|‚ñå         | 58.8M/990M [00:05<01:40, 9.24MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   6%|‚ñå         | 60.4M/990M [00:06<01:45, 8.84MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   6%|‚ñã         | 62.0M/990M [00:06<01:36, 9.59MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   6%|‚ñã         | 64.0M/990M [00:07<03:05, 4.99MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   7%|‚ñã         | 70.3M/990M [00:07<01:35, 9.60MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   8%|‚ñä         | 74.8M/990M [00:07<01:40, 9.09MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   8%|‚ñä         | 76.4M/990M [00:07<01:43, 8.85MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   8%|‚ñä         | 78.0M/990M [00:08<01:35, 9.51MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   8%|‚ñä         | 80.0M/990M [00:08<02:32, 5.97MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   9%|‚ñä         | 86.3M/990M [00:08<01:24, 10.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   9%|‚ñâ         | 90.8M/990M [00:09<01:32, 9.71MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   9%|‚ñâ         | 92.4M/990M [00:09<01:35, 9.36MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   9%|‚ñâ         | 94.1M/990M [00:09<01:30, 9.92MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  10%|‚ñâ         | 96.0M/990M [00:10<02:26, 6.12MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  10%|‚ñà         | 102M/990M [00:10<01:21, 10.9MB/s] \u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  11%|‚ñà         | 107M/990M [00:11<01:17, 11.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  11%|‚ñà         | 109M/990M [00:11<01:19, 11.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  11%|‚ñà         | 110M/990M [00:11<01:24, 10.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  11%|‚ñà‚ñè        | 112M/990M [00:12<02:27, 5.96MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  13%|‚ñà‚ñé        | 125M/990M [00:12<00:48, 17.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  13%|‚ñà‚ñé        | 130M/990M [00:12<00:54, 15.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  14%|‚ñà‚ñç        | 141M/990M [00:12<00:31, 26.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  15%|‚ñà‚ñç        | 147M/990M [00:13<00:37, 22.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  16%|‚ñà‚ñå        | 158M/990M [00:13<00:25, 33.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  17%|‚ñà‚ñã        | 164M/990M [00:13<00:33, 24.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  18%|‚ñà‚ñä        | 175M/990M [00:13<00:23, 34.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  18%|‚ñà‚ñä        | 182M/990M [00:14<00:28, 28.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  19%|‚ñà‚ñâ        | 187M/990M [00:14<00:29, 27.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  19%|‚ñà‚ñâ        | 191M/990M [00:15<00:56, 14.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  20%|‚ñà‚ñâ        | 194M/990M [00:15<01:16, 10.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  20%|‚ñà‚ñà        | 199M/990M [00:16<01:01, 12.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  21%|‚ñà‚ñà        | 203M/990M [00:16<00:48, 16.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  21%|‚ñà‚ñà        | 207M/990M [00:17<01:22, 9.54MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  21%|‚ñà‚ñà        | 209M/990M [00:17<01:48, 7.20MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  22%|‚ñà‚ñà‚ñè       | 219M/990M [00:17<00:55, 13.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  23%|‚ñà‚ñà‚ñé       | 224M/990M [00:18<00:56, 13.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  23%|‚ñà‚ñà‚ñé       | 232M/990M [00:18<00:39, 19.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  24%|‚ñà‚ñà‚ñç       | 237M/990M [00:18<00:32, 22.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  24%|‚ñà‚ñà‚ñç       | 241M/990M [00:18<00:40, 18.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  25%|‚ñà‚ñà‚ñå       | 249M/990M [00:18<00:28, 26.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  26%|‚ñà‚ñà‚ñå       | 254M/990M [00:19<00:24, 29.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  26%|‚ñà‚ñà‚ñå       | 259M/990M [00:19<00:33, 22.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  27%|‚ñà‚ñà‚ñã       | 265M/990M [00:19<00:26, 27.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  27%|‚ñà‚ñà‚ñã       | 271M/990M [00:19<00:21, 32.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  28%|‚ñà‚ñà‚ñä       | 276M/990M [00:21<01:18, 9.14MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  28%|‚ñà‚ñà‚ñä       | 279M/990M [00:21<01:04, 11.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  29%|‚ñà‚ñà‚ñâ       | 286M/990M [00:21<00:45, 15.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  29%|‚ñà‚ñà‚ñâ       | 290M/990M [00:21<00:50, 13.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  30%|‚ñà‚ñà‚ñâ       | 296M/990M [00:21<00:37, 18.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  31%|‚ñà‚ñà‚ñà       | 302M/990M [00:21<00:28, 24.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  31%|‚ñà‚ñà‚ñà       | 307M/990M [00:22<00:34, 20.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  31%|‚ñà‚ñà‚ñà‚ñè      | 311M/990M [00:22<00:28, 23.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  32%|‚ñà‚ñà‚ñà‚ñè      | 318M/990M [00:22<00:21, 30.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  33%|‚ñà‚ñà‚ñà‚ñé      | 323M/990M [00:22<00:27, 24.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  33%|‚ñà‚ñà‚ñà‚ñé      | 329M/990M [00:22<00:22, 29.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  34%|‚ñà‚ñà‚ñà‚ñç      | 336M/990M [00:23<00:28, 23.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  35%|‚ñà‚ñà‚ñà‚ñç      | 344M/990M [00:23<00:21, 30.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  35%|‚ñà‚ñà‚ñà‚ñå      | 349M/990M [00:23<00:19, 33.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  36%|‚ñà‚ñà‚ñà‚ñå      | 353M/990M [00:23<00:27, 23.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  36%|‚ñà‚ñà‚ñà‚ñã      | 360M/990M [00:24<00:21, 29.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  37%|‚ñà‚ñà‚ñà‚ñã      | 365M/990M [00:24<00:18, 33.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  37%|‚ñà‚ñà‚ñà‚ñã      | 369M/990M [00:24<00:26, 23.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  38%|‚ñà‚ñà‚ñà‚ñä      | 378M/990M [00:24<00:18, 32.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  39%|‚ñà‚ñà‚ñà‚ñâ      | 384M/990M [00:25<00:24, 24.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  40%|‚ñà‚ñà‚ñà‚ñâ      | 393M/990M [00:25<00:17, 33.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  40%|‚ñà‚ñà‚ñà‚ñà      | 400M/990M [00:25<00:21, 27.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 410M/990M [00:25<00:15, 37.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 416M/990M [00:26<00:21, 26.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 427M/990M [00:26<00:15, 37.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 433M/990M [00:26<00:17, 31.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 445M/990M [00:26<00:12, 43.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 451M/990M [00:26<00:15, 35.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 461M/990M [00:26<00:12, 43.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 467M/990M [00:27<00:16, 30.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 475M/990M [00:27<00:13, 39.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 481M/990M [00:27<00:16, 29.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 491M/990M [00:27<00:12, 39.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 497M/990M [00:28<00:15, 31.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 510M/990M [00:28<00:10, 44.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 516M/990M [00:28<00:15, 31.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 525M/990M [00:28<00:11, 39.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 532M/990M [00:29<00:14, 31.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 541M/990M [00:29<00:11, 40.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 548M/990M [00:29<00:14, 30.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 557M/990M [00:29<00:11, 39.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 563M/990M [00:30<00:14, 30.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 573M/990M [00:30<00:10, 39.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 579M/990M [00:30<00:14, 29.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 589M/990M [00:30<00:10, 38.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 596M/990M [00:31<00:13, 28.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 604M/990M [00:31<00:10, 36.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 610M/990M [00:31<00:13, 28.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 621M/990M [00:31<00:09, 39.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 627M/990M [00:32<00:11, 31.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 637M/990M [00:32<00:08, 42.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 644M/990M [00:32<00:10, 32.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 654M/990M [00:32<00:08, 41.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 660M/990M [00:32<00:10, 31.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 669M/990M [00:33<00:08, 39.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 675M/990M [00:33<00:10, 30.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 685M/990M [00:33<00:07, 39.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 691M/990M [00:33<00:10, 29.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 702M/990M [00:33<00:07, 40.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 709M/990M [00:34<00:09, 29.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 719M/990M [00:34<00:06, 39.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 726M/990M [00:34<00:08, 29.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 733M/990M [00:34<00:07, 35.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 739M/990M [00:35<00:09, 26.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 750M/990M [00:35<00:06, 36.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 756M/990M [00:35<00:08, 27.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 764M/990M [00:35<00:06, 34.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 769M/990M [00:36<00:08, 26.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 782M/990M [00:36<00:05, 39.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 788M/990M [00:36<00:06, 29.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 799M/990M [00:37<00:04, 39.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 806M/990M [00:37<00:06, 29.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 815M/990M [00:37<00:04, 38.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 822M/990M [00:37<00:06, 26.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 832M/990M [00:39<00:14, 11.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 843M/990M [00:39<00:08, 16.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 849M/990M [00:40<00:08, 16.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 861M/990M [00:40<00:05, 23.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 868M/990M [00:40<00:05, 21.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 877M/990M [00:40<00:03, 28.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 883M/990M [00:41<00:04, 23.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 893M/990M [00:41<00:03, 32.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 899M/990M [00:41<00:03, 26.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 910M/990M [00:41<00:02, 37.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 917M/990M [00:42<00:02, 29.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 928M/990M [00:42<00:01, 39.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 935M/990M [00:42<00:01, 30.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 944M/990M [00:43<00:01, 25.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 957M/990M [00:43<00:00, 36.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 964M/990M [00:43<00:00, 29.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 975M/990M [00:43<00:00, 39.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 990M/990M [00:44<00:00, 22.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Upload 5 LFS files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:45<00:00,  9.01s/it]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Arindam1975/flan-t5-base-samsum/commit/48d0a9f793c713bfef5e855505eebbcef550ad09', commit_message='End of training', commit_description='', oid='48d0a9f793c713bfef5e855505eebbcef550ad09', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save our tokenizer and create model card\n",
    "tokenizer.save_pretrained(repository_id)\n",
    "trainer.create_model_card()\n",
    "# Push the results to the hub\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85eab48-7fe5-430a-b697-5bb97b6edf07",
   "metadata": {},
   "source": [
    "### Load the Trained Model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e17838b9-7940-430a-9376-899a765667b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4772bbac-f30f-498d-b512-d5a4cf5551cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialogue: \n",
      "Michael: hey, how are you\n",
      "Kai: hey! I am fine, just working too much. what about you? you travel so much!\n",
      "Michael: haha yes. At airport on my way back. looong trip\n",
      "Kai: where have you been now?\n",
      "Michael: argentina brazil and chile\n",
      "Kai: wow! how long?\n",
      "Michael: 2 weeks, lots of flights to make it work. I'm in Boston next weekend!\n",
      "Kai: really??! how come?\n",
      "Michael: just because I found a cheap ticket üòã\n",
      "Kai: nice:) but it's cold\n",
      "Michael: hmm well.. I can deal with the cold now\n",
      "Kai: are you not tired of all this travelling?\n",
      "Michael: hmm, a little bit but not really. I‚Äôm more scared to stay in London and do nothing, because I‚Äôm so bored of it\n",
      "Kai: I see, a man full of energy :)\n",
      "Michael: well sort of, for fun stuff, but tired of work. \n",
      "Kai: yes, I remember quite well üòã\n",
      "Michael: Hahah. Thinking of resigning earlier than I was planning\n",
      "Kai: and then?\n",
      "Michael: I don‚Äôt have an answer to that one yet, and it‚Äôs not really a solution because I‚Äôd need to work 2 months notice period anyway, but I‚Äôve just lost motivation after 3 years\n",
      "Kai: Would you like to leave London?\n",
      "Michael: yes asap\n",
      "Kai: for the Netherlands?\n",
      "Michael: maybe to recharge and look for jobs abroad, but I don't want to stay there. I know it‚Äôs not smart to quit without an alternative\n",
      "Kai: depends on the field. I don't know yours and your experience. But it can be a bit stressful also\n",
      "Michael: true, in my field it wouldn‚Äôt be good. Anyway I need to speak to my manager and discuss because I‚Äôm not happy in my job now. How‚Äôs everything on your side? too much work?\n",
      "Kai: yes, I basically never rest these days\n",
      "Michael: that‚Äôs not good, you need rest to avoid a burnout, which I had just before I went on holiday\n",
      "Kai: Probably, I have a conference 8th of Dec, then I'll rest a bit, I hope.\n",
      "---------------\n",
      "flan-t5-base summary:\n",
      "Michael has been to argentina, brazil and Chile for 2 weeks. He's in Boston next weekend. Michael doesn't want to stay in London because he's bored of it. Kai has a conference on the 8th of December. Kai will probably rest a bit. Neither Michael nor Kai have any plans for the future. Hopefully, Michael will talk to his manager about the possibility of resigning. Probably, he will go to the Netherlands to look for a job abroad. Apparently, Michael has lost his motivation after 3 years of travelling. Nevertheless, Michael wants to leave London as soon as possible. Luckily, Kai is going to meet with Michael's manager on 8 December. Afterwards, Kai will rest. Eventually, Michael and Kai will meet again.  Michael / Kai   Kai / Michael  (Kai) / \n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from random import randrange\n",
    " \n",
    "# load model and tokenizer from huggingface hub with pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"Arindam1975/flan-t5-base-samsum\", device=device)\n",
    " \n",
    "# select a random test sample\n",
    "sample = dataset['test'][randrange(len(dataset[\"validation\"]))]\n",
    "print(f\"dialogue: \\n{sample['dialogue']}\\n---------------\")\n",
    " \n",
    "# summarize dialogue\n",
    "res = summarizer(sample[\"dialogue\"])\n",
    " \n",
    "print(f\"flan-t5-base summary:\\n{res[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3984a1f-2fab-4521-8758-3d673bc035fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
