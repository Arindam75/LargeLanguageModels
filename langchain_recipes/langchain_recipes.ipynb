{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "HF_API_KEY = os.getenv(\"HF_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "Basic usage by using the Huggingface API. Langchain 0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arind\\Documents\\LargeLanguageModels\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceEndpoint` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the from langchain-huggingface package and should be used instead. To use it run `pip install -U from langchain-huggingface` and import as `from from langchain_huggingface import llms import HuggingFaceEndpoint`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\arind\\Documents\\LargeLanguageModels\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\arind\\.cache\\huggingface\\token\n",
      "Login successful\n",
      ". The most common problem people complain about is foot odor. It can be embarrassing for both you and your feet.\n",
      "There are a few things you can do to help prevent foot odor. Here are some tips:\n",
      "1. Wear clean, dry shoes\n",
      "2. Wear socks\n",
      "3. Don’t wear your shoes too tight\n",
      "4. Keep your shoes in a cool, dry place\n",
      "5. Use an antiseptic spray or powder\n",
      "6. Try an odor eliminator spray\n",
      "7. Use a foot spray with alcohol\n",
      "8. Soak your feet in a foot bath\n",
      "9. Use foot powders or sprays to keep your feet dry\n",
      "10. Wear breathable shoes\n",
      "There are many other things you can do to prevent foot odor, such as using a foot bath to clean your feet, using a foot spray with a deodorizer, or using a foot powder. It’s also important to have a good foot hygiene routine.\n",
      "Many people believe that foot odor is caused by bacteria, but it can be caused by a variety of things, such as poor hygiene or fungal infections. If you have foot odor, it’s important to take care of your feet to prevent other problems.\n",
      "If you have persistent foot odor, you should consider seeing a doctor or podiatrist to determine the cause.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.huggingface_endpoint import HuggingFaceEndpoint\n",
    "\n",
    "# Define the LLM\n",
    "llm = HuggingFaceEndpoint(repo_id='tiiuae/falcon-7b-instruct', huggingfacehub_api_token=HF_API_KEY)\n",
    "\n",
    "# Predict the words following the text in question\n",
    "question = 'Whatever you do, take care of your shoes'\n",
    "output = llm.invoke(question)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using OPenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". That’s why I am such a fan of shoe trees. They prevent shoes from curling, wrinkling, and developing creases. They also extend the life of your shoes, so you don’t have to replace them as often.\n",
      "\n",
      "Whenever I purchase a new pair of shoes, I also buy a pair of cedar shoe trees to go with them. I’ve found that the combination of shoe trees and regular polishing can keep a pair of leather shoes looking like new for several years.\n",
      "\n",
      "Of course, not all shoe trees are created equal. In fact, most of the cheap ones are nothing but a waste of money. In this article, I’m going to explain what to look for in shoe trees and why it’s worth spending a little extra to get the best.\n",
      "\n",
      "What to Look for in a Shoe Tree\n",
      "\n",
      "When you’re shopping for shoe trees, there are a few things you want to keep in mind:\n",
      "\n",
      "1. Material\n",
      "\n",
      "The best shoe trees are made of aromatic red cedar. This is a type of wood that is known for its moisture-absorbing and deodorizing properties, which is perfect for your shoes. It also has a nice scent that will keep your shoes smelling fresh.\n",
      "\n",
      "Cedar is also durable and lightweight, so it won’t add any extra weight\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "# Define the LLM\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Predict the words following the text in question\n",
    "question = 'Whatever you do, take care of your shoes'\n",
    "output = llm.invoke(question)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts\n",
    "\n",
    "### Using huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\arind\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arind\\Documents\\LargeLanguageModels\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LangChain provides LLM application development tools and services that enable legal professionals to create intelligent, automated workflows. Our tools allow users to quickly generate legal documents and contracts, reducing time-to-market and ensuring accuracy. LangChain's services also include legal language model training, which allows clients to create customized models tailored to their specific needs. This helps LLM application developers save time and resources by avoiding the need to build and train a model from scratch.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Create a prompt template from the template string\n",
    "template = \"You are an artificial intelligence assistant, answer the question. {question}\"\n",
    "prompt = PromptTemplate(template = template, input_variables = [\"question\"])\n",
    "\n",
    "# Create a chain to integrate the prompt template and LLM\n",
    "llm = HuggingFaceEndpoint(repo_id='tiiuae/falcon-7b-instruct', huggingfacehub_api_token = HF_API_KEY)\n",
    "llm_chain = LLMChain(prompt = prompt, llm = llm)\n",
    "\n",
    "question = \"How does LangChain make LLM application development easier?\"\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To retain learning effectively, you can try the following strategies:\\n\\n1. Review and practice regularly: Consistent review and practice of the material you have learned can help reinforce your memory and understanding.\\n\\n2. Teach others: Explaining concepts to someone else can help solidify your own understanding and retention of the information.\\n\\n3. Use mnemonic devices: Mnemonics, such as acronyms or visual aids, can help you remember key information more easily.\\n\\n4. Stay engaged: Actively participate in your learning process by asking questions, taking notes, and seeking out additional resources to deepen your understanding.\\n\\n5. Apply what you've learned: Put your knowledge into practice by applying it to real-life situations or scenarios.\\n\\n6. Take breaks: Give yourself time to rest and recharge between study sessions to prevent burnout and improve retention.\\n\\nBy incorporating these strategies into your learning routine, you can enhance your ability to retain information effectively.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define an OpenAI chat model\n",
    "llm = ChatOpenAI(temperature = 0, openai_api_key = OPENAI_API_KEY)\t\t\n",
    "\n",
    "# Create a chat prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"Respond to question: {question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Insert a question into the template and call the model\n",
    "full_prompt = prompt_template.format_messages(question = 'How can I retain learning?')\n",
    "llm.invoke(full_prompt).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat History\n",
    "\n",
    "### ChatMessageHistory\n",
    "\n",
    "The ChatMessageHistory class stores the full history of messages between the user and model. By providing this to the model, we can provide follow-up questions and iterate on the response message. Let's implement this message history into an OpenAI model.<br>\n",
    "When additional user messages are provided, the model bases its response on the full context stored in the conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Learning Langchain is like navigating a complex and ever-changing maze, where each new turn reveals a deeper layer of interconnected knowledge waiting to be discovered.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_llm = ChatOpenAI(temperature = 0, openai_api_key = OPENAI_API_KEY)\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_ai_message(\"Hi! Ask me anything about Langchain.\")\n",
    "history.add_user_message(\"Describe a metaphor for learning Langchain in one sentence.\")\n",
    "\n",
    "chat_llm(history.messages).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Learning Langchain is like unraveling a complex puzzle.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.add_user_message(\"Summarize the preceding sentence in fewer words.\")\n",
    "\n",
    "chat_llm(history.messages).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationBufferMemory\n",
    "\n",
    "For many applications, storing and accessing the entire conversation history isn't technically feasible. In these cases, the messages must be condensed while retaining as much relevant context as possible. One common way of doing this is with a memory buffer, which stores only the most recent messages.<br>\n",
    "\n",
    "This gives the application a rolling buffer memory containing the last few messages in the conversation. Users can specify the number of messages to store with the size argument, and the application will discard older messages as newer ones are added.<br>\n",
    "\n",
    "To integrate the memory type into model, we use a special type of chain for conversations: ConversationChain. Setting verbose equal to true allows the model to output its decisions along with the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chat_llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\",\n",
    "                  temperature = 0,\n",
    "                  openai_api_key = OPENAI_API_KEY)\n",
    "memory = ConversationBufferMemory(size = 4)\n",
    "\n",
    "buffer_chain = ConversationChain(\n",
    "    llm = chat_llm,\n",
    "    verbose = True,\n",
    "    memory = memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Describe a language model in one sentence\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Describe a language model in one sentence\n",
      "AI:  A language model is a statistical model that is trained on a large corpus of text and is able to generate coherent and grammatically correct sentences based on the patterns and structures it has learned.\n",
      "Human: Describe it again using less words\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Describe a language model in one sentence\n",
      "AI:  A language model is a statistical model that is trained on a large corpus of text and is able to generate coherent and grammatically correct sentences based on the patterns and structures it has learned.\n",
      "Human: Describe it again using less words\n",
      "AI:  A language model is a computer program that can generate sentences based on patterns it has learned from a large amount of text.\n",
      "Human: Describe it again fewer words but at least one word\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Describe a language model in one sentence\n",
      "AI:  A language model is a statistical model that is trained on a large corpus of text and is able to generate coherent and grammatically correct sentences based on the patterns and structures it has learned.\n",
      "Human: Describe it again using less words\n",
      "AI:  A language model is a computer program that can generate sentences based on patterns it has learned from a large amount of text.\n",
      "Human: Describe it again fewer words but at least one word\n",
      "AI:  A language model is a program that generates sentences from text patterns.\n",
      "Human: What did I first ask you? I forgot.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' You asked me to describe a language model in one sentence.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer_chain.predict(input=\"Describe a language model in one sentence\")\n",
    "buffer_chain.predict(input=\"Describe it again using less words\")\n",
    "buffer_chain.predict(input=\"Describe it again fewer words but at least one word\")\n",
    "buffer_chain.predict(input=\"What did I first ask you? I forgot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another Example, but we have to re-instantiate the memory, so that the context from the previous cell is not carried forward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Write Python code to draw a scatter plot.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Write Python code to draw a scatter plot.\n",
      "AI:  Sure, I can definitely help you with that! To draw a scatter plot in Python, you will need to import the matplotlib library. This can be done by using the command \"import matplotlib.pyplot as plt\". Then, you can create a scatter plot by using the \"scatter()\" function and passing in the x and y values as parameters. For example, if you have two lists of data called \"x_data\" and \"y_data\", you can create a scatter plot by using the code \"plt.scatter(x_data, y_data)\". You can also customize your scatter plot by adding a title, labels for the x and y axes, and changing the color and size of the data points. Is there anything else you would like to know about drawing a scatter plot in Python?\n",
      "Human: Use the Seaborn library.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Ah, great question! Seaborn is another popular library for data visualization in Python. To use Seaborn to draw a scatter plot, you will first need to import it using the command \"import seaborn as sns\". Then, you can use the \"scatterplot()\" function and pass in the x and y values as well as the name of the data frame containing your data. For example, if your data frame is called \"df\" and you want to plot the columns \"x\" and \"y\", you can use the code \"sns.scatterplot(x=\\'x\\', y=\\'y\\', data=df)\". Seaborn also offers many customization options for your scatter plot, such as adding a trendline or changing the shape and color of the data points. Is there anything else you would like to know about using Seaborn for scatter plots?'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(size = 4)\n",
    "buffer_chain = ConversationChain(\n",
    "    llm = chat_llm,\n",
    "    verbose = True,\n",
    "    memory = memory\n",
    ")\n",
    "\n",
    "# Invoke the chain with the inputs provided\n",
    "buffer_chain.predict(input=\"Write Python code to draw a scatter plot.\")\n",
    "buffer_chain.predict(input=\"Use the Seaborn library.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationSummaryMemory\n",
    "\n",
    "For longer conversations, storing the entire memory, or even a long buffer memory, may not be technically feasible. In these cases, a summary memory implementation can be a good option. Summary memories summarize the conversation at each step to retain the key context for the model to use. This works by using another LLM for generating the summaries, alongside the LLM used for generating the responses.<br>\n",
    "\n",
    "The ConversationSummaryMemory class summarizes the conversation over time, condensing the information. This means that the chat model can remember key pieces of context without needing to store and process the entire conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Please summarize the future in 2 sentences.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Please summarize the future in 2 sentences.\n",
      "AI:  The future is a vast and complex concept that is constantly changing and evolving. It is impossible to accurately summarize it in just two sentences, as it encompasses everything from technological advancements to societal shifts and individual experiences. However, one thing is certain: the future holds endless possibilities and opportunities for growth and progress.\n",
      "Human: Why?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human asks the AI to summarize the future in two sentences. The AI explains that the future is constantly changing and impossible to summarize in just two sentences, but it holds endless possibilities for growth and progress.\n",
      "Human: Why?\n",
      "AI:  Because the future is shaped by the choices and actions of individuals and society as a whole. As we continue to innovate and adapt, the future will continue to unfold in ways we cannot even imagine. Additionally, the future is constantly being influenced by various factors such as politics, economics, and technology, making it impossible to predict with complete certainty.\n",
      "Human: What will I need to shape this?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' As an individual, you have the power to shape the future through your actions and decisions. It is important to stay informed, open-minded, and proactive in order to contribute to a positive and progressive future. Additionally, collaboration and cooperation with others is crucial in shaping a collective future that benefits everyone.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm = chat_llm, max_token_limit = 100)\n",
    "summary_chain = ConversationChain(llm = chat_llm, verbose = True, memory = memory)\n",
    "\n",
    "summary_chain.predict(input=\"Please summarize the future in 2 sentences.\")\n",
    "summary_chain.predict(input=\"Why?\")\n",
    "summary_chain.predict(input=\"What will I need to shape this?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
