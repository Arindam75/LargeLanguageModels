{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link](https://www.philschmid.de/sagemaker-llama2-qlora)<br>\n",
    "[Mistral](https://medium.com/@pierre_guillou/fine-tune-the-llm-mistral-7b-on-amazon-sagemaker-today-4791613c335b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\arind\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os \n",
    "import dotenv\n",
    "\n",
    "from datasets import load_dataset\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name arindam.d.dey@gmail.com to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::570517415597:role/arindam_sagemaker\n",
      "sagemaker bucket: sagemaker-ap-south-1-570517415597\n",
      "sagemaker session region: ap-south-1\n"
     ]
    }
   ],
   "source": [
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    " \n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='arindam_sagemaker')['Role']['Arn']\n",
    "    \n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    " \n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup HF for Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] =  os.getenv(\"HF_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 15011\n",
      "A random samples\n",
      "{'instruction': 'What is the Taubate Prison known for', 'context': 'Taubaté Prison is a prison in Taubaté in São Paulo, Brazil. It is notorious for containing some of the most violent prisoners, for repeated prison riots, and for being the place where the Primeiro Comando da Capital criminal gang originated.\\n\\nOn December 19, 2000 The Prison Uprising ended at Taubaté Prison\\n\\nreleased more than 20 hostages on Monday, ending an uprising at a maximum security facility that left nine prisoners dead, officials said.\\n\\nThe rebellion at the Taubate House of Custody and Psychiatric Treatment, about 80 miles outside Sao Paulo, began during visiting hours Sunday when an inmate opened fire with a revolver, provoking a fight with prisoners from another pavilion.\\n\\nTaking advantage of the confusion, prisoners took 23 hostages including four children.\\n\\nInmates began releasing hostages in small groups Monday after authorities agreed to transfer 10 prisoners to another facility. The hostages, all of whom were unhurt said the prisoners treated them well during the ordeal.\\n\\nThe department said the nine victims were probably killed in a settling of scores between rival gangs.', 'response': 'The Taubate Prison is a prison in Taubate in Sao Paulo, Brazil. It is notorious for containing some of the most violent prisoners, for repeated prison riots, and for being the place where the Primeiro Comando da Capital criminal gang originated.\\n\\nOn December 19, 2000 The Prison Uprising ended at Taubate Prison\\n\\nreleased more than 20 hostages on Monday, ending an uprising at a maximum security facility that left nine prisoners dead, officials said.\\n\\nThe rebellion at the Taubate House of Custody and Psychiatric Treatment, about 80 miles outside Sao Paulo, began during visiting hours Sunday when an inmate opened fire with a revolver, provoking a fight with prisoners from another pavilion.\\n\\nTaking advantage of the confusion, prisoners took 23 hostages including four children.\\n\\nInmates began releasing hostages in small groups Monday after authorities agreed to transfer 10 prisoners to another facility. The hostages, all of whom were unhurt said the prisoners treated them well during the ordeal.\\n\\nThe department said the nine victims were probably killed in a settling of scores between rival gangs.', 'category': 'summarization'}\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print('A random samples')\n",
    "print(dataset[randrange(len(dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Dataset\n",
    "\n",
    "Our dataset is currently a list of dictionaries. Each dictionary entry in the dataset has the keys __instruction, context, response and category__<br>\n",
    "\n",
    "Our objective is to remove the key/value pairs from each dictionary and keep only one key called __text__. The value of the key shall contain the prompt terminated by the selected tokenizer's eos token.<br>\n",
    "\n",
    "{'text': \n",
    "\"### Instruction<br>\n",
    "When did Virgin Australia start operating?<br>\n",
    "<br>\n",
    "<br>\n",
    "\\### Context<br>\n",
    "Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline.<br>\n",
    "<br>\n",
    "<br>\n",
    "\\### Response<br>\n",
    "Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.\\</s>\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-2-7b-hf\" # sharded weights\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,use_auth_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dolly(sample):\n",
    "\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample['context'])>0 else None\n",
    "    response = f\"### Response\\n{sample['response']}\"\n",
    "\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "How does Windows Fast Startup Work?\n",
      "\n",
      "### Response\n",
      "Fast Startup is a Windows feature that allows you to boot your computer in a few seconds rather than a minute. Rather than going through the cold boot path, Fast Startup uses a minimal hiberfile to resume the system. When the feature is enabled, selecting “Shutdown” in the Windows UI doesn’t actually shutdown the system. Instead, it closes all user applications, logs the current user out, and then creates a hiberfile. Because this hiberfile only includes the kernel, device drivers and a subset of applications, it is small and can be reloaded quickly. \n",
      "\n",
      "Alternatively, the cold boot path requires loading the kernel and drivers from disk, initializing the kernel and drivers, and launching various user mode applications. This can be especially slow on computers that use spinning hard drives.\n"
     ]
    }
   ],
   "source": [
    "print(format_dolly(dataset[randrange(len(dataset))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "Tell me which animals are bigger than the average human: Dog, Mouse, Elephant, Rhino, Hippo, Cat, Squirrel.\n",
      "\n",
      "### Response\n",
      "Sure. Here are the selections from above that are larger than the average human: Elephant, Rhino, and Hippo.</s>\n"
     ]
    }
   ],
   "source": [
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "# print random sample\n",
    "print(dataset[randint(0, len(dataset))][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n.map(\\n    partial(chunk, chunk_length=2048),\\n    batched=True,\\n)\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    " \n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    #print(sample.keys())\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    \n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    " \n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    " \n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ")\n",
    "'''\n",
    ".map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = lm_dataset.map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 835,\n",
       " 2799,\n",
       " 4080,\n",
       " 13,\n",
       " 10401,\n",
       " 1258,\n",
       " 9167,\n",
       " 8314,\n",
       " 1369,\n",
       " 13598,\n",
       " 29973,\n",
       " 13,\n",
       " 13,\n",
       " 2277,\n",
       " 29937,\n",
       " 15228,\n",
       " 13,\n",
       " 29963,\n",
       " 381,\n",
       " 5359,\n",
       " 8314,\n",
       " 29892,\n",
       " 278,\n",
       " 3534,\n",
       " 292,\n",
       " 1024,\n",
       " 310,\n",
       " 9167,\n",
       " 8314,\n",
       " 29718,\n",
       " 349,\n",
       " 1017,\n",
       " 19806,\n",
       " 29892,\n",
       " 338,\n",
       " 385,\n",
       " 9870,\n",
       " 29899,\n",
       " 6707,\n",
       " 4799,\n",
       " 1220,\n",
       " 29889,\n",
       " 739,\n",
       " 338,\n",
       " 278,\n",
       " 10150,\n",
       " 4799,\n",
       " 1220,\n",
       " 491,\n",
       " 22338,\n",
       " 2159,\n",
       " 304,\n",
       " 671,\n",
       " 278,\n",
       " 9167,\n",
       " 14982,\n",
       " 29889,\n",
       " 739,\n",
       " 844,\n",
       " 9223,\n",
       " 5786,\n",
       " 373,\n",
       " 29871,\n",
       " 29941,\n",
       " 29896,\n",
       " 3111,\n",
       " 29871,\n",
       " 29906,\n",
       " 29900,\n",
       " 29900,\n",
       " 29900,\n",
       " 408,\n",
       " 9167,\n",
       " 10924,\n",
       " 29892,\n",
       " 411,\n",
       " 1023,\n",
       " 15780,\n",
       " 373,\n",
       " 263,\n",
       " 2323,\n",
       " 5782,\n",
       " 29889,\n",
       " 739,\n",
       " 11584,\n",
       " 1476,\n",
       " 3528,\n",
       " 408,\n",
       " 263,\n",
       " 4655,\n",
       " 4799,\n",
       " 1220,\n",
       " 297,\n",
       " 8314,\n",
       " 29915,\n",
       " 29879,\n",
       " 21849,\n",
       " 9999,\n",
       " 1156,\n",
       " 278,\n",
       " 24382,\n",
       " 310,\n",
       " 530,\n",
       " 9915,\n",
       " 8314,\n",
       " 297,\n",
       " 3839,\n",
       " 29871,\n",
       " 29906,\n",
       " 29900,\n",
       " 29900,\n",
       " 29896,\n",
       " 29889,\n",
       " 450,\n",
       " 4799,\n",
       " 1220,\n",
       " 756,\n",
       " 1951,\n",
       " 21633,\n",
       " 304,\n",
       " 4153,\n",
       " 9080,\n",
       " 29871,\n",
       " 29941,\n",
       " 29906,\n",
       " 14368,\n",
       " 297,\n",
       " 8314,\n",
       " 29892,\n",
       " 515,\n",
       " 19766,\n",
       " 29879,\n",
       " 297,\n",
       " 1771,\n",
       " 275,\n",
       " 29890,\n",
       " 1662,\n",
       " 29892,\n",
       " 22103,\n",
       " 322,\n",
       " 16198,\n",
       " 29889,\n",
       " 13,\n",
       " 13,\n",
       " 2277,\n",
       " 29937,\n",
       " 13291,\n",
       " 13,\n",
       " 29963,\n",
       " 381,\n",
       " 5359,\n",
       " 8314,\n",
       " 844,\n",
       " 9223,\n",
       " 5786,\n",
       " 373,\n",
       " 29871,\n",
       " 29941,\n",
       " 29896,\n",
       " 3111,\n",
       " 29871,\n",
       " 29906,\n",
       " 29900,\n",
       " 29900,\n",
       " 29900,\n",
       " 408,\n",
       " 9167,\n",
       " 10924,\n",
       " 29892,\n",
       " 411,\n",
       " 1023,\n",
       " 15780,\n",
       " 373,\n",
       " 263,\n",
       " 2323,\n",
       " 5782,\n",
       " 29889,\n",
       " 2,\n",
       " 1,\n",
       " 835,\n",
       " 2799,\n",
       " 4080,\n",
       " 13,\n",
       " 8809,\n",
       " 436,\n",
       " 338,\n",
       " 263,\n",
       " 6606,\n",
       " 310,\n",
       " 9427,\n",
       " 29973,\n",
       " 1763,\n",
       " 412,\n",
       " 470,\n",
       " 1528,\n",
       " 412,\n",
       " 13,\n",
       " 13,\n",
       " 2277,\n",
       " 29937,\n",
       " 13291,\n",
       " 13,\n",
       " 1762,\n",
       " 412,\n",
       " 2,\n",
       " 1,\n",
       " 835,\n",
       " 2799,\n",
       " 4080,\n",
       " 13,\n",
       " 11008,\n",
       " 508,\n",
       " 3949,\n",
       " 1379,\n",
       " 10503,\n",
       " 573,\n",
       " 363,\n",
       " 1472,\n",
       " 1728,\n",
       " 4094,\n",
       " 29973,\n",
       " 13,\n",
       " 13,\n",
       " 2277,\n",
       " 29937,\n",
       " 13291,\n",
       " 13,\n",
       " 14353,\n",
       " 1379,\n",
       " 671,\n",
       " 278,\n",
       " 9950,\n",
       " 297,\n",
       " 1009,\n",
       " 3165,\n",
       " 567,\n",
       " 304,\n",
       " 3013,\n",
       " 963,\n",
       " 10423,\n",
       " 411,\n",
       " 5864,\n",
       " 322,\n",
       " 27246,\n",
       " 29878,\n",
       " 362,\n",
       " 363,\n",
       " 1472,\n",
       " 23704,\n",
       " 310,\n",
       " 931,\n",
       " 29889,\n",
       " 2,\n",
       " 1,\n",
       " 835,\n",
       " 2799,\n",
       " 4080,\n",
       " 13,\n",
       " 29909,\n",
       " 5897,\n",
       " 29915,\n",
       " 29879,\n",
       " 11825,\n",
       " 505,\n",
       " 2211,\n",
       " 29215,\n",
       " 29901,\n",
       " 28533,\n",
       " 29892,\n",
       " 23010,\n",
       " 29891,\n",
       " 29892,\n",
       " 322,\n",
       " 825,\n",
       " 30010,\n",
       " 29879,\n",
       " 278,\n",
       " 1024,\n",
       " 310,\n",
       " 278,\n",
       " 4654,\n",
       " 8750,\n",
       " 29973,\n",
       " 13,\n",
       " 13,\n",
       " 2277,\n",
       " 29937,\n",
       " 13291,\n",
       " 13,\n",
       " 1576,\n",
       " 1024,\n",
       " 310,\n",
       " 278,\n",
       " 4654,\n",
       " 8750,\n",
       " 338,\n",
       " 16308,\n",
       " 2,\n",
       " 1,\n",
       " 835,\n",
       " 2799,\n",
       " 4080,\n",
       " 13,\n",
       " 10401,\n",
       " 471,\n",
       " 4335,\n",
       " 29877,\n",
       " 9940,\n",
       " 7107,\n",
       " 272,\n",
       " 1458,\n",
       " 6345,\n",
       " 29973,\n",
       " 13,\n",
       " 13,\n",
       " 2277,\n",
       " 29937,\n",
       " 15228,\n",
       " 13,\n",
       " 29968,\n",
       " 290,\n",
       " 272,\n",
       " 1458,\n",
       " 471,\n",
       " 6345,\n",
       " 297,\n",
       " 476,\n",
       " 398,\n",
       " 314,\n",
       " 3747,\n",
       " 27611,\n",
       " 522,\n",
       " 545,\n",
       " 373,\n",
       " 5468,\n",
       " 29871,\n",
       " 29896,\n",
       " 29900,\n",
       " 29892,\n",
       " 29871,\n",
       " 29896,\n",
       " 29929,\n",
       " 29947,\n",
       " 29896,\n",
       " 29889,\n",
       " 2860,\n",
       " 10591,\n",
       " 1218,\n",
       " 515,\n",
       " 1880,\n",
       " 3762,\n",
       " 29892,\n",
       " 540,\n",
       " 8772,\n",
       " 278,\n",
       " 435,\n",
       " 29896,\n",
       " 5165,\n",
       " 4402,\n",
       " 319,\n",
       " 1730,\n",
       " 3274,\n",
       " 11871,\n",
       " 2120,\n",
       " 17029,\n",
       " 297,\n",
       " 29871,\n",
       " 29906,\n",
       " 29900,\n",
       " 29900,\n",
       " 29900,\n",
       " 29889,\n",
       " 8512,\n",
       " 540,\n",
       " 2553,\n",
       " 3860,\n",
       " 408,\n",
       " 263,\n",
       " 7145,\n",
       " 29888,\n",
       " 709,\n",
       " 672,\n",
       " 297,\n",
       " 29871,\n",
       " 29906,\n",
       " 29900,\n",
       " 29900,\n",
       " 29896,\n",
       " 29892,\n",
       " 540,\n",
       " 1258,\n",
       " 451,\n",
       " 1708,\n",
       " 1568,\n",
       " 322,\n",
       " 278,\n",
       " 4402,\n",
       " 471,\n",
       " 337,\n",
       " 1397,\n",
       " 630,\n",
       " 304,\n",
       " 278,\n",
       " 435,\n",
       " 29906,\n",
       " 5165,\n",
       " 472,\n",
       " 278,\n",
       " 1095,\n",
       " 310,\n",
       " 278,\n",
       " 29871,\n",
       " 29906,\n",
       " 29900,\n",
       " 29900,\n",
       " 29896,\n",
       " 4259,\n",
       " 29889,\n",
       " 512,\n",
       " 29871,\n",
       " 29906,\n",
       " 29900,\n",
       " 29900,\n",
       " 29906,\n",
       " 29892,\n",
       " 540,\n",
       " 6153,\n",
       " 304,\n",
       " 278,\n",
       " 435,\n",
       " 29906,\n",
       " 4402,\n",
       " 438,\n",
       " 2028,\n",
       " 1605,\n",
       " 262,\n",
       " 2028,\n",
       " 29889,\n",
       " 940,\n",
       " 3897,\n",
       " 263,\n",
       " 4943,\n",
       " 4847,\n",
       " 408,\n",
       " 263,\n",
       " 822,\n",
       " 6270,\n",
       " 7145,\n",
       " 29888,\n",
       " 709,\n",
       " 672,\n",
       " 322,\n",
       " 278,\n",
       " 4402,\n",
       " 2113,\n",
       " 278,\n",
       " 22401,\n",
       " 297,\n",
       " 29871,\n",
       " 29906,\n",
       " 29900,\n",
       " 29900,\n",
       " 29906,\n",
       " 322,\n",
       " 471,\n",
       " 21201,\n",
       " 297,\n",
       " 29871,\n",
       " 29906,\n",
       " 29900,\n",
       " 29900,\n",
       " 29941,\n",
       " 29889,\n",
       " 940,\n",
       " 5318,\n",
       " 1784,\n",
       " 7087,\n",
       " 2745,\n",
       " 29871,\n",
       " 29906,\n",
       " 29900,\n",
       " 29900,\n",
       " 29945,\n",
       " 29889,\n",
       " 512,\n",
       " 3839,\n",
       " 29871,\n",
       " 29906,\n",
       " 29900,\n",
       " 29900,\n",
       " 29945,\n",
       " 29892,\n",
       " 540,\n",
       " 6153,\n",
       " 304,\n",
       " 278,\n",
       " 435,\n",
       " 29906,\n",
       " 4402,\n",
       " 4526,\n",
       " 287,\n",
       " 601,\n",
       " 22740,\n",
       " 351,\n",
       " 532,\n",
       " 29889,\n",
       " 512,\n",
       " 29871,\n",
       " 29906,\n",
       " 29900,\n",
       " 29900,\n",
       " 29953,\n",
       " 29892,\n",
       " 540,\n",
       " 6153,\n",
       " 304,\n",
       " 278,\n",
       " 435,\n",
       " 29906,\n",
       " 4402,\n",
       " 478,\n",
       " 790,\n",
       " 295,\n",
       " 476,\n",
       " 16945,\n",
       " 29889,\n",
       " 8512,\n",
       " 540,\n",
       " 3897,\n",
       " 263,\n",
       " 4943,\n",
       " 4847,\n",
       " 408,\n",
       " 263,\n",
       " 822,\n",
       " 6270,\n",
       " 7145,\n",
       " 29888,\n",
       " 709,\n",
       " 672,\n",
       " 29892,\n",
       " 670,\n",
       " 22020,\n",
       " 471,\n",
       " 5318,\n",
       " 3109,\n",
       " 2645,\n",
       " 278,\n",
       " 11801,\n",
       " 29889,\n",
       " 512,\n",
       " 29871,\n",
       " 29906,\n",
       " 29900,\n",
       " 29900,\n",
       " 29955,\n",
       " 29892,\n",
       " 540,\n",
       " 6153,\n",
       " 304,\n",
       " 278,\n",
       " 5546,\n",
       " 8914,\n",
       " 5165,\n",
       " 4402,\n",
       " 5678,\n",
       " 578,\n",
       " 476,\n",
       " 398,\n",
       " 314,\n",
       " 3747,\n",
       " 313,\n",
       " 29880,\n",
       " 1008,\n",
       " 1528,\n",
       " 9979,\n",
       " 476,\n",
       " 398,\n",
       " 314,\n",
       " 3747,\n",
       " 29897,\n",
       " 2729,\n",
       " 297,\n",
       " 670,\n",
       " 1887,\n",
       " 5120,\n",
       " 29889,\n",
       " 940,\n",
       " 5318,\n",
       " 408,\n",
       " 263,\n",
       " 4943,\n",
       " 4847,\n",
       " 322,\n",
       " 278,\n",
       " 4402,\n",
       " 471,\n",
       " 21201,\n",
       " 304,\n",
       " 435,\n",
       " 29906,\n",
       " 297,\n",
       " 29871,\n",
       " 29906,\n",
       " 29900,\n",
       " 29900,\n",
       " 29947,\n",
       " 29889,\n",
       " 8512,\n",
       " 540,\n",
       " 1258,\n",
       " 451,\n",
       " 1708,\n",
       " 408,\n",
       " 1568,\n",
       " 29892,\n",
       " 540,\n",
       " 1603,\n",
       " 5318,\n",
       " 297,\n",
       " 1784,\n",
       " 7087,\n",
       " 29889,\n",
       " 512,\n",
       " 29871,\n",
       " 29906,\n",
       " 29900,\n",
       " 29896,\n",
       " 29900,\n",
       " 29892,\n",
       " 540,\n",
       " 6153,\n",
       " 304,\n",
       " 16704,\n",
       " 423,\n",
       " 322,\n",
       " 8772,\n",
       " 9034,\n",
       " 3100,\n",
       " 12718,\n",
       " 549,\n",
       " 273,\n",
       " 29889,\n",
       " 512,\n",
       " 5468,\n",
       " 29871,\n",
       " 29906,\n",
       " 29900,\n",
       " 29896,\n",
       " 29900,\n",
       " 29892,\n",
       " 540,\n",
       " 4133,\n",
       " 304,\n",
       " 5546,\n",
       " 322,\n",
       " 8772,\n",
       " 278,\n",
       " 435,\n",
       " 29906,\n",
       " 4402,\n",
       " 402,\n",
       " 3055,\n",
       " 3703,\n",
       " 29920,\n",
       " 26240,\n",
       " 557,\n",
       " 29891,\n",
       " 1878,\n",
       " 29884,\n",
       " 29889,\n",
       " 940,\n",
       " 5318,\n",
       " 4049,\n",
       " 408,\n",
       " 263,\n",
       " 822,\n",
       " 6270,\n",
       " 7145,\n",
       " 29888,\n",
       " 709,\n",
       " 672,\n",
       " 322,\n",
       " 4818,\n",
       " 1250,\n",
       " 2745,\n",
       " 29871,\n",
       " 29906,\n",
       " 29900,\n",
       " 29896,\n",
       " 29906,\n",
       " 746,\n",
       " 540,\n",
       " 16528,\n",
       " 29889,\n",
       " 13,\n",
       " 13,\n",
       " 2277,\n",
       " 29937,\n",
       " 13291,\n",
       " 13,\n",
       " 29911,\n",
       " 10730,\n",
       " 9940,\n",
       " 7107,\n",
       " 272,\n",
       " 1458,\n",
       " 471,\n",
       " 6345,\n",
       " 373,\n",
       " 5468,\n",
       " 29871,\n",
       " 29896,\n",
       " 29900,\n",
       " 29892,\n",
       " 29896,\n",
       " 29929,\n",
       " 29947,\n",
       " 29896,\n",
       " 29889,\n",
       " 2,\n",
       " 1,\n",
       " 835,\n",
       " 2799,\n",
       " 4080,\n",
       " 13,\n",
       " 3644,\n",
       " 306,\n",
       " 505,\n",
       " 901,\n",
       " 12785,\n",
       " 472,\n",
       " 278,\n",
       " 931,\n",
       " 310,\n",
       " 380,\n",
       " 12698,\n",
       " 403,\n",
       " 29892,\n",
       " 505,\n",
       " 306,\n",
       " 2113,\n",
       " 29973,\n",
       " 13,\n",
       " 13,\n",
       " 2277,\n",
       " 29937,\n",
       " 15228,\n",
       " 13,\n",
       " 855,\n",
       " 12698,\n",
       " 403,\n",
       " 338,\n",
       " 263,\n",
       " 6434,\n",
       " 297,\n",
       " 521,\n",
       " 404,\n",
       " 988,\n",
       " 278,\n",
       " 4847,\n",
       " 5069,\n",
       " 2507,\n",
       " 372,\n",
       " 338,\n",
       " 304,\n",
       " 4337,\n",
       " 338,\n",
       " 451,\n",
       " 297,\n",
       " 1423,\n",
       " 322,\n",
       " 756,\n",
       " 694,\n",
       " 11706,\n",
       " 4337,\n",
       " 29889,\n",
       " 624,\n",
       " 12698,\n",
       " 403,\n",
       " 2582,\n",
       " 297,\n",
       " 263,\n",
       " 4216,\n",
       " 29889,\n",
       " 7133,\n",
       " 278,\n",
       " 1095,\n",
       " 11802,\n",
       " 29892,\n",
       " 380,\n",
       " 12698,\n",
       " 403,\n",
       " 338,\n",
       " 263,\n",
       " 6503,\n",
       " 393,\n",
       " 508,\n",
       " 9025,\n",
       " 278,\n",
       " 4847,\n",
       " 411,\n",
       " 278,\n",
       " 20773,\n",
       " 2602,\n",
       " 304,\n",
       " 4216,\n",
       " 278,\n",
       " 3748,\n",
       " 3265,\n",
       " 1135,\n",
       " 14074,\n",
       " 29889,\n",
       " 512,\n",
       " 901,\n",
       " 4280,\n",
       " 11909,\n",
       " 29892,\n",
       " 380,\n",
       " 12698,\n",
       " 403,\n",
       " 338,\n",
       " 1568,\n",
       " 364,\n",
       " 279,\n",
       " 261,\n",
       " 29892,\n",
       " 5491,\n",
       " 5622,\n",
       " 278,\n",
       " 883,\n",
       " 310,\n",
       " 263,\n",
       " 2381,\n",
       " 513,\n",
       " 280,\n",
       " 393,\n",
       " 9269,\n",
       " 29879,\n",
       " 871,\n",
       " 565,\n",
       " 278,\n",
       " 11558,\n",
       " 2625,\n",
       " 338,\n",
       " 297,\n",
       " 1131,\n",
       " 296,\n",
       " 573,\n",
       " 7226,\n",
       " 29883,\n",
       " 7018,\n",
       " 4312,\n",
       " 29962,\n",
       " 624,\n",
       " 12698,\n",
       " 403,\n",
       " 338,\n",
       " 884,\n",
       " 263,\n",
       " 3619,\n",
       " 10929,\n",
       " 297,\n",
       " 1095,\n",
       " 11802,\n",
       " 11898,\n",
       " 322,\n",
       " 916,\n",
       " 521,\n",
       " 404,\n",
       " 4828,\n",
       " 29889,\n",
       " 13,\n",
       " 13,\n",
       " 1576,\n",
       " 21957,\n",
       " 310,\n",
       " 263,\n",
       " 380,\n",
       " 12698,\n",
       " 403,\n",
       " 471,\n",
       " 3918,\n",
       " 1891,\n",
       " 408,\n",
       " 263,\n",
       " 4216,\n",
       " 297,\n",
       " 278,\n",
       " 29871,\n",
       " 29896,\n",
       " 29929,\n",
       " 386,\n",
       " 6462,\n",
       " 29889,\n",
       " 10949,\n",
       " 445,\n",
       " 3918,\n",
       " 2133,\n",
       " 29892,\n",
       " 967,\n",
       " 14502,\n",
       " 23821,\n",
       " 17644,\n",
       " 29892,\n",
       " 3704,\n",
       " 1641,\n",
       " 316,\n",
       " 22580,\n",
       " 263,\n",
       " 5401,\n",
       " 363,\n",
       " 278,\n",
       " 380,\n",
       " 12698,\n",
       " 1218,\n",
       " 4847,\n",
       " 29892,\n",
       " 263,\n",
       " 4203,\n",
       " 29899,\n",
       " 5080,\n",
       " 363,\n",
       " 393,\n",
       " 4847,\n",
       " 29892,\n",
       " 470,\n",
       " 263,\n",
       " 6410,\n",
       " 363,\n",
       " 393,\n",
       " 4847,\n",
       " 29936,\n",
       " 451,\n",
       " 1641,\n",
       " 21905,\n",
       " 29936,\n",
       " 322,\n",
       " 9819,\n",
       " 297,\n",
       " 278,\n",
       " 380,\n",
       " 12698,\n",
       " 630,\n",
       " 4847,\n",
       " 4567,\n",
       " 263,\n",
       " 2507,\n",
       " 29889,\n",
       " 624,\n",
       " 12698,\n",
       " 403,\n",
       " 6865,\n",
       " 13100,\n",
       " 297,\n",
       " 916,\n",
       " 8090,\n",
       " 310,\n",
       " 278,\n",
       " 521,\n",
       " 404,\n",
       " 3942,\n",
       " 29889,\n",
       " 13,\n",
       " 13,\n",
       " 2277,\n",
       " 29937,\n",
       " 13291,\n",
       " 13,\n",
       " 3782,\n",
       " 29889,\n",
       " 29871,\n",
       " 13,\n",
       " 855,\n",
       " 12698,\n",
       " 403,\n",
       " 338,\n",
       " 263,\n",
       " 12061,\n",
       " 2602,\n",
       " 29889,\n",
       " 739,\n",
       " 1838,\n",
       " 29915,\n",
       " 29873,\n",
       " 4383,\n",
       " 1058,\n",
       " 756,\n",
       " 15468,\n",
       " 901,\n",
       " 12785,\n",
       " 470,\n",
       " 338,\n",
       " 297,\n",
       " 263,\n",
       " 15613,\n",
       " 2602,\n",
       " 2,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset[0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ### Instruction\n",
      "When did Virgin Australia start operating?\n",
      "\n",
      "### Context\n",
      "Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\n",
      "\n",
      "### Response\n",
      "Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(lm_dataset[0]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = iter(lm_dataset)\n",
    "y = next(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ### Instruction\n",
      "When did Virgin Australia start operating?\n",
      "\n",
      "### Context\n",
      "Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\n",
      "\n",
      "### Response\n",
      "Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.</s><s> ### Instruction\n",
      "Which is a species of fish? Tope or Rope\n",
      "\n",
      "### Response\n",
      "Tope</s><s> ### Instruction\n",
      "Why can camels survive for long without water?\n",
      "\n",
      "### Response\n",
      "Camels use the fat in their humps to keep them filled with energy and hydration for long periods of time.</s><s> ### Instruction\n",
      "Alice's parents have three daughters: Amy, Jessy, and what’s the name of the third daughter?\n",
      "\n",
      "### Response\n",
      "The name of the third daughter is Alice</s><s> ### Instruction\n",
      "When was Tomoaki Komorida born?\n",
      "\n",
      "### Context\n",
      "Komorida was born in Kumamoto Prefecture on July 10, 1981. After graduating from high school, he joined the J1 League club Avispa Fukuoka in 2000. Although he debuted as a midfielder in 2001, he did not play much and the club was relegated to the J2 League at the end of the 2001 season. In 2002, he moved to the J2 club Oita Trinita. He became a regular player as a defensive midfielder and the club won the championship in 2002 and was promoted in 2003. He played many matches until 2005. In September 2005, he moved to the J2 club Montedio Yamagata. In 2006, he moved to the J2 club Vissel Kobe. Although he became a regular player as a defensive midfielder, his gradually was played less during the summer. In 2007, he moved to the Japan Football League club Rosso Kumamoto (later Roasso Kumamoto) based in his local region. He played as a regular player and the club was promoted to J2 in 2008. Although he did not play as much, he still played in many matches. In 2010, he moved to Indonesia and joined Persela Lamongan. In July 2010, he returned to Japan and joined the J2 club Giravanz Kitakyushu. He played often as a defensive midfielder and center back until 2012 when he retired.\n",
      "\n",
      "### Response\n",
      "Tomoaki Komorida was born on July 10,1981.</s><s> ### Instruction\n",
      "If I have more pieces at the time of stalemate, have I won?\n",
      "\n",
      "### Context\n",
      "Stalemate is a situation in chess where the player whose turn it is to move is not in check and has no legal move. Stalemate results in a draw. During the endgame, stalemate is a resource that can enable the player with the inferior position to draw the game rather than lose. In more complex positions, stalemate is much rarer, usually taking the form of a swindle that succeeds only if the superior side is inattentive.[citation needed] Stalemate is also a common theme in endgame studies and other chess problems.\n",
      "\n",
      "The outcome of a stalemate was standardized as a draw in the 19th century. Before this standardization, its treatment varied widely, including being deemed a win for the stalemating player, a half-win for that player, or a loss for that player; not being permitted; and resulting in the stalemated player missing a turn. Stalemate rules vary in other games of the chess family.\n",
      "\n",
      "### Response\n",
      "No. \n",
      "Stalemate is a drawn position. It doesn't matter who has captured more pieces or is in a winning position</s><s> ### Instruction\n",
      "Given a reference text about Lollapalooza, where does it take place, who started it and what is it?\n",
      "\n",
      "### Context\n",
      "Lollapalooza /ˌlɒləpəˈluːzə/ (Lolla) is an annual American four-day music festival held in Grant Park in Chicago. It originally started as a touring event in 1991, but several years later, Chicago became its permanent location. Music genres include but are not limited to alternative rock, heavy metal, punk rock, hip hop, and electronic dance music. Lollapalooza has also featured visual arts, nonprofit organizations, and political organizations. The festival, held in Grant Park, hosts an estimated 400,000 people each July and sells out annually. Lollapalooza is one of the largest and most iconic music festivals in the world and one of the longest-running in the United States.\n",
      "\n",
      "Lollapalooza was conceived and created in 1991 as a farewell tour by Perry Farrell, singer of the group Jane's Addiction.\n",
      "\n",
      "### Response\n",
      "Lollapalooze is an annual musical festival held in Grant Park in Chicago, Illinois. It was started in 1991 as a farewell tour by Perry Farrell, singe of the group Jane's Addiction. The festival includes an array of musical genres including alternative rock, heavy metal, punk rock, hip hop, and electronic dance music. The festivals welcomes an estimated 400,000 people each year and sells out annually. Some notable headliners include: the Red Hot Chili Peppers, Chance the Rapper, Metallica, and Lady Gage. Lollapalooza is one of the largest and most iconic festivals in the world and a staple of Chicago.</s><s> ### Instruction\n",
      "Who gave the UN the land in NY to build their HQ\n",
      "\n",
      "### Response\n",
      "John D Rockerfeller</s><s> ### Instruction\n",
      "Why mobile is bad for human\n",
      "\n",
      "### Response\n",
      "We are always engaged one phone which is not good.</s><s> ### Instruction\n",
      "Who was John Moses Browning?\n",
      "\n",
      "### Context\n",
      "John Moses Browning (January 23, 1855 – November 26, 1926) was an American firearm designer who developed many varieties of military and civilian firearms, cartridges, and gun mechanisms – many of which are still in use around the world. He made his first firearm at age 13 in his father's gun shop and was awarded the first of his 128 firearm patents on October 7, 1879, at the age of 24. He is regarded as one of the most successful firearms designers of the 19th and 20th centuries and pioneered the development of modern repeating, semi-automatic, and automatic firearms.\n",
      "\n",
      "Browning influenced nearly all categories of firearms design, especially the autoloading of ammunition. He invented, or made significant improvements to, single-shot, lever-action, and pump-action rifles and shotguns. He developed the first reliable and compact autoloading pistols by inventing the telescoping bolt, then integrating the bolt and barrel shroud into what is known as the pistol slide. Browning's telescoping bolt design is now found on nearly every modern semi-automatic pistol, as well as several modern fully automatic weapons. He also developed the first gas-operated firearm, the Colt–Browning Model 1895 machine gun – a system that surpassed mechanical recoil operation to become the standard for most high-power self-loading firearm designs worldwide. He also made significant contributions to automatic cannon development.\n",
      "\n",
      "Browning's most successful designs include the M1911 pistol, the water-cooled M1917, the air-cooled M1919, and heavy M2 machine guns, the M1918 Browning Automatic Rifle, and the Browning Auto-5 – the first semi-automatic shotgun. Some of these arms are still manufactured, often with only minor changes in detail and cosmetics to those assembled by Browning or his licensees. The Browning-designed M1911 and Hi-Power are some of the most copied firearms in the world.\n",
      "\n",
      "### Response\n",
      "John Moses Browning is one of the most well-known designer of modern firearms.  He started building firearms in his father's shop at\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(y['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ### Instruction\n",
      "When did Virgin Australia start operating?\n",
      "\n",
      "### Context\n",
      "Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\n",
      "\n",
      "### Response\n",
      "Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.</s><s> ### Instruction\n",
      "Which is a species of fish? Tope or Rope\n",
      "\n",
      "### Response\n",
      "Tope</s><s> ### Instruction\n",
      "Why can camels survive for long without water?\n",
      "\n",
      "### Response\n",
      "Camels use the fat in their humps to keep them filled with energy and hydration for long periods of time.</s><s> ### Instruction\n",
      "Alice's parents have three daughters: Amy, Jessy, and what’s the name of the third daughter?\n",
      "\n",
      "### Response\n",
      "The name of the third daughter is Alice</s><s> ### Instruction\n",
      "When was Tomoaki Komorida born?\n",
      "\n",
      "### Context\n",
      "Komorida was born in Kumamoto Prefecture on July 10, 1981. After graduating from high school, he joined the J1 League club Avispa Fukuoka in 2000. Although he debuted as a midfielder in 2001, he did not play much and the club was relegated to the J2 League at the end of the 2001 season. In 2002, he moved to the J2 club Oita Trinita. He became a regular player as a defensive midfielder and the club won the championship in 2002 and was promoted in 2003. He played many matches until 2005. In September 2005, he moved to the J2 club Montedio Yamagata. In 2006, he moved to the J2 club Vissel Kobe. Although he became a regular player as a defensive midfielder, his gradually was played less during the summer. In 2007, he moved to the Japan Football League club Rosso Kumamoto (later Roasso Kumamoto) based in his local region. He played as a regular player and the club was promoted to J2 in 2008. Although he did not play as much, he still played in many matches. In 2010, he moved to Indonesia and joined Persela Lamongan. In July 2010, he returned to Japan and joined the J2 club Giravanz Kitakyushu. He played often as a defensive midfielder and center back until 2012 when he retired.\n",
      "\n",
      "### Response\n",
      "Tomoaki Komorida was born on July 10,1981.</s><s> ### Instruction\n",
      "If I have more pieces at the time of stalemate, have I won?\n",
      "\n",
      "### Context\n",
      "Stalemate is a situation in chess where the player whose turn it is to move is not in check and has no legal move. Stalemate results in a draw. During the endgame, stalemate is a resource that can enable the player with the inferior position to draw the game rather than lose. In more complex positions, stalemate is much rarer, usually taking the form of a swindle that succeeds only if the superior side is inattentive.[citation needed] Stalemate is also a common theme in endgame studies and other chess problems.\n",
      "\n",
      "The outcome of a stalemate was standardized as a draw in the 19th century. Before this standardization, its treatment varied widely, including being deemed a win for the stalemating player, a half-win for that player, or a loss for that player; not being permitted; and resulting in the stalemated player missing a turn. Stalemate rules vary in other games of the chess family.\n",
      "\n",
      "### Response\n",
      "No. \n",
      "Stalemate is a drawn position. It doesn't matter who has captured more pieces or is in a winning position</s><s> ### Instruction\n",
      "Given a reference text about Lollapalooza, where does it take place, who started it and what is it?\n",
      "\n",
      "### Context\n",
      "Lollapalooza /ˌlɒləpəˈluːzə/ (Lolla) is an annual American four-day music festival held in Grant Park in Chicago. It originally started as a touring event in 1991, but several years later, Chicago became its permanent location. Music genres include but are not limited to alternative rock, heavy metal, punk rock, hip hop, and electronic dance music. Lollapalooza has also featured visual arts, nonprofit organizations, and political organizations. The festival, held in Grant Park, hosts an estimated 400,000 people each July and sells out annually. Lollapalooza is one of the largest and most iconic music festivals in the world and one of the longest-running in the United States.\n",
      "\n",
      "Lollapalooza was conceived and created in 1991 as a farewell tour by Perry Farrell, singer of the group Jane's Addiction.\n",
      "\n",
      "### Response\n",
      "Lollapalooze is an annual musical festival held in Grant Park in Chicago, Illinois. It was started in 1991 as a farewell tour by Perry Farrell, singe of the group Jane's Addiction. The festival includes an array of musical genres including alternative rock, heavy metal, punk rock, hip hop, and electronic dance music. The festivals welcomes an estimated 400,000 people each year and sells out annually. Some notable headliners include: the Red Hot Chili Peppers, Chance the Rapper, Metallica, and Lady Gage. Lollapalooza is one of the largest and most iconic festivals in the world and a staple of Chicago.</s><s> ### Instruction\n",
      "Who gave the UN the land in NY to build their HQ\n",
      "\n",
      "### Response\n",
      "John D Rockerfeller</s><s> ### Instruction\n",
      "Why mobile is bad for human\n",
      "\n",
      "### Response\n",
      "We are always engaged one phone which is not good.</s><s> ### Instruction\n",
      "Who was John Moses Browning?\n",
      "\n",
      "### Context\n",
      "John Moses Browning (January 23, 1855 – November 26, 1926) was an American firearm designer who developed many varieties of military and civilian firearms, cartridges, and gun mechanisms – many of which are still in use around the world. He made his first firearm at age 13 in his father's gun shop and was awarded the first of his 128 firearm patents on October 7, 1879, at the age of 24. He is regarded as one of the most successful firearms designers of the 19th and 20th centuries and pioneered the development of modern repeating, semi-automatic, and automatic firearms.\n",
      "\n",
      "Browning influenced nearly all categories of firearms design, especially the autoloading of ammunition. He invented, or made significant improvements to, single-shot, lever-action, and pump-action rifles and shotguns. He developed the first reliable and compact autoloading pistols by inventing the telescoping bolt, then integrating the bolt and barrel shroud into what is known as the pistol slide. Browning's telescoping bolt design is now found on nearly every modern semi-automatic pistol, as well as several modern fully automatic weapons. He also developed the first gas-operated firearm, the Colt–Browning Model 1895 machine gun – a system that surpassed mechanical recoil operation to become the standard for most high-power self-loading firearm designs worldwide. He also made significant contributions to automatic cannon development.\n",
      "\n",
      "Browning's most successful designs include the M1911 pistol, the water-cooled M1917, the air-cooled M1919, and heavy M2 machine guns, the M1918 Browning Automatic Rifle, and the Browning Auto-5 – the first semi-automatic shotgun. Some of these arms are still manufactured, often with only minor changes in detail and cosmetics to those assembled by Browning or his licensees. The Browning-designed M1911 and Hi-Power are some of the most copied firearms in the world.\n",
      "\n",
      "### Response\n",
      "John Moses Browning is one of the most well-known designer of modern firearms.  He started building firearms in his father's shop at\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(y['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ffeb070d66431f8dab88f6c7bc6af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1581 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded data to:\n",
      "training dataset to: s3://innovationdatasets/processed/llama/dolly/train\n"
     ]
    }
   ],
   "source": [
    "#s3://innovationdatasets/dolly/\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://innovationdatasets/processed/llama/dolly/train'\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    " \n",
    "print(\"uploaded data to:\")\n",
    "print(f\"training dataset to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    " \n",
    "# define Training Job Name\n",
    "job_name = f'huggingface-qlora-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    " \n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  'epochs': 2,                                      # number of training epochs\n",
    "  'per_device_train_batch_size': 3,                 # batch size for training\n",
    "  'lr': 2e-4,                                       # learning rate used during training\n",
    "  'hf_token': HfFolder.get_token(),                 # huggingface token to access llama 2\n",
    "  'merge_weights': True,                            # wether to merge LoRA into the model (needs more memory)\n",
    "}\n",
    " \n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_clm.py',      # train script\n",
    "    source_dir           = 'scripts',         # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.2xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-qlora-2024-07-14-17-32-23-2024-07-14-12-02-26-208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-14 12:02:27 Starting - Starting the training job......\n",
      "2024-07-14 12:03:19 Starting - Preparing the instances for training...\n",
      "2024-07-14 12:04:00 Downloading - Downloading the training image...........................\n",
      "2024-07-14 12:08:13 Training - Training image download completed. Training in progress..bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2024-07-14 12:08:32,338 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2024-07-14 12:08:32,355 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-07-14 12:08:32,365 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2024-07-14 12:08:32,366 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2024-07-14 12:08:33,732 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "/opt/conda/bin/python3.10 -m pip install -r requirements.txt\n",
      "Collecting transformers==4.31.0 (from -r requirements.txt (line 1))\n",
      "Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116.9/116.9 kB 16.6 MB/s eta 0:00:00\n",
      "Collecting peft==0.4.0 (from -r requirements.txt (line 2))\n",
      "Downloading peft-0.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting accelerate==0.21.0 (from -r requirements.txt (line 3))\n",
      "Downloading accelerate-0.21.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting bitsandbytes==0.40.2 (from -r requirements.txt (line 4))\n",
      "Downloading bitsandbytes-0.40.2-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting safetensors>=0.3.1 (from -r requirements.txt (line 5))\n",
      "Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.13.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (4.65.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 2)) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 2)) (2.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0->-r requirements.txt (line 1)) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0->-r requirements.txt (line 1)) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (1.3.0)\n",
      "Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/7.4 MB 83.8 MB/s eta 0:00:00\n",
      "Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.9/72.9 kB 12.2 MB/s eta 0:00:00\n",
      "Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 244.2/244.2 kB 31.1 MB/s eta 0:00:00\n",
      "Downloading bitsandbytes-0.40.2-py3-none-any.whl (92.5 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.5/92.5 MB 20.3 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 61.6 MB/s eta 0:00:00\n",
      "Installing collected packages: bitsandbytes, safetensors, transformers, accelerate, peft\n",
      "Attempting uninstall: transformers\n",
      "Found existing installation: transformers 4.28.1\n",
      "Uninstalling transformers-4.28.1:\n",
      "Successfully uninstalled transformers-4.28.1\n",
      "Attempting uninstall: accelerate\n",
      "Found existing installation: accelerate 0.19.0\n",
      "Uninstalling accelerate-0.19.0:\n",
      "Successfully uninstalled accelerate-0.19.0\n",
      "Successfully installed accelerate-0.21.0 bitsandbytes-0.40.2 peft-0.4.0 safetensors-0.4.3 transformers-4.31.0\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "2024-07-14 12:08:45,543 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2024-07-14 12:08:45,543 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2024-07-14 12:08:45,584 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-07-14 12:08:45,611 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-07-14 12:08:45,638 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-07-14 12:08:45,649 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"dataset_path\": \"/opt/ml/input/data/training\",\n",
      "        \"epochs\": 2,\n",
      "        \"hf_token\": \"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\",\n",
      "        \"lr\": 0.0002,\n",
      "        \"merge_weights\": true,\n",
      "        \"model_id\": \"meta-llama/Llama-2-7b-hf\",\n",
      "        \"per_device_train_batch_size\": 3\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-qlora-2024-07-14-17-32-23-2024-07-14-12-02-26-208\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-south-1-570517415597/huggingface-qlora-2024-07-14-17-32-23-2024-07-14-12-02-26-208/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_clm\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_clm.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"dataset_path\":\"/opt/ml/input/data/training\",\"epochs\":2,\"hf_token\":\"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\",\"lr\":0.0002,\"merge_weights\":true,\"model_id\":\"meta-llama/Llama-2-7b-hf\",\"per_device_train_batch_size\":3}\n",
      "SM_USER_ENTRY_POINT=run_clm.py\n",
      "SM_FRAMEWORK_PARAMS={}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"training\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.g5.2xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=run_clm\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=8\n",
      "SM_NUM_GPUS=1\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-ap-south-1-570517415597/huggingface-qlora-2024-07-14-17-32-23-2024-07-14-12-02-26-208/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"dataset_path\":\"/opt/ml/input/data/training\",\"epochs\":2,\"hf_token\":\"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\",\"lr\":0.0002,\"merge_weights\":true,\"model_id\":\"meta-llama/Llama-2-7b-hf\",\"per_device_train_batch_size\":3},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-qlora-2024-07-14-17-32-23-2024-07-14-12-02-26-208\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-south-1-570517415597/huggingface-qlora-2024-07-14-17-32-23-2024-07-14-12-02-26-208/source/sourcedir.tar.gz\",\"module_name\":\"run_clm\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_clm.py\"}\n",
      "SM_USER_ARGS=[\"--dataset_path\",\"/opt/ml/input/data/training\",\"--epochs\",\"2\",\"--hf_token\",\"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\",\"--lr\",\"0.0002\",\"--merge_weights\",\"True\",\"--model_id\",\"meta-llama/Llama-2-7b-hf\",\"--per_device_train_batch_size\",\"3\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "SM_HP_DATASET_PATH=/opt/ml/input/data/training\n",
      "SM_HP_EPOCHS=2\n",
      "SM_HP_HF_TOKEN=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\n",
      "SM_HP_LR=0.0002\n",
      "SM_HP_MERGE_WEIGHTS=true\n",
      "SM_HP_MODEL_ID=meta-llama/Llama-2-7b-hf\n",
      "SM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=3\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\n",
      "Invoking script with the following command:\n",
      "/opt/conda/bin/python3.10 run_clm.py --dataset_path /opt/ml/input/data/training --epochs 2 --hf_token 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 --lr 0.0002 --merge_weights True --model_id meta-llama/Llama-2-7b-hf --per_device_train_batch_size 3\n",
      "2024-07-14 12:08:45,680 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\n",
      "Logging into the Hugging Face Hub with token hf_OSGMsUT...\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n",
      "config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]\n",
      "config.json: 100%|██████████| 609/609 [00:00<00:00, 4.19MB/s]\n",
      "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]\n",
      "model.safetensors.index.json: 100%|██████████| 26.8k/26.8k [00:00<00:00, 100MB/s]\n",
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]#033[A\n",
      "model-00001-of-00002.safetensors:   0%|          | 41.9M/9.98G [00:00<00:28, 349MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   1%|          | 83.9M/9.98G [00:00<00:27, 360MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   1%|▏         | 126M/9.98G [00:00<00:27, 362MB/s] #033[A\n",
      "model-00001-of-00002.safetensors:   2%|▏         | 168M/9.98G [00:00<00:26, 364MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   2%|▏         | 210M/9.98G [00:00<00:26, 364MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   3%|▎         | 252M/9.98G [00:00<00:26, 364MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   3%|▎         | 294M/9.98G [00:00<00:26, 366MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   3%|▎         | 336M/9.98G [00:00<00:26, 363MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   4%|▍         | 377M/9.98G [00:01<00:26, 362MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   4%|▍         | 419M/9.98G [00:01<00:26, 361MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   5%|▍         | 461M/9.98G [00:01<00:26, 361MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   5%|▌         | 503M/9.98G [00:01<00:26, 362MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   5%|▌         | 545M/9.98G [00:01<00:26, 360MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   6%|▌         | 587M/9.98G [00:01<00:26, 359MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   6%|▋         | 629M/9.98G [00:01<00:25, 360MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   7%|▋         | 671M/9.98G [00:01<00:25, 363MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   7%|▋         | 713M/9.98G [00:01<00:25, 365MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   8%|▊         | 755M/9.98G [00:02<00:25, 365MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   8%|▊         | 797M/9.98G [00:02<00:25, 364MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   8%|▊         | 839M/9.98G [00:02<00:25, 364MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   9%|▉         | 881M/9.98G [00:02<00:24, 365MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   9%|▉         | 923M/9.98G [00:02<00:24, 366MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  10%|▉         | 965M/9.98G [00:02<00:24, 367MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  10%|█         | 1.01G/9.98G [00:02<00:24, 368MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  11%|█         | 1.05G/9.98G [00:02<00:24, 368MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  11%|█         | 1.09G/9.98G [00:02<00:24, 369MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  11%|█▏        | 1.13G/9.98G [00:03<00:23, 372MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  12%|█▏        | 1.17G/9.98G [00:03<00:23, 373MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  12%|█▏        | 1.22G/9.98G [00:03<00:23, 370MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  13%|█▎        | 1.26G/9.98G [00:03<00:23, 368MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  13%|█▎        | 1.30G/9.98G [00:03<00:23, 366MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  13%|█▎        | 1.34G/9.98G [00:03<00:23, 364MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  14%|█▍        | 1.38G/9.98G [00:03<00:23, 364MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  14%|█▍        | 1.43G/9.98G [00:03<00:23, 363MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  15%|█▍        | 1.47G/9.98G [00:04<00:23, 364MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  15%|█▌        | 1.51G/9.98G [00:04<00:23, 364MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  16%|█▌        | 1.55G/9.98G [00:04<00:23, 364MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  16%|█▌        | 1.59G/9.98G [00:04<00:22, 365MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  16%|█▋        | 1.64G/9.98G [00:04<00:26, 319MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  17%|█▋        | 1.68G/9.98G [00:04<00:25, 329MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  17%|█▋        | 1.72G/9.98G [00:04<00:24, 339MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  18%|█▊        | 1.76G/9.98G [00:04<00:23, 345MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  18%|█▊        | 1.80G/9.98G [00:05<00:23, 351MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  18%|█▊        | 1.85G/9.98G [00:05<00:22, 355MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  19%|█▉        | 1.89G/9.98G [00:05<00:22, 358MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  19%|█▉        | 1.93G/9.98G [00:05<00:22, 359MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  20%|█▉        | 1.97G/9.98G [00:05<00:22, 360MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  20%|██        | 2.01G/9.98G [00:05<00:22, 360MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  21%|██        | 2.06G/9.98G [00:05<00:22, 359MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  21%|██        | 2.10G/9.98G [00:05<00:21, 360MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  21%|██▏       | 2.14G/9.98G [00:05<00:21, 360MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  22%|██▏       | 2.18G/9.98G [00:06<00:21, 360MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  22%|██▏       | 2.22G/9.98G [00:06<00:21, 359MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  23%|██▎       | 2.26G/9.98G [00:06<00:21, 357MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  23%|██▎       | 2.31G/9.98G [00:06<00:21, 358MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  24%|██▎       | 2.35G/9.98G [00:06<00:21, 357MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  24%|██▍       | 2.39G/9.98G [00:06<00:21, 358MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  24%|██▍       | 2.43G/9.98G [00:06<00:21, 359MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  25%|██▍       | 2.47G/9.98G [00:06<00:20, 359MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  25%|██▌       | 2.52G/9.98G [00:06<00:20, 359MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  26%|██▌       | 2.56G/9.98G [00:07<00:20, 359MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  26%|██▌       | 2.60G/9.98G [00:07<00:20, 360MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  26%|██▋       | 2.64G/9.98G [00:07<00:20, 360MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  27%|██▋       | 2.68G/9.98G [00:07<00:20, 361MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  27%|██▋       | 2.73G/9.98G [00:07<00:20, 360MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  28%|██▊       | 2.77G/9.98G [00:07<00:20, 360MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  28%|██▊       | 2.81G/9.98G [00:07<00:19, 361MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  29%|██▊       | 2.85G/9.98G [00:07<00:19, 361MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  29%|██▉       | 2.89G/9.98G [00:08<00:19, 362MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  29%|██▉       | 2.94G/9.98G [00:08<00:19, 363MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  30%|██▉       | 2.98G/9.98G [00:08<00:19, 363MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  30%|███       | 3.02G/9.98G [00:08<00:19, 365MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  31%|███       | 3.06G/9.98G [00:08<00:18, 366MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  31%|███       | 3.10G/9.98G [00:08<00:18, 365MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  32%|███▏      | 3.15G/9.98G [00:08<00:18, 364MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  32%|███▏      | 3.19G/9.98G [00:08<00:18, 365MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  32%|███▏      | 3.23G/9.98G [00:08<00:18, 365MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  33%|███▎      | 3.27G/9.98G [00:09<00:18, 365MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  33%|███▎      | 3.31G/9.98G [00:09<00:18, 365MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  34%|███▎      | 3.36G/9.98G [00:09<00:18, 365MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  34%|███▍      | 3.40G/9.98G [00:09<00:18, 365MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  34%|███▍      | 3.44G/9.98G [00:09<00:17, 365MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  35%|███▍      | 3.48G/9.98G [00:09<00:17, 364MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  35%|███▌      | 3.52G/9.98G [00:09<00:17, 360MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  36%|███▌      | 3.57G/9.98G [00:09<00:17, 359MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  36%|███▌      | 3.61G/9.98G [00:09<00:17, 359MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  37%|███▋      | 3.65G/9.98G [00:10<00:27, 230MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  37%|███▋      | 3.69G/9.98G [00:10<00:24, 258MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  37%|███▋      | 3.73G/9.98G [00:10<00:22, 281MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  38%|███▊      | 3.77G/9.98G [00:10<00:20, 300MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  38%|███▊      | 3.82G/9.98G [00:10<00:19, 313MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  39%|███▊      | 3.86G/9.98G [00:10<00:19, 321MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  39%|███▉      | 3.90G/9.98G [00:11<00:18, 329MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  40%|███▉      | 3.94G/9.98G [00:11<00:17, 337MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  40%|███▉      | 3.98G/9.98G [00:11<00:17, 342MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  40%|████      | 4.03G/9.98G [00:11<00:17, 337MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  41%|████      | 4.07G/9.98G [00:11<00:17, 340MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  41%|████      | 4.11G/9.98G [00:11<00:17, 341MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  42%|████▏     | 4.15G/9.98G [00:11<00:16, 348MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  42%|████▏     | 4.19G/9.98G [00:11<00:16, 351MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  42%|████▏     | 4.24G/9.98G [00:12<00:16, 355MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  43%|████▎     | 4.28G/9.98G [00:12<00:16, 355MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  43%|████▎     | 4.32G/9.98G [00:12<00:15, 355MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  44%|████▎     | 4.36G/9.98G [00:12<00:15, 356MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  44%|████▍     | 4.40G/9.98G [00:12<00:15, 358MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  45%|████▍     | 4.45G/9.98G [00:12<00:15, 357MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  45%|████▍     | 4.49G/9.98G [00:12<00:15, 358MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  45%|████▌     | 4.53G/9.98G [00:12<00:15, 360MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  46%|████▌     | 4.57G/9.98G [00:12<00:14, 361MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  46%|████▌     | 4.61G/9.98G [00:13<00:14, 360MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  47%|████▋     | 4.66G/9.98G [00:13<00:14, 357MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  47%|████▋     | 4.70G/9.98G [00:13<00:14, 357MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  48%|████▊     | 4.74G/9.98G [00:13<00:14, 359MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  48%|████▊     | 4.78G/9.98G [00:13<00:14, 361MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  48%|████▊     | 4.82G/9.98G [00:13<00:14, 361MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  49%|████▉     | 4.87G/9.98G [00:13<00:14, 362MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  49%|████▉     | 4.91G/9.98G [00:13<00:14, 362MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  50%|████▉     | 4.95G/9.98G [00:13<00:13, 361MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  50%|█████     | 4.99G/9.98G [00:14<00:13, 361MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  50%|█████     | 5.03G/9.98G [00:14<00:13, 362MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  51%|█████     | 5.08G/9.98G [00:14<00:13, 362MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  51%|█████▏    | 5.12G/9.98G [00:14<00:13, 362MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  52%|█████▏    | 5.16G/9.98G [00:14<00:13, 361MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  52%|█████▏    | 5.20G/9.98G [00:14<00:13, 361MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  53%|█████▎    | 5.24G/9.98G [00:14<00:13, 362MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  53%|█████▎    | 5.28G/9.98G [00:14<00:12, 363MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  53%|█████▎    | 5.33G/9.98G [00:15<00:12, 364MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  54%|█████▍    | 5.37G/9.98G [00:15<00:14, 326MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  54%|█████▍    | 5.41G/9.98G [00:15<00:13, 338MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  55%|█████▍    | 5.45G/9.98G [00:15<00:13, 343MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  55%|█████▌    | 5.49G/9.98G [00:15<00:12, 349MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  55%|█████▌    | 5.54G/9.98G [00:15<00:12, 354MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  56%|█████▌    | 5.58G/9.98G [00:15<00:12, 358MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  56%|█████▋    | 5.62G/9.98G [00:15<00:12, 360MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  57%|█████▋    | 5.66G/9.98G [00:15<00:11, 362MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  57%|█████▋    | 5.70G/9.98G [00:16<00:11, 365MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  58%|█████▊    | 5.75G/9.98G [00:16<00:11, 365MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  58%|█████▊    | 5.79G/9.98G [00:16<00:11, 366MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  58%|█████▊    | 5.83G/9.98G [00:16<00:11, 366MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  59%|█████▉    | 5.87G/9.98G [00:16<00:11, 366MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  59%|█████▉    | 5.91G/9.98G [00:16<00:11, 365MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  60%|█████▉    | 5.96G/9.98G [00:16<00:10, 366MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  60%|██████    | 6.00G/9.98G [00:16<00:10, 367MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  61%|██████    | 6.04G/9.98G [00:17<00:10, 367MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  61%|██████    | 6.08G/9.98G [00:17<00:10, 367MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  61%|██████▏   | 6.12G/9.98G [00:17<00:10, 368MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  62%|██████▏   | 6.17G/9.98G [00:17<00:10, 370MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  62%|██████▏   | 6.21G/9.98G [00:17<00:10, 365MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  63%|██████▎   | 6.25G/9.98G [00:17<00:10, 367MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  63%|██████▎   | 6.29G/9.98G [00:17<00:10, 366MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  63%|██████▎   | 6.33G/9.98G [00:17<00:10, 364MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  64%|██████▍   | 6.38G/9.98G [00:17<00:09, 363MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  64%|██████▍   | 6.42G/9.98G [00:18<00:09, 365MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  65%|██████▍   | 6.46G/9.98G [00:18<00:09, 365MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  65%|██████▌   | 6.50G/9.98G [00:18<00:09, 364MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  66%|██████▌   | 6.54G/9.98G [00:18<00:09, 366MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  66%|██████▌   | 6.59G/9.98G [00:18<00:09, 347MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  66%|██████▋   | 6.63G/9.98G [00:18<00:09, 352MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  67%|██████▋   | 6.67G/9.98G [00:18<00:09, 347MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  67%|██████▋   | 6.71G/9.98G [00:18<00:09, 352MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  68%|██████▊   | 6.75G/9.98G [00:19<00:09, 355MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  68%|██████▊   | 6.79G/9.98G [00:19<00:08, 359MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  69%|██████▊   | 6.84G/9.98G [00:19<00:10, 298MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  69%|██████▉   | 6.88G/9.98G [00:19<00:11, 264MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  69%|██████▉   | 6.91G/9.98G [00:19<00:12, 247MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  70%|██████▉   | 6.94G/9.98G [00:19<00:12, 237MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  70%|██████▉   | 6.97G/9.98G [00:19<00:13, 226MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  70%|███████   | 7.00G/9.98G [00:20<00:13, 220MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  71%|███████   | 7.04G/9.98G [00:20<00:18, 157MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  71%|███████   | 7.08G/9.98G [00:20<00:14, 194MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  71%|███████▏  | 7.12G/9.98G [00:20<00:12, 229MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  72%|███████▏  | 7.16G/9.98G [00:20<00:10, 260MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  72%|███████▏  | 7.19G/9.98G [00:20<00:10, 262MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  72%|███████▏  | 7.22G/9.98G [00:21<00:11, 245MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  73%|███████▎  | 7.26G/9.98G [00:21<00:11, 233MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  73%|███████▎  | 7.29G/9.98G [00:21<00:11, 228MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  73%|███████▎  | 7.32G/9.98G [00:21<00:11, 225MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  74%|███████▎  | 7.35G/9.98G [00:21<00:11, 221MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  74%|███████▍  | 7.38G/9.98G [00:21<00:11, 220MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  74%|███████▍  | 7.41G/9.98G [00:21<00:11, 218MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  75%|███████▍  | 7.44G/9.98G [00:22<00:11, 214MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  75%|███████▍  | 7.48G/9.98G [00:22<00:11, 212MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  75%|███████▌  | 7.51G/9.98G [00:22<00:11, 214MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  76%|███████▌  | 7.54G/9.98G [00:22<00:11, 212MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  76%|███████▌  | 7.57G/9.98G [00:22<00:11, 211MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  76%|███████▌  | 7.60G/9.98G [00:22<00:11, 210MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  77%|███████▋  | 7.63G/9.98G [00:23<00:11, 210MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  77%|███████▋  | 7.67G/9.98G [00:23<00:10, 214MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  77%|███████▋  | 7.70G/9.98G [00:23<00:10, 212MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  77%|███████▋  | 7.73G/9.98G [00:23<00:10, 211MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  78%|███████▊  | 7.76G/9.98G [00:23<00:10, 210MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  78%|███████▊  | 7.79G/9.98G [00:23<00:10, 215MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  78%|███████▊  | 7.82G/9.98G [00:23<00:10, 213MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  79%|███████▊  | 7.85G/9.98G [00:24<00:09, 214MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  79%|███████▉  | 7.89G/9.98G [00:24<00:09, 213MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  79%|███████▉  | 7.92G/9.98G [00:24<00:09, 211MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  80%|███████▉  | 7.95G/9.98G [00:24<00:09, 210MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  80%|███████▉  | 7.98G/9.98G [00:24<00:09, 209MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  80%|████████  | 8.01G/9.98G [00:24<00:09, 214MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  81%|████████  | 8.04G/9.98G [00:24<00:09, 213MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  81%|████████  | 8.07G/9.98G [00:25<00:09, 196MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  81%|████████  | 8.11G/9.98G [00:25<00:08, 215MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  82%|████████▏ | 8.14G/9.98G [00:25<00:08, 207MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  82%|████████▏ | 8.17G/9.98G [00:25<00:09, 197MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  82%|████████▏ | 8.19G/9.98G [00:25<00:09, 190MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  82%|████████▏ | 8.21G/9.98G [00:25<00:09, 188MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  83%|████████▎ | 8.23G/9.98G [00:25<00:09, 183MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  83%|████████▎ | 8.25G/9.98G [00:26<00:09, 179MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  83%|████████▎ | 8.27G/9.98G [00:26<00:09, 175MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  83%|████████▎ | 8.29G/9.98G [00:26<00:09, 171MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  83%|████████▎ | 8.32G/9.98G [00:26<00:09, 176MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  84%|████████▎ | 8.34G/9.98G [00:26<00:09, 179MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  84%|████████▍ | 8.36G/9.98G [00:26<00:09, 168MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  84%|████████▍ | 8.38G/9.98G [00:26<00:09, 169MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  84%|████████▍ | 8.40G/9.98G [00:26<00:08, 176MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  84%|████████▍ | 8.42G/9.98G [00:27<00:09, 172MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  85%|████████▍ | 8.44G/9.98G [00:27<00:09, 163MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  85%|████████▍ | 8.46G/9.98G [00:27<00:09, 167MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  85%|████████▌ | 8.48G/9.98G [00:27<00:08, 172MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  85%|████████▌ | 8.50G/9.98G [00:27<00:08, 173MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  85%|████████▌ | 8.52G/9.98G [00:27<00:08, 166MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  86%|████████▌ | 8.55G/9.98G [00:27<00:08, 167MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  86%|████████▌ | 8.57G/9.98G [00:27<00:08, 173MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  86%|████████▌ | 8.59G/9.98G [00:28<00:07, 176MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  86%|████████▋ | 8.61G/9.98G [00:28<00:08, 168MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  87%|████████▋ | 8.63G/9.98G [00:28<00:07, 168MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  87%|████████▋ | 8.65G/9.98G [00:28<00:07, 166MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  87%|████████▋ | 8.67G/9.98G [00:28<00:07, 168MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  87%|████████▋ | 8.69G/9.98G [00:28<00:07, 167MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  87%|████████▋ | 8.71G/9.98G [00:28<00:07, 171MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  88%|████████▊ | 8.73G/9.98G [00:28<00:07, 173MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  88%|████████▊ | 8.76G/9.98G [00:29<00:07, 167MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  88%|████████▊ | 8.78G/9.98G [00:29<00:06, 174MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  88%|████████▊ | 8.80G/9.98G [00:29<00:06, 169MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  88%|████████▊ | 8.82G/9.98G [00:29<00:06, 168MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  89%|████████▊ | 8.84G/9.98G [00:29<00:06, 163MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  89%|████████▉ | 8.86G/9.98G [00:29<00:06, 167MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  89%|████████▉ | 8.88G/9.98G [00:29<00:06, 171MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  89%|████████▉ | 8.90G/9.98G [00:29<00:06, 172MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  89%|████████▉ | 8.92G/9.98G [00:30<00:06, 162MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  90%|████████▉ | 8.94G/9.98G [00:30<00:06, 169MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  90%|████████▉ | 8.97G/9.98G [00:30<00:05, 169MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  90%|█████████ | 8.99G/9.98G [00:30<00:05, 170MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  90%|█████████ | 9.01G/9.98G [00:30<00:08, 119MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  91%|█████████ | 9.05G/9.98G [00:30<00:05, 171MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  91%|█████████ | 9.09G/9.98G [00:30<00:04, 203MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  91%|█████████▏| 9.12G/9.98G [00:31<00:04, 185MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  92%|█████████▏| 9.14G/9.98G [00:31<00:04, 174MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  92%|█████████▏| 9.16G/9.98G [00:31<00:04, 166MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  92%|█████████▏| 9.19G/9.98G [00:31<00:04, 167MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  92%|█████████▏| 9.21G/9.98G [00:31<00:04, 164MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  92%|█████████▏| 9.23G/9.98G [00:31<00:04, 162MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  93%|█████████▎| 9.25G/9.98G [00:31<00:04, 164MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  93%|█████████▎| 9.27G/9.98G [00:32<00:04, 159MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  93%|█████████▎| 9.29G/9.98G [00:32<00:04, 160MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  93%|█████████▎| 9.31G/9.98G [00:32<00:04, 159MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  94%|█████████▎| 9.33G/9.98G [00:32<00:04, 160MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  94%|█████████▍| 9.35G/9.98G [00:32<00:03, 156MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  94%|█████████▍| 9.37G/9.98G [00:32<00:03, 160MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  94%|█████████▍| 9.40G/9.98G [00:32<00:03, 164MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  94%|█████████▍| 9.42G/9.98G [00:33<00:03, 162MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  95%|█████████▍| 9.45G/9.98G [00:33<00:03, 170MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  95%|█████████▍| 9.47G/9.98G [00:33<00:03, 167MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  95%|█████████▌| 9.49G/9.98G [00:33<00:02, 171MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  95%|█████████▌| 9.51G/9.98G [00:33<00:02, 170MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  96%|█████████▌| 9.53G/9.98G [00:33<00:02, 167MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  96%|█████████▌| 9.55G/9.98G [00:33<00:02, 165MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  96%|█████████▌| 9.57G/9.98G [00:33<00:02, 167MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  96%|█████████▌| 9.59G/9.98G [00:34<00:02, 169MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  96%|█████████▋| 9.62G/9.98G [00:34<00:02, 162MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  97%|█████████▋| 9.64G/9.98G [00:34<00:02, 164MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  97%|█████████▋| 9.66G/9.98G [00:34<00:01, 174MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  97%|█████████▋| 9.68G/9.98G [00:34<00:01, 172MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  97%|█████████▋| 9.70G/9.98G [00:34<00:01, 173MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  97%|█████████▋| 9.72G/9.98G [00:34<00:01, 173MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  98%|█████████▊| 9.74G/9.98G [00:34<00:01, 173MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  98%|█████████▊| 9.76G/9.98G [00:35<00:01, 173MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  98%|█████████▊| 9.78G/9.98G [00:35<00:01, 164MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  98%|█████████▊| 9.80G/9.98G [00:35<00:01, 169MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  98%|█████████▊| 9.83G/9.98G [00:35<00:00, 171MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  99%|█████████▊| 9.85G/9.98G [00:35<00:00, 168MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  99%|█████████▉| 9.87G/9.98G [00:35<00:00, 173MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  99%|█████████▉| 9.89G/9.98G [00:35<00:00, 170MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  99%|█████████▉| 9.91G/9.98G [00:35<00:00, 172MB/s]#033[A\n",
      "model-00001-of-00002.safetensors: 100%|█████████▉| 9.93G/9.98G [00:36<00:00, 175MB/s]#033[A\n",
      "model-00001-of-00002.safetensors: 100%|█████████▉| 9.95G/9.98G [00:36<00:00, 172MB/s]#033[A\n",
      "model-00001-of-00002.safetensors: 100%|█████████▉| 9.97G/9.98G [00:36<00:00, 173MB/s]#033[A\n",
      "model-00001-of-00002.safetensors: 100%|██████████| 9.98G/9.98G [00:36<00:00, 274MB/s]\n",
      "Downloading shards:  50%|█████     | 1/2 [00:36<00:36, 36.63s/it]\n",
      "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]#033[A\n",
      "model-00002-of-00002.safetensors:   1%|          | 31.5M/3.50G [00:00<00:13, 256MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:   2%|▏         | 62.9M/3.50G [00:00<00:12, 268MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:   3%|▎         | 105M/3.50G [00:00<00:11, 307MB/s] #033[A\n",
      "model-00002-of-00002.safetensors:   4%|▍         | 147M/3.50G [00:00<00:12, 279MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:   5%|▌         | 178M/3.50G [00:00<00:14, 234MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:   6%|▌         | 210M/3.50G [00:00<00:15, 212MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:   7%|▋         | 241M/3.50G [00:01<00:16, 199MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:   7%|▋         | 262M/3.50G [00:01<00:17, 189MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:   8%|▊         | 283M/3.50G [00:01<00:17, 187MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:   9%|▊         | 304M/3.50G [00:01<00:17, 180MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:   9%|▉         | 325M/3.50G [00:01<00:17, 180MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  10%|▉         | 346M/3.50G [00:01<00:17, 182MB/s]\n",
      "#033[A\n",
      "model-00002-of-00002.safetensors:  10%|█         | 367M/3.50G [00:01<00:17, 180MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  11%|█         | 388M/3.50G [00:01<00:17, 175MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  12%|█▏        | 409M/3.50G [00:02<00:17, 175MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  12%|█▏        | 430M/3.50G [00:02<00:17, 174MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  13%|█▎        | 451M/3.50G [00:02<00:17, 175MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  13%|█▎        | 472M/3.50G [00:02<00:18, 168MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  14%|█▍        | 493M/3.50G [00:02<00:17, 172MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  15%|█▍        | 514M/3.50G [00:02<00:16, 177MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  15%|█▌        | 535M/3.50G [00:02<00:16, 176MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  16%|█▌        | 556M/3.50G [00:02<00:17, 172MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  16%|█▋        | 577M/3.50G [00:03<00:16, 173MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  17%|█▋        | 598M/3.50G [00:03<00:16, 178MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  18%|█▊        | 629M/3.50G [00:03<00:14, 202MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  19%|█▉        | 661M/3.50G [00:03<00:12, 223MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  20%|█▉        | 692M/3.50G [00:03<00:11, 235MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  21%|██        | 724M/3.50G [00:03<00:11, 233MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  22%|██▏       | 755M/3.50G [00:03<00:10, 250MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  22%|██▏       | 786M/3.50G [00:03<00:10, 256MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  23%|██▎       | 818M/3.50G [00:03<00:10, 254MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  24%|██▍       | 849M/3.50G [00:04<00:10, 260MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  25%|██▌       | 881M/3.50G [00:04<00:10, 259MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  26%|██▌       | 912M/3.50G [00:04<00:09, 262MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  27%|██▋       | 944M/3.50G [00:04<00:09, 262MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  28%|██▊       | 975M/3.50G [00:04<00:09, 262MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  29%|██▉       | 1.01G/3.50G [00:04<00:09, 260MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  30%|██▉       | 1.04G/3.50G [00:04<00:09, 260MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  31%|███       | 1.07G/3.50G [00:04<00:09, 261MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  31%|███▏      | 1.10G/3.50G [00:05<00:09, 259MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  32%|███▏      | 1.13G/3.50G [00:05<00:08, 264MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  33%|███▎      | 1.16G/3.50G [00:05<00:08, 262MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  34%|███▍      | 1.20G/3.50G [00:05<00:08, 262MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  35%|███▌      | 1.23G/3.50G [00:05<00:08, 262MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  36%|███▌      | 1.26G/3.50G [00:05<00:09, 245MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  37%|███▋      | 1.29G/3.50G [00:05<00:10, 217MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  38%|███▊      | 1.32G/3.50G [00:06<00:11, 197MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  38%|███▊      | 1.34G/3.50G [00:06<00:11, 186MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  39%|███▉      | 1.36G/3.50G [00:06<00:11, 183MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  40%|███▉      | 1.38G/3.50G [00:06<00:11, 178MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  40%|████      | 1.41G/3.50G [00:06<00:12, 172MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  41%|████      | 1.43G/3.50G [00:06<00:12, 172MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  41%|████▏     | 1.45G/3.50G [00:06<00:11, 171MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  42%|████▏     | 1.47G/3.50G [00:06<00:12, 164MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  43%|████▎     | 1.49G/3.50G [00:07<00:11, 168MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  43%|████▎     | 1.51G/3.50G [00:07<00:12, 162MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  44%|████▎     | 1.53G/3.50G [00:07<00:12, 164MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  44%|████▍     | 1.55G/3.50G [00:07<00:11, 165MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  45%|████▍     | 1.57G/3.50G [00:07<00:12, 159MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  46%|████▌     | 1.59G/3.50G [00:07<00:12, 156MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  46%|████▌     | 1.61G/3.50G [00:07<00:11, 159MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  47%|████▋     | 1.64G/3.50G [00:07<00:11, 159MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  47%|████▋     | 1.66G/3.50G [00:08<00:11, 161MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  48%|████▊     | 1.68G/3.50G [00:08<00:11, 162MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  49%|████▊     | 1.70G/3.50G [00:08<00:10, 166MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  49%|████▉     | 1.72G/3.50G [00:08<00:12, 141MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  50%|█████     | 1.75G/3.50G [00:08<00:09, 178MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  51%|█████     | 1.77G/3.50G [00:08<00:10, 166MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  51%|█████     | 1.79G/3.50G [00:08<00:10, 162MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  52%|█████▏    | 1.81G/3.50G [00:09<00:10, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  52%|█████▏    | 1.84G/3.50G [00:09<00:10, 159MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  53%|█████▎    | 1.86G/3.50G [00:09<00:09, 166MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  54%|█████▎    | 1.88G/3.50G [00:09<00:09, 164MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  54%|█████▍    | 1.90G/3.50G [00:09<00:09, 161MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  55%|█████▍    | 1.92G/3.50G [00:09<00:09, 165MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  55%|█████▌    | 1.94G/3.50G [00:09<00:09, 162MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  56%|█████▌    | 1.96G/3.50G [00:09<00:09, 162MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  57%|█████▋    | 1.98G/3.50G [00:10<00:13, 116MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  58%|█████▊    | 2.02G/3.50G [00:10<00:08, 165MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  59%|█████▉    | 2.07G/3.50G [00:10<00:06, 205MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  60%|█████▉    | 2.10G/3.50G [00:10<00:07, 183MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  61%|██████    | 2.12G/3.50G [00:10<00:08, 168MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  61%|██████    | 2.14G/3.50G [00:11<00:08, 159MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  62%|██████▏   | 2.16G/3.50G [00:11<00:08, 156MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  62%|██████▏   | 2.18G/3.50G [00:11<00:08, 159MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  63%|██████▎   | 2.20G/3.50G [00:11<00:08, 159MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  64%|██████▎   | 2.22G/3.50G [00:11<00:07, 160MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  64%|██████▍   | 2.24G/3.50G [00:11<00:07, 159MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  65%|██████▍   | 2.26G/3.50G [00:11<00:07, 161MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  65%|██████▌   | 2.29G/3.50G [00:12<00:07, 160MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  66%|██████▌   | 2.31G/3.50G [00:12<00:07, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  67%|██████▋   | 2.33G/3.50G [00:12<00:07, 160MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  67%|██████▋   | 2.35G/3.50G [00:12<00:07, 164MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  68%|██████▊   | 2.37G/3.50G [00:12<00:06, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  68%|██████▊   | 2.39G/3.50G [00:12<00:06, 162MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  69%|██████▉   | 2.41G/3.50G [00:12<00:06, 164MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  69%|██████▉   | 2.43G/3.50G [00:12<00:06, 158MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  70%|███████   | 2.45G/3.50G [00:13<00:06, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  71%|███████   | 2.47G/3.50G [00:13<00:06, 166MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  71%|███████▏  | 2.50G/3.50G [00:13<00:06, 161MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  72%|███████▏  | 2.52G/3.50G [00:13<00:06, 159MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  72%|███████▏  | 2.54G/3.50G [00:13<00:06, 150MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  73%|███████▎  | 2.56G/3.50G [00:13<00:05, 159MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  74%|███████▎  | 2.58G/3.50G [00:13<00:05, 160MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  74%|███████▍  | 2.60G/3.50G [00:13<00:05, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  75%|███████▍  | 2.62G/3.50G [00:14<00:05, 162MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  75%|███████▌  | 2.64G/3.50G [00:14<00:05, 162MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  76%|███████▌  | 2.66G/3.50G [00:14<00:05, 160MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  77%|███████▋  | 2.68G/3.50G [00:14<00:04, 164MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  77%|███████▋  | 2.71G/3.50G [00:14<00:04, 161MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  78%|███████▊  | 2.73G/3.50G [00:14<00:04, 165MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  78%|███████▊  | 2.75G/3.50G [00:14<00:04, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  79%|███████▉  | 2.77G/3.50G [00:14<00:04, 162MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  80%|███████▉  | 2.79G/3.50G [00:15<00:04, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  80%|████████  | 2.81G/3.50G [00:15<00:04, 164MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  81%|████████  | 2.83G/3.50G [00:15<00:04, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  81%|████████▏ | 2.85G/3.50G [00:15<00:03, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  82%|████████▏ | 2.87G/3.50G [00:15<00:03, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  83%|████████▎ | 2.89G/3.50G [00:15<00:03, 161MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  83%|████████▎ | 2.92G/3.50G [00:15<00:03, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  84%|████████▍ | 2.94G/3.50G [00:16<00:03, 161MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  84%|████████▍ | 2.96G/3.50G [00:16<00:03, 168MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  85%|████████▌ | 2.98G/3.50G [00:16<00:03, 164MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  86%|████████▌ | 3.00G/3.50G [00:16<00:03, 158MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  86%|████████▋ | 3.02G/3.50G [00:16<00:02, 162MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  87%|████████▋ | 3.04G/3.50G [00:16<00:02, 161MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  87%|████████▋ | 3.06G/3.50G [00:16<00:02, 159MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  88%|████████▊ | 3.08G/3.50G [00:16<00:02, 162MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  89%|████████▊ | 3.10G/3.50G [00:17<00:02, 161MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  89%|████████▉ | 3.12G/3.50G [00:17<00:02, 161MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  90%|████████▉ | 3.15G/3.50G [00:17<00:02, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  90%|█████████ | 3.17G/3.50G [00:17<00:02, 162MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  91%|█████████ | 3.19G/3.50G [00:17<00:01, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  92%|█████████▏| 3.21G/3.50G [00:17<00:01, 164MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  92%|█████████▏| 3.23G/3.50G [00:17<00:01, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  93%|█████████▎| 3.25G/3.50G [00:17<00:01, 162MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  93%|█████████▎| 3.27G/3.50G [00:18<00:01, 165MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  94%|█████████▍| 3.29G/3.50G [00:18<00:01, 162MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  95%|█████████▍| 3.31G/3.50G [00:18<00:01, 160MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  95%|█████████▌| 3.33G/3.50G [00:18<00:01, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  96%|█████████▌| 3.36G/3.50G [00:18<00:00, 159MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  96%|█████████▋| 3.38G/3.50G [00:18<00:00, 161MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  97%|█████████▋| 3.40G/3.50G [00:18<00:00, 161MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  98%|█████████▊| 3.42G/3.50G [00:18<00:00, 161MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  98%|█████████▊| 3.44G/3.50G [00:19<00:00, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  99%|█████████▉| 3.46G/3.50G [00:19<00:00, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  99%|█████████▉| 3.48G/3.50G [00:19<00:00, 162MB/s]#033[A\n",
      "model-00002-of-00002.safetensors: 100%|██████████| 3.50G/3.50G [00:19<00:00, 160MB/s]#033[A\n",
      "model-00002-of-00002.safetensors: 100%|██████████| 3.50G/3.50G [00:19<00:00, 179MB/s]\n",
      "Downloading shards: 100%|██████████| 2/2 [00:56<00:00, 26.71s/it]\n",
      "Downloading shards: 100%|██████████| 2/2 [00:56<00:00, 28.20s/it]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.82s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.27s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.25s/it]\n",
      "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]\n",
      "generation_config.json: 100%|██████████| 188/188 [00:00<00:00, 1.50MB/s]\n",
      "Found 7 modules to quantize: ['gate_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'down_proj']\n",
      "trainable params: 159,907,840 || all params: 3,660,320,768 || trainable%: 4.368683788535114\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "0%|          | 0/1054 [00:00<?, ?it/s]\n",
      "0%|          | 1/1054 [00:11<3:28:32, 11.88s/it]\n",
      "0%|          | 2/1054 [00:23<3:25:03, 11.70s/it]\n",
      "0%|          | 3/1054 [00:35<3:23:49, 11.64s/it]\n",
      "0%|          | 4/1054 [00:46<3:23:07, 11.61s/it]\n",
      "0%|          | 5/1054 [00:58<3:22:39, 11.59s/it]\n",
      "1%|          | 6/1054 [01:09<3:22:17, 11.58s/it]\n",
      "1%|          | 7/1054 [01:21<3:21:59, 11.58s/it]\n",
      "1%|          | 8/1054 [01:32<3:21:44, 11.57s/it]\n",
      "1%|          | 9/1054 [01:44<3:21:29, 11.57s/it]\n",
      "1%|          | 10/1054 [01:55<3:21:16, 11.57s/it]\n",
      "{'loss': 1.6176, 'learning_rate': 0.0001981024667931689, 'epoch': 0.02}\n",
      "1%|          | 10/1054 [01:55<3:21:16, 11.57s/it]\n",
      "1%|          | 11/1054 [02:07<3:21:05, 11.57s/it]\n",
      "1%|          | 12/1054 [02:19<3:20:52, 11.57s/it]\n",
      "1%|          | 13/1054 [02:30<3:20:40, 11.57s/it]\n",
      "1%|▏         | 14/1054 [02:42<3:20:27, 11.57s/it]\n",
      "1%|▏         | 15/1054 [02:53<3:20:15, 11.56s/it]\n",
      "2%|▏         | 16/1054 [03:05<3:20:03, 11.56s/it]\n",
      "2%|▏         | 17/1054 [03:16<3:19:51, 11.56s/it]\n",
      "2%|▏         | 18/1054 [03:28<3:19:39, 11.56s/it]\n",
      "2%|▏         | 19/1054 [03:40<3:19:28, 11.56s/it]\n",
      "2%|▏         | 20/1054 [03:51<3:19:16, 11.56s/it]\n",
      "{'loss': 1.4961, 'learning_rate': 0.00019620493358633777, 'epoch': 0.04}\n",
      "2%|▏         | 20/1054 [03:51<3:19:16, 11.56s/it]\n",
      "2%|▏         | 21/1054 [04:03<3:19:05, 11.56s/it]\n",
      "2%|▏         | 22/1054 [04:14<3:18:54, 11.56s/it]\n",
      "2%|▏         | 23/1054 [04:26<3:18:42, 11.56s/it]\n",
      "2%|▏         | 24/1054 [04:37<3:18:30, 11.56s/it]\n",
      "2%|▏         | 25/1054 [04:49<3:18:19, 11.56s/it]\n",
      "2%|▏         | 26/1054 [05:00<3:18:07, 11.56s/it]\n",
      "3%|▎         | 27/1054 [05:12<3:17:56, 11.56s/it]\n",
      "3%|▎         | 28/1054 [05:24<3:17:44, 11.56s/it]\n",
      "3%|▎         | 29/1054 [05:35<3:17:32, 11.56s/it]\n",
      "3%|▎         | 30/1054 [05:47<3:17:21, 11.56s/it]\n",
      "{'loss': 1.5119, 'learning_rate': 0.00019430740037950666, 'epoch': 0.06}\n",
      "3%|▎         | 30/1054 [05:47<3:17:21, 11.56s/it]\n",
      "3%|▎         | 31/1054 [05:58<3:17:09, 11.56s/it]\n",
      "3%|▎         | 32/1054 [06:10<3:16:57, 11.56s/it]\n",
      "3%|▎         | 33/1054 [06:21<3:16:46, 11.56s/it]\n",
      "3%|▎         | 34/1054 [06:33<3:16:34, 11.56s/it]\n",
      "3%|▎         | 35/1054 [06:45<3:16:22, 11.56s/it]\n",
      "3%|▎         | 36/1054 [06:56<3:16:11, 11.56s/it]\n",
      "4%|▎         | 37/1054 [07:08<3:16:01, 11.56s/it]\n",
      "4%|▎         | 38/1054 [07:19<3:15:49, 11.56s/it]\n",
      "4%|▎         | 39/1054 [07:31<3:15:37, 11.56s/it]\n",
      "4%|▍         | 40/1054 [07:42<3:15:25, 11.56s/it]\n",
      "{'loss': 1.4088, 'learning_rate': 0.00019240986717267554, 'epoch': 0.08}\n",
      "4%|▍         | 40/1054 [07:42<3:15:25, 11.56s/it]\n",
      "4%|▍         | 41/1054 [07:54<3:15:13, 11.56s/it]\n",
      "4%|▍         | 42/1054 [08:05<3:15:02, 11.56s/it]\n",
      "4%|▍         | 43/1054 [08:17<3:14:50, 11.56s/it]\n",
      "4%|▍         | 44/1054 [08:29<3:14:38, 11.56s/it]\n",
      "4%|▍         | 45/1054 [08:40<3:14:27, 11.56s/it]\n",
      "4%|▍         | 46/1054 [08:52<3:14:16, 11.56s/it]\n",
      "4%|▍         | 47/1054 [09:03<3:14:04, 11.56s/it]\n",
      "5%|▍         | 48/1054 [09:15<3:13:52, 11.56s/it]\n",
      "5%|▍         | 49/1054 [09:26<3:13:41, 11.56s/it]\n",
      "5%|▍         | 50/1054 [09:38<3:13:29, 11.56s/it]\n",
      "{'loss': 1.391, 'learning_rate': 0.0001905123339658444, 'epoch': 0.09}\n",
      "5%|▍         | 50/1054 [09:38<3:13:29, 11.56s/it]\n",
      "5%|▍         | 51/1054 [09:50<3:13:18, 11.56s/it]\n",
      "5%|▍         | 52/1054 [10:01<3:13:06, 11.56s/it]\n",
      "5%|▌         | 53/1054 [10:13<3:12:55, 11.56s/it]\n",
      "5%|▌         | 54/1054 [10:24<3:12:43, 11.56s/it]\n",
      "5%|▌         | 55/1054 [10:36<3:12:31, 11.56s/it]\n",
      "5%|▌         | 56/1054 [10:47<3:12:20, 11.56s/it]\n",
      "5%|▌         | 57/1054 [10:59<3:12:08, 11.56s/it]\n",
      "6%|▌         | 58/1054 [11:11<3:11:57, 11.56s/it]\n",
      "6%|▌         | 59/1054 [11:22<3:11:45, 11.56s/it]\n",
      "6%|▌         | 60/1054 [11:34<3:11:34, 11.56s/it]\n",
      "{'loss': 1.4773, 'learning_rate': 0.0001886148007590133, 'epoch': 0.11}\n",
      "6%|▌         | 60/1054 [11:34<3:11:34, 11.56s/it]\n",
      "6%|▌         | 61/1054 [11:45<3:11:22, 11.56s/it]\n",
      "6%|▌         | 62/1054 [11:57<3:11:11, 11.56s/it]\n",
      "6%|▌         | 63/1054 [12:08<3:10:59, 11.56s/it]\n",
      "6%|▌         | 64/1054 [12:20<3:10:48, 11.56s/it]\n",
      "6%|▌         | 65/1054 [12:31<3:10:36, 11.56s/it]\n",
      "6%|▋         | 66/1054 [12:43<3:10:24, 11.56s/it]\n",
      "6%|▋         | 67/1054 [12:55<3:10:13, 11.56s/it]\n",
      "6%|▋         | 68/1054 [13:06<3:10:01, 11.56s/it]\n",
      "7%|▋         | 69/1054 [13:18<3:09:50, 11.56s/it]\n",
      "7%|▋         | 70/1054 [13:29<3:09:39, 11.56s/it]\n",
      "{'loss': 1.4472, 'learning_rate': 0.00018671726755218219, 'epoch': 0.13}\n",
      "7%|▋         | 70/1054 [13:29<3:09:39, 11.56s/it]\n",
      "7%|▋         | 71/1054 [13:41<3:09:28, 11.56s/it]\n",
      "7%|▋         | 72/1054 [13:52<3:09:16, 11.56s/it]\n",
      "7%|▋         | 73/1054 [14:04<3:09:04, 11.56s/it]\n",
      "7%|▋         | 74/1054 [14:16<3:08:52, 11.56s/it]\n",
      "7%|▋         | 75/1054 [14:27<3:08:40, 11.56s/it]\n",
      "7%|▋         | 76/1054 [14:39<3:08:29, 11.56s/it]\n",
      "7%|▋         | 77/1054 [14:50<3:08:18, 11.56s/it]\n",
      "7%|▋         | 78/1054 [15:02<3:08:06, 11.56s/it]\n",
      "7%|▋         | 79/1054 [15:13<3:07:54, 11.56s/it]\n",
      "8%|▊         | 80/1054 [15:25<3:07:42, 11.56s/it]\n",
      "{'loss': 1.3859, 'learning_rate': 0.00018481973434535104, 'epoch': 0.15}\n",
      "8%|▊         | 80/1054 [15:25<3:07:42, 11.56s/it]\n",
      "8%|▊         | 81/1054 [15:36<3:07:31, 11.56s/it]\n",
      "8%|▊         | 82/1054 [15:48<3:07:20, 11.56s/it]\n",
      "8%|▊         | 83/1054 [16:00<3:07:08, 11.56s/it]\n",
      "8%|▊         | 84/1054 [16:11<3:06:57, 11.56s/it]\n",
      "8%|▊         | 85/1054 [16:23<3:06:45, 11.56s/it]\n",
      "8%|▊         | 86/1054 [16:34<3:06:34, 11.56s/it]\n",
      "8%|▊         | 87/1054 [16:46<3:06:22, 11.56s/it]\n",
      "8%|▊         | 88/1054 [16:57<3:06:10, 11.56s/it]\n",
      "8%|▊         | 89/1054 [17:09<3:05:59, 11.56s/it]\n",
      "9%|▊         | 90/1054 [17:21<3:05:47, 11.56s/it]\n",
      "{'loss': 1.3293, 'learning_rate': 0.00018292220113851992, 'epoch': 0.17}\n",
      "9%|▊         | 90/1054 [17:21<3:05:47, 11.56s/it]\n",
      "9%|▊         | 91/1054 [17:32<3:05:35, 11.56s/it]\n",
      "9%|▊         | 92/1054 [17:44<3:05:24, 11.56s/it]\n",
      "9%|▉         | 93/1054 [17:55<3:05:12, 11.56s/it]\n",
      "9%|▉         | 94/1054 [18:07<3:05:01, 11.56s/it]\n",
      "9%|▉         | 95/1054 [18:18<3:04:49, 11.56s/it]\n",
      "9%|▉         | 96/1054 [18:30<3:04:37, 11.56s/it]\n",
      "9%|▉         | 97/1054 [18:42<3:04:27, 11.56s/it]\n",
      "9%|▉         | 98/1054 [18:53<3:04:15, 11.56s/it]\n",
      "9%|▉         | 99/1054 [19:05<3:04:03, 11.56s/it]\n",
      "9%|▉         | 100/1054 [19:16<3:03:52, 11.56s/it]\n",
      "{'loss': 1.405, 'learning_rate': 0.0001810246679316888, 'epoch': 0.19}\n",
      "9%|▉         | 100/1054 [19:16<3:03:52, 11.56s/it]\n",
      "10%|▉         | 101/1054 [19:28<3:03:40, 11.56s/it]\n",
      "10%|▉         | 102/1054 [19:39<3:03:28, 11.56s/it]\n",
      "10%|▉         | 103/1054 [19:51<3:03:17, 11.56s/it]\n",
      "10%|▉         | 104/1054 [20:02<3:03:06, 11.56s/it]\n",
      "10%|▉         | 105/1054 [20:14<3:02:54, 11.56s/it]\n",
      "10%|█         | 106/1054 [20:26<3:02:42, 11.56s/it]\n",
      "10%|█         | 107/1054 [20:37<3:02:30, 11.56s/it]\n",
      "10%|█         | 108/1054 [20:49<3:02:18, 11.56s/it]\n",
      "10%|█         | 109/1054 [21:00<3:02:07, 11.56s/it]\n",
      "10%|█         | 110/1054 [21:12<3:01:55, 11.56s/it]\n",
      "{'loss': 1.35, 'learning_rate': 0.0001791271347248577, 'epoch': 0.21}\n",
      "10%|█         | 110/1054 [21:12<3:01:55, 11.56s/it]\n",
      "11%|█         | 111/1054 [21:23<3:01:44, 11.56s/it]\n",
      "11%|█         | 112/1054 [21:35<3:01:32, 11.56s/it]\n",
      "11%|█         | 113/1054 [21:47<3:01:20, 11.56s/it]\n",
      "11%|█         | 114/1054 [21:58<3:01:09, 11.56s/it]\n",
      "11%|█         | 115/1054 [22:10<3:00:58, 11.56s/it]\n",
      "11%|█         | 116/1054 [22:21<3:00:46, 11.56s/it]\n",
      "11%|█         | 117/1054 [22:33<3:00:34, 11.56s/it]\n",
      "11%|█         | 118/1054 [22:44<3:00:23, 11.56s/it]\n",
      "11%|█▏        | 119/1054 [22:56<3:00:11, 11.56s/it]\n",
      "11%|█▏        | 120/1054 [23:07<2:59:59, 11.56s/it]\n",
      "{'loss': 1.4231, 'learning_rate': 0.00017722960151802657, 'epoch': 0.23}\n",
      "11%|█▏        | 120/1054 [23:07<2:59:59, 11.56s/it]\n",
      "11%|█▏        | 121/1054 [23:19<2:59:49, 11.56s/it]\n",
      "12%|█▏        | 122/1054 [23:31<2:59:37, 11.56s/it]\n",
      "12%|█▏        | 123/1054 [23:42<2:59:26, 11.56s/it]\n",
      "12%|█▏        | 124/1054 [23:54<2:59:14, 11.56s/it]\n",
      "12%|█▏        | 125/1054 [24:05<2:59:02, 11.56s/it]\n",
      "12%|█▏        | 126/1054 [24:17<2:58:50, 11.56s/it]\n",
      "12%|█▏        | 127/1054 [24:28<2:58:38, 11.56s/it]\n",
      "12%|█▏        | 128/1054 [24:40<2:58:27, 11.56s/it]\n",
      "12%|█▏        | 129/1054 [24:52<2:58:16, 11.56s/it]\n",
      "12%|█▏        | 130/1054 [25:03<2:58:04, 11.56s/it]\n",
      "{'loss': 1.3549, 'learning_rate': 0.00017533206831119545, 'epoch': 0.25}\n",
      "12%|█▏        | 130/1054 [25:03<2:58:04, 11.56s/it]\n",
      "12%|█▏        | 131/1054 [25:15<2:57:53, 11.56s/it]\n",
      "13%|█▎        | 132/1054 [25:26<2:57:41, 11.56s/it]\n",
      "13%|█▎        | 133/1054 [25:38<2:57:29, 11.56s/it]\n",
      "13%|█▎        | 134/1054 [25:49<2:57:18, 11.56s/it]\n",
      "13%|█▎        | 135/1054 [26:01<2:57:06, 11.56s/it]\n",
      "13%|█▎        | 136/1054 [26:12<2:56:55, 11.56s/it]\n",
      "13%|█▎        | 137/1054 [26:24<2:56:43, 11.56s/it]\n",
      "13%|█▎        | 138/1054 [26:36<2:56:32, 11.56s/it]\n",
      "13%|█▎        | 139/1054 [26:47<2:56:20, 11.56s/it]\n",
      "13%|█▎        | 140/1054 [26:59<2:56:09, 11.56s/it]\n",
      "{'loss': 1.4474, 'learning_rate': 0.00017343453510436434, 'epoch': 0.27}\n",
      "13%|█▎        | 140/1054 [26:59<2:56:09, 11.56s/it]\n",
      "13%|█▎        | 141/1054 [27:10<2:55:57, 11.56s/it]\n",
      "13%|█▎        | 142/1054 [27:22<2:55:46, 11.56s/it]\n",
      "14%|█▎        | 143/1054 [27:33<2:55:35, 11.56s/it]\n",
      "14%|█▎        | 144/1054 [27:45<2:55:23, 11.56s/it]\n",
      "14%|█▍        | 145/1054 [27:57<2:55:12, 11.56s/it]\n",
      "14%|█▍        | 146/1054 [28:08<2:55:00, 11.56s/it]\n",
      "14%|█▍        | 147/1054 [28:20<2:54:48, 11.56s/it]\n",
      "14%|█▍        | 148/1054 [28:31<2:54:36, 11.56s/it]\n",
      "14%|█▍        | 149/1054 [28:43<2:54:25, 11.56s/it]\n",
      "14%|█▍        | 150/1054 [28:54<2:54:13, 11.56s/it]\n",
      "{'loss': 1.3512, 'learning_rate': 0.00017153700189753322, 'epoch': 0.28}\n",
      "14%|█▍        | 150/1054 [28:54<2:54:13, 11.56s/it]\n",
      "14%|█▍        | 151/1054 [29:06<2:54:02, 11.56s/it]\n",
      "14%|█▍        | 152/1054 [29:18<2:53:50, 11.56s/it]\n",
      "15%|█▍        | 153/1054 [29:29<2:53:38, 11.56s/it]\n",
      "15%|█▍        | 154/1054 [29:41<2:53:27, 11.56s/it]\n",
      "15%|█▍        | 155/1054 [29:52<2:53:15, 11.56s/it]\n",
      "15%|█▍        | 156/1054 [30:04<2:53:04, 11.56s/it]\n",
      "15%|█▍        | 157/1054 [30:15<2:52:52, 11.56s/it]\n",
      "15%|█▍        | 158/1054 [30:27<2:52:41, 11.56s/it]\n",
      "15%|█▌        | 159/1054 [30:38<2:52:30, 11.57s/it]\n",
      "15%|█▌        | 160/1054 [30:50<2:52:18, 11.56s/it]\n",
      "{'loss': 1.4183, 'learning_rate': 0.00016963946869070207, 'epoch': 0.3}\n",
      "15%|█▌        | 160/1054 [30:50<2:52:18, 11.56s/it]\n",
      "15%|█▌        | 161/1054 [31:02<2:52:08, 11.57s/it]\n",
      "15%|█▌        | 162/1054 [31:13<2:51:56, 11.57s/it]\n",
      "15%|█▌        | 163/1054 [31:25<2:51:44, 11.56s/it]\n",
      "16%|█▌        | 164/1054 [31:36<2:51:32, 11.56s/it]\n",
      "16%|█▌        | 165/1054 [31:48<2:51:20, 11.56s/it]\n",
      "16%|█▌        | 166/1054 [31:59<2:51:08, 11.56s/it]\n",
      "16%|█▌        | 167/1054 [32:11<2:50:56, 11.56s/it]\n",
      "16%|█▌        | 168/1054 [32:23<2:50:44, 11.56s/it]\n",
      "16%|█▌        | 169/1054 [32:34<2:50:33, 11.56s/it]\n",
      "16%|█▌        | 170/1054 [32:46<2:50:21, 11.56s/it]\n",
      "{'loss': 1.3524, 'learning_rate': 0.00016774193548387098, 'epoch': 0.32}\n",
      "16%|█▌        | 170/1054 [32:46<2:50:21, 11.56s/it]\n",
      "16%|█▌        | 171/1054 [32:57<2:50:10, 11.56s/it]\n",
      "16%|█▋        | 172/1054 [33:09<2:49:58, 11.56s/it]\n",
      "16%|█▋        | 173/1054 [33:20<2:49:47, 11.56s/it]\n",
      "17%|█▋        | 174/1054 [33:32<2:49:35, 11.56s/it]\n",
      "17%|█▋        | 175/1054 [33:43<2:49:24, 11.56s/it]\n",
      "17%|█▋        | 176/1054 [33:55<2:49:12, 11.56s/it]\n",
      "17%|█▋        | 177/1054 [34:07<2:49:01, 11.56s/it]\n",
      "17%|█▋        | 178/1054 [34:18<2:48:49, 11.56s/it]\n",
      "17%|█▋        | 179/1054 [34:30<2:48:37, 11.56s/it]\n",
      "17%|█▋        | 180/1054 [34:41<2:48:26, 11.56s/it]\n",
      "{'loss': 1.4156, 'learning_rate': 0.00016584440227703987, 'epoch': 0.34}\n",
      "17%|█▋        | 180/1054 [34:41<2:48:26, 11.56s/it]\n",
      "17%|█▋        | 181/1054 [34:53<2:48:14, 11.56s/it]\n",
      "17%|█▋        | 182/1054 [35:04<2:48:03, 11.56s/it]\n",
      "17%|█▋        | 183/1054 [35:16<2:47:51, 11.56s/it]\n",
      "17%|█▋        | 184/1054 [35:28<2:47:40, 11.56s/it]\n",
      "18%|█▊        | 185/1054 [35:39<2:47:28, 11.56s/it]\n",
      "18%|█▊        | 186/1054 [35:51<2:47:16, 11.56s/it]\n",
      "18%|█▊        | 187/1054 [36:02<2:47:05, 11.56s/it]\n",
      "18%|█▊        | 188/1054 [36:14<2:46:53, 11.56s/it]\n",
      "18%|█▊        | 189/1054 [36:25<2:46:42, 11.56s/it]\n",
      "18%|█▊        | 190/1054 [36:37<2:46:31, 11.56s/it]\n",
      "{'loss': 1.5219, 'learning_rate': 0.00016394686907020872, 'epoch': 0.36}\n",
      "18%|█▊        | 190/1054 [36:37<2:46:31, 11.56s/it]\n",
      "18%|█▊        | 191/1054 [36:48<2:46:19, 11.56s/it]\n",
      "18%|█▊        | 192/1054 [37:00<2:46:07, 11.56s/it]\n",
      "18%|█▊        | 193/1054 [37:12<2:45:56, 11.56s/it]\n",
      "18%|█▊        | 194/1054 [37:23<2:45:44, 11.56s/it]\n",
      "19%|█▊        | 195/1054 [37:35<2:45:33, 11.56s/it]\n",
      "19%|█▊        | 196/1054 [37:46<2:45:21, 11.56s/it]\n",
      "19%|█▊        | 197/1054 [37:58<2:45:09, 11.56s/it]\n",
      "19%|█▉        | 198/1054 [38:09<2:44:57, 11.56s/it]\n",
      "19%|█▉        | 199/1054 [38:21<2:44:46, 11.56s/it]\n",
      "19%|█▉        | 200/1054 [38:33<2:44:35, 11.56s/it]\n",
      "{'loss': 1.4322, 'learning_rate': 0.00016204933586337763, 'epoch': 0.38}\n",
      "19%|█▉        | 200/1054 [38:33<2:44:35, 11.56s/it]\n",
      "19%|█▉        | 201/1054 [38:44<2:44:23, 11.56s/it]\n",
      "19%|█▉        | 202/1054 [38:56<2:44:12, 11.56s/it]\n",
      "19%|█▉        | 203/1054 [39:07<2:44:00, 11.56s/it]\n",
      "19%|█▉        | 204/1054 [39:19<2:43:49, 11.56s/it]\n",
      "19%|█▉        | 205/1054 [39:30<2:43:37, 11.56s/it]\n",
      "20%|█▉        | 206/1054 [39:42<2:43:26, 11.56s/it]\n",
      "20%|█▉        | 207/1054 [39:54<2:43:14, 11.56s/it]\n",
      "20%|█▉        | 208/1054 [40:05<2:43:03, 11.56s/it]\n",
      "20%|█▉        | 209/1054 [40:17<2:42:51, 11.56s/it]\n",
      "20%|█▉        | 210/1054 [40:28<2:42:39, 11.56s/it]\n",
      "{'loss': 1.3984, 'learning_rate': 0.0001601518026565465, 'epoch': 0.4}\n",
      "20%|█▉        | 210/1054 [40:28<2:42:39, 11.56s/it]\n",
      "20%|██        | 211/1054 [40:40<2:42:28, 11.56s/it]\n",
      "20%|██        | 212/1054 [40:51<2:42:16, 11.56s/it]\n",
      "20%|██        | 213/1054 [41:03<2:42:04, 11.56s/it]\n",
      "20%|██        | 214/1054 [41:14<2:41:53, 11.56s/it]\n",
      "20%|██        | 215/1054 [41:26<2:41:42, 11.56s/it]\n",
      "20%|██        | 216/1054 [41:38<2:41:30, 11.56s/it]\n",
      "21%|██        | 217/1054 [41:49<2:41:18, 11.56s/it]\n",
      "21%|██        | 218/1054 [42:01<2:41:07, 11.56s/it]\n",
      "21%|██        | 219/1054 [42:12<2:40:55, 11.56s/it]\n",
      "21%|██        | 220/1054 [42:24<2:40:43, 11.56s/it]\n",
      "{'loss': 1.4697, 'learning_rate': 0.00015825426944971537, 'epoch': 0.42}\n",
      "21%|██        | 220/1054 [42:24<2:40:43, 11.56s/it]\n",
      "21%|██        | 221/1054 [42:35<2:40:32, 11.56s/it]\n",
      "21%|██        | 222/1054 [42:47<2:40:21, 11.56s/it]\n",
      "21%|██        | 223/1054 [42:59<2:40:09, 11.56s/it]\n",
      "21%|██▏       | 224/1054 [43:10<2:39:57, 11.56s/it]\n",
      "21%|██▏       | 225/1054 [43:22<2:39:46, 11.56s/it]\n",
      "21%|██▏       | 226/1054 [43:33<2:39:34, 11.56s/it]\n",
      "22%|██▏       | 227/1054 [43:45<2:39:22, 11.56s/it]\n",
      "22%|██▏       | 228/1054 [43:56<2:39:11, 11.56s/it]\n",
      "22%|██▏       | 229/1054 [44:08<2:38:59, 11.56s/it]\n",
      "22%|██▏       | 230/1054 [44:19<2:38:48, 11.56s/it]\n",
      "{'loss': 1.4162, 'learning_rate': 0.00015635673624288425, 'epoch': 0.44}\n",
      "22%|██▏       | 230/1054 [44:19<2:38:48, 11.56s/it]\n",
      "22%|██▏       | 231/1054 [44:31<2:38:37, 11.56s/it]\n",
      "22%|██▏       | 232/1054 [44:43<2:38:25, 11.56s/it]\n",
      "22%|██▏       | 233/1054 [44:54<2:38:13, 11.56s/it]\n",
      "22%|██▏       | 234/1054 [45:06<2:38:01, 11.56s/it]\n",
      "22%|██▏       | 235/1054 [45:17<2:37:50, 11.56s/it]\n",
      "22%|██▏       | 236/1054 [45:29<2:37:38, 11.56s/it]\n",
      "22%|██▏       | 237/1054 [45:40<2:37:27, 11.56s/it]\n",
      "23%|██▎       | 238/1054 [45:52<2:37:15, 11.56s/it]\n",
      "23%|██▎       | 239/1054 [46:04<2:37:04, 11.56s/it]\n",
      "23%|██▎       | 240/1054 [46:15<2:36:52, 11.56s/it]\n",
      "{'loss': 1.5151, 'learning_rate': 0.00015445920303605316, 'epoch': 0.46}\n",
      "23%|██▎       | 240/1054 [46:15<2:36:52, 11.56s/it]\n",
      "23%|██▎       | 241/1054 [46:27<2:36:41, 11.56s/it]\n",
      "23%|██▎       | 242/1054 [46:38<2:36:29, 11.56s/it]\n",
      "23%|██▎       | 243/1054 [46:50<2:36:18, 11.56s/it]\n",
      "23%|██▎       | 244/1054 [47:01<2:36:06, 11.56s/it]\n",
      "23%|██▎       | 245/1054 [47:13<2:35:55, 11.56s/it]\n",
      "23%|██▎       | 246/1054 [47:24<2:35:43, 11.56s/it]\n",
      "23%|██▎       | 247/1054 [47:36<2:35:31, 11.56s/it]\n",
      "24%|██▎       | 248/1054 [47:48<2:35:20, 11.56s/it]\n",
      "24%|██▎       | 249/1054 [47:59<2:35:08, 11.56s/it]\n",
      "24%|██▎       | 250/1054 [48:11<2:34:57, 11.56s/it]\n",
      "{'loss': 1.4259, 'learning_rate': 0.00015256166982922201, 'epoch': 0.47}\n",
      "24%|██▎       | 250/1054 [48:11<2:34:57, 11.56s/it]\n",
      "24%|██▍       | 251/1054 [48:22<2:34:45, 11.56s/it]\n",
      "24%|██▍       | 252/1054 [48:34<2:34:34, 11.56s/it]\n",
      "24%|██▍       | 253/1054 [48:45<2:34:22, 11.56s/it]\n",
      "24%|██▍       | 254/1054 [48:57<2:34:11, 11.56s/it]\n",
      "24%|██▍       | 255/1054 [49:09<2:33:59, 11.56s/it]\n",
      "24%|██▍       | 256/1054 [49:20<2:33:48, 11.56s/it]\n",
      "24%|██▍       | 257/1054 [49:32<2:33:36, 11.56s/it]\n",
      "24%|██▍       | 258/1054 [49:43<2:33:25, 11.57s/it]\n",
      "25%|██▍       | 259/1054 [49:55<2:33:14, 11.57s/it]\n",
      "25%|██▍       | 260/1054 [50:06<2:33:02, 11.56s/it]\n",
      "{'loss': 1.2812, 'learning_rate': 0.0001506641366223909, 'epoch': 0.49}\n",
      "25%|██▍       | 260/1054 [50:06<2:33:02, 11.56s/it]\n",
      "25%|██▍       | 261/1054 [50:18<2:32:50, 11.56s/it]\n",
      "25%|██▍       | 262/1054 [50:30<2:32:38, 11.56s/it]\n",
      "25%|██▍       | 263/1054 [50:41<2:32:27, 11.56s/it]\n",
      "25%|██▌       | 264/1054 [50:53<2:32:14, 11.56s/it]\n",
      "25%|██▌       | 265/1054 [51:04<2:32:03, 11.56s/it]\n",
      "25%|██▌       | 266/1054 [51:16<2:31:51, 11.56s/it]\n",
      "25%|██▌       | 267/1054 [51:27<2:31:40, 11.56s/it]\n",
      "25%|██▌       | 268/1054 [51:39<2:31:28, 11.56s/it]\n",
      "26%|██▌       | 269/1054 [51:50<2:31:17, 11.56s/it]\n",
      "26%|██▌       | 270/1054 [52:02<2:31:05, 11.56s/it]\n",
      "{'loss': 1.4898, 'learning_rate': 0.00014876660341555978, 'epoch': 0.51}\n",
      "26%|██▌       | 270/1054 [52:02<2:31:05, 11.56s/it]\n",
      "26%|██▌       | 271/1054 [52:14<2:30:54, 11.56s/it]\n",
      "26%|██▌       | 272/1054 [52:25<2:30:42, 11.56s/it]\n",
      "26%|██▌       | 273/1054 [52:37<2:30:31, 11.56s/it]\n",
      "26%|██▌       | 274/1054 [52:48<2:30:19, 11.56s/it]\n",
      "26%|██▌       | 275/1054 [53:00<2:30:08, 11.56s/it]\n",
      "26%|██▌       | 276/1054 [53:11<2:29:56, 11.56s/it]\n",
      "26%|██▋       | 277/1054 [53:23<2:29:44, 11.56s/it]\n",
      "26%|██▋       | 278/1054 [53:35<2:29:33, 11.56s/it]\n",
      "26%|██▋       | 279/1054 [53:46<2:29:21, 11.56s/it]\n",
      "27%|██▋       | 280/1054 [53:58<2:29:10, 11.56s/it]\n",
      "{'loss': 1.2999, 'learning_rate': 0.00014686907020872866, 'epoch': 0.53}\n",
      "27%|██▋       | 280/1054 [53:58<2:29:10, 11.56s/it]\n",
      "27%|██▋       | 281/1054 [54:09<2:28:58, 11.56s/it]\n",
      "27%|██▋       | 282/1054 [54:21<2:28:46, 11.56s/it]\n",
      "27%|██▋       | 283/1054 [54:32<2:28:35, 11.56s/it]\n",
      "27%|██▋       | 284/1054 [54:44<2:28:23, 11.56s/it]\n",
      "27%|██▋       | 285/1054 [54:55<2:28:12, 11.56s/it]\n",
      "27%|██▋       | 286/1054 [55:07<2:28:00, 11.56s/it]\n",
      "27%|██▋       | 287/1054 [55:19<2:27:49, 11.56s/it]\n",
      "27%|██▋       | 288/1054 [55:30<2:27:37, 11.56s/it]\n",
      "27%|██▋       | 289/1054 [55:42<2:27:26, 11.56s/it]\n",
      "28%|██▊       | 290/1054 [55:53<2:27:14, 11.56s/it]\n",
      "{'loss': 1.3659, 'learning_rate': 0.00014497153700189754, 'epoch': 0.55}\n",
      "28%|██▊       | 290/1054 [55:53<2:27:14, 11.56s/it]\n",
      "28%|██▊       | 291/1054 [56:05<2:27:03, 11.56s/it]\n",
      "28%|██▊       | 292/1054 [56:16<2:26:51, 11.56s/it]\n",
      "28%|██▊       | 293/1054 [56:28<2:26:39, 11.56s/it]\n",
      "28%|██▊       | 294/1054 [56:40<2:26:28, 11.56s/it]\n",
      "28%|██▊       | 295/1054 [56:51<2:26:16, 11.56s/it]\n",
      "28%|██▊       | 296/1054 [57:03<2:26:05, 11.56s/it]\n",
      "28%|██▊       | 297/1054 [57:14<2:25:53, 11.56s/it]\n",
      "28%|██▊       | 298/1054 [57:26<2:25:41, 11.56s/it]\n",
      "28%|██▊       | 299/1054 [57:37<2:25:30, 11.56s/it]\n",
      "28%|██▊       | 300/1054 [57:49<2:25:18, 11.56s/it]\n",
      "{'loss': 1.4738, 'learning_rate': 0.00014307400379506643, 'epoch': 0.57}\n",
      "28%|██▊       | 300/1054 [57:49<2:25:18, 11.56s/it]\n",
      "29%|██▊       | 301/1054 [58:01<2:25:07, 11.56s/it]\n",
      "29%|██▊       | 302/1054 [58:12<2:24:55, 11.56s/it]\n",
      "29%|██▊       | 303/1054 [58:24<2:24:44, 11.56s/it]\n",
      "29%|██▉       | 304/1054 [58:35<2:24:32, 11.56s/it]\n",
      "29%|██▉       | 305/1054 [58:47<2:24:21, 11.56s/it]\n",
      "29%|██▉       | 306/1054 [58:58<2:24:09, 11.56s/it]\n",
      "29%|██▉       | 307/1054 [59:10<2:23:57, 11.56s/it]\n",
      "29%|██▉       | 308/1054 [59:21<2:23:46, 11.56s/it]\n",
      "29%|██▉       | 309/1054 [59:33<2:23:34, 11.56s/it]\n",
      "29%|██▉       | 310/1054 [59:45<2:23:23, 11.56s/it]\n",
      "{'loss': 1.3469, 'learning_rate': 0.0001411764705882353, 'epoch': 0.59}\n",
      "29%|██▉       | 310/1054 [59:45<2:23:23, 11.56s/it]\n",
      "30%|██▉       | 311/1054 [59:56<2:23:12, 11.56s/it]\n",
      "30%|██▉       | 312/1054 [1:00:08<2:23:00, 11.56s/it]\n",
      "30%|██▉       | 313/1054 [1:00:19<2:22:48, 11.56s/it]\n",
      "30%|██▉       | 314/1054 [1:00:31<2:22:36, 11.56s/it]\n",
      "30%|██▉       | 315/1054 [1:00:42<2:22:25, 11.56s/it]\n",
      "30%|██▉       | 316/1054 [1:00:54<2:22:13, 11.56s/it]\n",
      "30%|███       | 317/1054 [1:01:06<2:22:02, 11.56s/it]\n",
      "30%|███       | 318/1054 [1:01:17<2:21:50, 11.56s/it]\n",
      "30%|███       | 319/1054 [1:01:29<2:21:39, 11.56s/it]\n",
      "30%|███       | 320/1054 [1:01:40<2:21:27, 11.56s/it]\n",
      "{'loss': 1.3583, 'learning_rate': 0.0001392789373814042, 'epoch': 0.61}\n",
      "30%|███       | 320/1054 [1:01:40<2:21:27, 11.56s/it]\n",
      "30%|███       | 321/1054 [1:01:52<2:21:16, 11.56s/it]\n",
      "31%|███       | 322/1054 [1:02:03<2:21:04, 11.56s/it]\n",
      "31%|███       | 323/1054 [1:02:15<2:20:52, 11.56s/it]\n",
      "31%|███       | 324/1054 [1:02:26<2:20:41, 11.56s/it]\n",
      "31%|███       | 325/1054 [1:02:38<2:20:29, 11.56s/it]\n",
      "31%|███       | 326/1054 [1:02:50<2:20:18, 11.56s/it]\n",
      "31%|███       | 327/1054 [1:03:01<2:20:06, 11.56s/it]\n",
      "31%|███       | 328/1054 [1:03:13<2:19:54, 11.56s/it]\n",
      "31%|███       | 329/1054 [1:03:24<2:19:43, 11.56s/it]\n",
      "31%|███▏      | 330/1054 [1:03:36<2:19:31, 11.56s/it]\n",
      "{'loss': 1.3778, 'learning_rate': 0.00013738140417457305, 'epoch': 0.63}\n",
      "31%|███▏      | 330/1054 [1:03:36<2:19:31, 11.56s/it]\n",
      "31%|███▏      | 331/1054 [1:03:47<2:19:20, 11.56s/it]\n",
      "31%|███▏      | 332/1054 [1:03:59<2:19:08, 11.56s/it]\n",
      "32%|███▏      | 333/1054 [1:04:11<2:18:57, 11.56s/it]\n",
      "32%|███▏      | 334/1054 [1:04:22<2:18:45, 11.56s/it]\n",
      "32%|███▏      | 335/1054 [1:04:34<2:18:33, 11.56s/it]\n",
      "32%|███▏      | 336/1054 [1:04:45<2:18:22, 11.56s/it]\n",
      "32%|███▏      | 337/1054 [1:04:57<2:18:10, 11.56s/it]\n",
      "32%|███▏      | 338/1054 [1:05:08<2:17:59, 11.56s/it]\n",
      "32%|███▏      | 339/1054 [1:05:20<2:17:48, 11.56s/it]\n",
      "32%|███▏      | 340/1054 [1:05:31<2:17:36, 11.56s/it]\n",
      "{'loss': 1.3212, 'learning_rate': 0.00013548387096774193, 'epoch': 0.65}\n",
      "32%|███▏      | 340/1054 [1:05:31<2:17:36, 11.56s/it]\n",
      "32%|███▏      | 341/1054 [1:05:43<2:17:25, 11.56s/it]\n",
      "32%|███▏      | 342/1054 [1:05:55<2:17:13, 11.56s/it]\n",
      "33%|███▎      | 343/1054 [1:06:06<2:17:01, 11.56s/it]\n",
      "33%|███▎      | 344/1054 [1:06:18<2:16:51, 11.57s/it]\n",
      "33%|███▎      | 345/1054 [1:06:29<2:16:42, 11.57s/it]\n",
      "33%|███▎      | 346/1054 [1:06:41<2:16:32, 11.57s/it]\n",
      "33%|███▎      | 347/1054 [1:06:52<2:16:21, 11.57s/it]\n",
      "33%|███▎      | 348/1054 [1:07:04<2:16:09, 11.57s/it]\n",
      "33%|███▎      | 349/1054 [1:07:16<2:15:56, 11.57s/it]\n",
      "33%|███▎      | 350/1054 [1:07:27<2:15:43, 11.57s/it]\n",
      "{'loss': 1.3783, 'learning_rate': 0.00013358633776091084, 'epoch': 0.66}\n",
      "33%|███▎      | 350/1054 [1:07:27<2:15:43, 11.57s/it]\n",
      "33%|███▎      | 351/1054 [1:07:39<2:15:32, 11.57s/it]\n",
      "33%|███▎      | 352/1054 [1:07:50<2:15:19, 11.57s/it]\n",
      "33%|███▎      | 353/1054 [1:08:02<2:15:08, 11.57s/it]\n",
      "34%|███▎      | 354/1054 [1:08:13<2:14:56, 11.57s/it]\n",
      "34%|███▎      | 355/1054 [1:08:25<2:14:43, 11.56s/it]\n",
      "34%|███▍      | 356/1054 [1:08:37<2:14:32, 11.56s/it]\n",
      "34%|███▍      | 357/1054 [1:08:48<2:14:20, 11.56s/it]\n",
      "34%|███▍      | 358/1054 [1:09:00<2:14:08, 11.56s/it]\n",
      "34%|███▍      | 359/1054 [1:09:11<2:13:57, 11.56s/it]\n",
      "34%|███▍      | 360/1054 [1:09:23<2:13:45, 11.56s/it]\n",
      "{'loss': 1.3969, 'learning_rate': 0.0001316888045540797, 'epoch': 0.68}\n",
      "34%|███▍      | 360/1054 [1:09:23<2:13:45, 11.56s/it]\n",
      "34%|███▍      | 361/1054 [1:09:34<2:13:33, 11.56s/it]\n",
      "34%|███▍      | 362/1054 [1:09:46<2:13:22, 11.56s/it]\n",
      "34%|███▍      | 363/1054 [1:09:58<2:13:10, 11.56s/it]\n",
      "35%|███▍      | 364/1054 [1:10:09<2:12:59, 11.56s/it]\n",
      "35%|███▍      | 365/1054 [1:10:21<2:12:47, 11.56s/it]\n",
      "35%|███▍      | 366/1054 [1:10:32<2:12:36, 11.56s/it]\n",
      "35%|███▍      | 367/1054 [1:10:44<2:12:24, 11.56s/it]\n",
      "35%|███▍      | 368/1054 [1:10:55<2:12:13, 11.56s/it]\n",
      "35%|███▌      | 369/1054 [1:11:07<2:12:01, 11.56s/it]\n",
      "35%|███▌      | 370/1054 [1:11:18<2:11:49, 11.56s/it]\n",
      "{'loss': 1.4047, 'learning_rate': 0.00012979127134724858, 'epoch': 0.7}\n",
      "35%|███▌      | 370/1054 [1:11:18<2:11:49, 11.56s/it]\n",
      "35%|███▌      | 371/1054 [1:11:30<2:11:38, 11.56s/it]\n",
      "35%|███▌      | 372/1054 [1:11:42<2:11:27, 11.56s/it]\n",
      "35%|███▌      | 373/1054 [1:11:53<2:11:15, 11.56s/it]\n",
      "35%|███▌      | 374/1054 [1:12:05<2:11:03, 11.56s/it]\n",
      "36%|███▌      | 375/1054 [1:12:16<2:10:51, 11.56s/it]\n",
      "36%|███▌      | 376/1054 [1:12:28<2:10:40, 11.56s/it]\n",
      "36%|███▌      | 377/1054 [1:12:39<2:10:28, 11.56s/it]\n",
      "36%|███▌      | 378/1054 [1:12:51<2:10:16, 11.56s/it]\n",
      "36%|███▌      | 379/1054 [1:13:03<2:10:04, 11.56s/it]\n",
      "36%|███▌      | 380/1054 [1:13:14<2:09:53, 11.56s/it]\n",
      "{'loss': 1.3843, 'learning_rate': 0.00012789373814041749, 'epoch': 0.72}\n",
      "36%|███▌      | 380/1054 [1:13:14<2:09:53, 11.56s/it]\n",
      "36%|███▌      | 381/1054 [1:13:26<2:09:42, 11.56s/it]\n",
      "36%|███▌      | 382/1054 [1:13:37<2:09:30, 11.56s/it]\n",
      "36%|███▋      | 383/1054 [1:13:49<2:09:19, 11.56s/it]\n",
      "36%|███▋      | 384/1054 [1:14:00<2:09:07, 11.56s/it]\n",
      "37%|███▋      | 385/1054 [1:14:12<2:08:56, 11.56s/it]\n",
      "37%|███▋      | 386/1054 [1:14:23<2:08:44, 11.56s/it]\n",
      "37%|███▋      | 387/1054 [1:14:35<2:08:33, 11.56s/it]\n",
      "37%|███▋      | 388/1054 [1:14:47<2:08:21, 11.56s/it]\n",
      "37%|███▋      | 389/1054 [1:14:58<2:08:10, 11.56s/it]\n",
      "37%|███▋      | 390/1054 [1:15:10<2:07:58, 11.56s/it]\n",
      "{'loss': 1.3715, 'learning_rate': 0.00012599620493358634, 'epoch': 0.74}\n",
      "37%|███▋      | 390/1054 [1:15:10<2:07:58, 11.56s/it]\n",
      "37%|███▋      | 391/1054 [1:15:21<2:07:47, 11.56s/it]\n",
      "37%|███▋      | 392/1054 [1:15:33<2:07:35, 11.56s/it]\n",
      "37%|███▋      | 393/1054 [1:15:44<2:07:23, 11.56s/it]\n",
      "37%|███▋      | 394/1054 [1:15:56<2:07:11, 11.56s/it]\n",
      "37%|███▋      | 395/1054 [1:16:08<2:07:00, 11.56s/it]\n",
      "38%|███▊      | 396/1054 [1:16:19<2:06:48, 11.56s/it]\n",
      "38%|███▊      | 397/1054 [1:16:31<2:06:37, 11.56s/it]\n",
      "38%|███▊      | 398/1054 [1:16:42<2:06:25, 11.56s/it]\n",
      "38%|███▊      | 399/1054 [1:16:54<2:06:14, 11.56s/it]\n",
      "38%|███▊      | 400/1054 [1:17:05<2:06:02, 11.56s/it]\n",
      "{'loss': 1.3929, 'learning_rate': 0.00012409867172675522, 'epoch': 0.76}\n",
      "38%|███▊      | 400/1054 [1:17:05<2:06:02, 11.56s/it]\n",
      "38%|███▊      | 401/1054 [1:17:17<2:05:51, 11.56s/it]\n",
      "38%|███▊      | 402/1054 [1:17:28<2:05:39, 11.56s/it]\n",
      "38%|███▊      | 403/1054 [1:17:40<2:05:28, 11.56s/it]\n",
      "38%|███▊      | 404/1054 [1:17:52<2:05:17, 11.56s/it]\n",
      "38%|███▊      | 405/1054 [1:18:03<2:05:05, 11.56s/it]\n",
      "39%|███▊      | 406/1054 [1:18:15<2:04:53, 11.56s/it]\n",
      "39%|███▊      | 407/1054 [1:18:26<2:04:41, 11.56s/it]\n",
      "39%|███▊      | 408/1054 [1:18:38<2:04:30, 11.56s/it]\n",
      "39%|███▉      | 409/1054 [1:18:49<2:04:18, 11.56s/it]\n",
      "39%|███▉      | 410/1054 [1:19:01<2:04:07, 11.56s/it]\n",
      "{'loss': 1.4588, 'learning_rate': 0.0001222011385199241, 'epoch': 0.78}\n",
      "39%|███▉      | 410/1054 [1:19:01<2:04:07, 11.56s/it]\n",
      "39%|███▉      | 411/1054 [1:19:13<2:03:55, 11.56s/it]\n",
      "39%|███▉      | 412/1054 [1:19:24<2:03:44, 11.56s/it]\n",
      "39%|███▉      | 413/1054 [1:19:36<2:03:32, 11.56s/it]\n",
      "39%|███▉      | 414/1054 [1:19:47<2:03:21, 11.56s/it]\n",
      "39%|███▉      | 415/1054 [1:19:59<2:03:09, 11.56s/it]\n",
      "39%|███▉      | 416/1054 [1:20:10<2:02:57, 11.56s/it]\n",
      "40%|███▉      | 417/1054 [1:20:22<2:02:45, 11.56s/it]\n",
      "40%|███▉      | 418/1054 [1:20:34<2:02:34, 11.56s/it]\n",
      "40%|███▉      | 419/1054 [1:20:45<2:02:23, 11.56s/it]\n",
      "40%|███▉      | 420/1054 [1:20:57<2:02:11, 11.56s/it]\n",
      "{'loss': 1.3383, 'learning_rate': 0.00012030360531309299, 'epoch': 0.8}\n",
      "40%|███▉      | 420/1054 [1:20:57<2:02:11, 11.56s/it]\n",
      "40%|███▉      | 421/1054 [1:21:08<2:02:00, 11.56s/it]\n",
      "40%|████      | 422/1054 [1:21:20<2:01:48, 11.56s/it]\n",
      "40%|████      | 423/1054 [1:21:31<2:01:36, 11.56s/it]\n",
      "40%|████      | 424/1054 [1:21:43<2:01:24, 11.56s/it]\n",
      "40%|████      | 425/1054 [1:21:54<2:01:13, 11.56s/it]\n",
      "40%|████      | 426/1054 [1:22:06<2:01:01, 11.56s/it]\n",
      "41%|████      | 427/1054 [1:22:18<2:00:50, 11.56s/it]\n",
      "41%|████      | 428/1054 [1:22:29<2:00:38, 11.56s/it]\n",
      "41%|████      | 429/1054 [1:22:41<2:00:27, 11.56s/it]\n",
      "41%|████      | 430/1054 [1:22:52<2:00:15, 11.56s/it]\n",
      "{'loss': 1.3343, 'learning_rate': 0.00011840607210626187, 'epoch': 0.82}\n",
      "41%|████      | 430/1054 [1:22:52<2:00:15, 11.56s/it]\n",
      "41%|████      | 431/1054 [1:23:04<2:00:04, 11.56s/it]\n",
      "41%|████      | 432/1054 [1:23:15<1:59:52, 11.56s/it]\n",
      "41%|████      | 433/1054 [1:23:27<1:59:40, 11.56s/it]\n",
      "41%|████      | 434/1054 [1:23:39<1:59:29, 11.56s/it]\n",
      "41%|████▏     | 435/1054 [1:23:50<1:59:17, 11.56s/it]\n",
      "41%|████▏     | 436/1054 [1:24:02<1:59:06, 11.56s/it]\n",
      "41%|████▏     | 437/1054 [1:24:13<1:58:54, 11.56s/it]\n",
      "42%|████▏     | 438/1054 [1:24:25<1:58:42, 11.56s/it]\n",
      "42%|████▏     | 439/1054 [1:24:36<1:58:31, 11.56s/it]\n",
      "42%|████▏     | 440/1054 [1:24:48<1:58:19, 11.56s/it]\n",
      "{'loss': 1.3887, 'learning_rate': 0.00011650853889943074, 'epoch': 0.83}\n",
      "42%|████▏     | 440/1054 [1:24:48<1:58:19, 11.56s/it]\n",
      "42%|████▏     | 441/1054 [1:24:59<1:58:08, 11.56s/it]\n",
      "42%|████▏     | 442/1054 [1:25:11<1:57:57, 11.56s/it]\n",
      "42%|████▏     | 443/1054 [1:25:23<1:57:45, 11.56s/it]\n",
      "42%|████▏     | 444/1054 [1:25:34<1:57:34, 11.56s/it]\n",
      "42%|████▏     | 445/1054 [1:25:46<1:57:22, 11.56s/it]\n",
      "42%|████▏     | 446/1054 [1:25:57<1:57:10, 11.56s/it]\n",
      "42%|████▏     | 447/1054 [1:26:09<1:56:59, 11.56s/it]\n",
      "43%|████▎     | 448/1054 [1:26:20<1:56:47, 11.56s/it]\n",
      "43%|████▎     | 449/1054 [1:26:32<1:56:35, 11.56s/it]\n",
      "43%|████▎     | 450/1054 [1:26:44<1:56:24, 11.56s/it]\n",
      "{'loss': 1.426, 'learning_rate': 0.00011461100569259962, 'epoch': 0.85}\n",
      "43%|████▎     | 450/1054 [1:26:44<1:56:24, 11.56s/it]\n",
      "43%|████▎     | 451/1054 [1:26:55<1:56:12, 11.56s/it]\n",
      "43%|████▎     | 452/1054 [1:27:07<1:56:01, 11.56s/it]\n",
      "43%|████▎     | 453/1054 [1:27:18<1:55:50, 11.56s/it]\n",
      "43%|████▎     | 454/1054 [1:27:30<1:55:38, 11.56s/it]\n",
      "43%|████▎     | 455/1054 [1:27:41<1:55:26, 11.56s/it]\n",
      "43%|████▎     | 456/1054 [1:27:53<1:55:15, 11.56s/it]\n",
      "43%|████▎     | 457/1054 [1:28:05<1:55:03, 11.56s/it]\n",
      "43%|████▎     | 458/1054 [1:28:16<1:54:52, 11.56s/it]\n",
      "44%|████▎     | 459/1054 [1:28:28<1:54:40, 11.56s/it]\n",
      "44%|████▎     | 460/1054 [1:28:39<1:54:28, 11.56s/it]\n",
      "{'loss': 1.3951, 'learning_rate': 0.00011271347248576852, 'epoch': 0.87}\n",
      "44%|████▎     | 460/1054 [1:28:39<1:54:28, 11.56s/it]\n",
      "44%|████▎     | 461/1054 [1:28:51<1:54:17, 11.56s/it]\n",
      "44%|████▍     | 462/1054 [1:29:02<1:54:05, 11.56s/it]\n",
      "44%|████▍     | 463/1054 [1:29:14<1:53:53, 11.56s/it]\n",
      "44%|████▍     | 464/1054 [1:29:25<1:53:42, 11.56s/it]\n",
      "44%|████▍     | 465/1054 [1:29:37<1:53:30, 11.56s/it]\n",
      "44%|████▍     | 466/1054 [1:29:49<1:53:19, 11.56s/it]\n",
      "44%|████▍     | 467/1054 [1:30:00<1:53:07, 11.56s/it]\n",
      "44%|████▍     | 468/1054 [1:30:12<1:52:56, 11.56s/it]\n",
      "44%|████▍     | 469/1054 [1:30:23<1:52:44, 11.56s/it]\n",
      "45%|████▍     | 470/1054 [1:30:35<1:52:32, 11.56s/it]\n",
      "{'loss': 1.3548, 'learning_rate': 0.00011081593927893739, 'epoch': 0.89}\n",
      "45%|████▍     | 470/1054 [1:30:35<1:52:32, 11.56s/it]\n",
      "45%|████▍     | 471/1054 [1:30:46<1:52:21, 11.56s/it]\n",
      "45%|████▍     | 472/1054 [1:30:58<1:52:09, 11.56s/it]\n",
      "45%|████▍     | 473/1054 [1:31:10<1:51:58, 11.56s/it]\n",
      "45%|████▍     | 474/1054 [1:31:21<1:51:46, 11.56s/it]\n",
      "45%|████▌     | 475/1054 [1:31:33<1:51:35, 11.56s/it]\n",
      "45%|████▌     | 476/1054 [1:31:44<1:51:23, 11.56s/it]\n",
      "45%|████▌     | 477/1054 [1:31:56<1:51:11, 11.56s/it]\n",
      "45%|████▌     | 478/1054 [1:32:07<1:51:00, 11.56s/it]\n",
      "45%|████▌     | 479/1054 [1:32:19<1:50:48, 11.56s/it]\n",
      "46%|████▌     | 480/1054 [1:32:30<1:50:37, 11.56s/it]\n",
      "{'loss': 1.3944, 'learning_rate': 0.00010891840607210625, 'epoch': 0.91}\n",
      "46%|████▌     | 480/1054 [1:32:30<1:50:37, 11.56s/it]\n",
      "46%|████▌     | 481/1054 [1:32:42<1:50:25, 11.56s/it]\n",
      "46%|████▌     | 482/1054 [1:32:54<1:50:14, 11.56s/it]\n",
      "46%|████▌     | 483/1054 [1:33:05<1:50:02, 11.56s/it]\n",
      "46%|████▌     | 484/1054 [1:33:17<1:49:50, 11.56s/it]\n",
      "46%|████▌     | 485/1054 [1:33:28<1:49:39, 11.56s/it]\n",
      "46%|████▌     | 486/1054 [1:33:40<1:49:27, 11.56s/it]\n",
      "46%|████▌     | 487/1054 [1:33:51<1:49:16, 11.56s/it]\n",
      "46%|████▋     | 488/1054 [1:34:03<1:49:04, 11.56s/it]\n",
      "46%|████▋     | 489/1054 [1:34:15<1:48:53, 11.56s/it]\n",
      "46%|████▋     | 490/1054 [1:34:26<1:48:41, 11.56s/it]\n",
      "{'loss': 1.359, 'learning_rate': 0.00010702087286527515, 'epoch': 0.93}\n",
      "46%|████▋     | 490/1054 [1:34:26<1:48:41, 11.56s/it]\n",
      "47%|████▋     | 491/1054 [1:34:38<1:48:30, 11.56s/it]\n",
      "47%|████▋     | 492/1054 [1:34:49<1:48:18, 11.56s/it]\n",
      "47%|████▋     | 493/1054 [1:35:01<1:48:07, 11.56s/it]\n",
      "47%|████▋     | 494/1054 [1:35:12<1:47:55, 11.56s/it]\n",
      "47%|████▋     | 495/1054 [1:35:24<1:47:43, 11.56s/it]\n",
      "47%|████▋     | 496/1054 [1:35:35<1:47:32, 11.56s/it]\n",
      "47%|████▋     | 497/1054 [1:35:47<1:47:20, 11.56s/it]\n",
      "47%|████▋     | 498/1054 [1:35:59<1:47:09, 11.56s/it]\n",
      "47%|████▋     | 499/1054 [1:36:10<1:46:57, 11.56s/it]\n",
      "47%|████▋     | 500/1054 [1:36:22<1:46:46, 11.56s/it]\n",
      "{'loss': 1.4098, 'learning_rate': 0.00010512333965844403, 'epoch': 0.95}\n",
      "47%|████▋     | 500/1054 [1:36:22<1:46:46, 11.56s/it]\n",
      "48%|████▊     | 501/1054 [1:36:33<1:46:35, 11.56s/it]\n",
      "48%|████▊     | 502/1054 [1:36:45<1:46:23, 11.56s/it]\n",
      "48%|████▊     | 503/1054 [1:36:56<1:46:11, 11.56s/it]\n",
      "48%|████▊     | 504/1054 [1:37:08<1:45:59, 11.56s/it]\n",
      "48%|████▊     | 505/1054 [1:37:20<1:45:48, 11.56s/it]\n",
      "48%|████▊     | 506/1054 [1:37:31<1:45:36, 11.56s/it]\n",
      "48%|████▊     | 507/1054 [1:37:43<1:45:25, 11.56s/it]\n",
      "48%|████▊     | 508/1054 [1:37:54<1:45:13, 11.56s/it]\n",
      "48%|████▊     | 509/1054 [1:38:06<1:45:02, 11.56s/it]\n",
      "48%|████▊     | 510/1054 [1:38:17<1:44:51, 11.56s/it]\n",
      "{'loss': 1.4308, 'learning_rate': 0.0001032258064516129, 'epoch': 0.97}\n",
      "48%|████▊     | 510/1054 [1:38:17<1:44:51, 11.56s/it]\n",
      "48%|████▊     | 511/1054 [1:38:29<1:44:39, 11.56s/it]\n",
      "49%|████▊     | 512/1054 [1:38:40<1:44:27, 11.56s/it]\n",
      "49%|████▊     | 513/1054 [1:38:52<1:44:15, 11.56s/it]\n",
      "49%|████▉     | 514/1054 [1:39:04<1:44:04, 11.56s/it]\n",
      "49%|████▉     | 515/1054 [1:39:15<1:43:52, 11.56s/it]\n",
      "49%|████▉     | 516/1054 [1:39:27<1:43:41, 11.56s/it]\n",
      "49%|████▉     | 517/1054 [1:39:38<1:43:29, 11.56s/it]\n",
      "49%|████▉     | 518/1054 [1:39:50<1:43:17, 11.56s/it]\n",
      "49%|████▉     | 519/1054 [1:40:01<1:43:06, 11.56s/it]\n",
      "49%|████▉     | 520/1054 [1:40:13<1:42:54, 11.56s/it]\n",
      "{'loss': 1.4132, 'learning_rate': 0.00010132827324478178, 'epoch': 0.99}\n",
      "49%|████▉     | 520/1054 [1:40:13<1:42:54, 11.56s/it]\n",
      "49%|████▉     | 521/1054 [1:40:25<1:42:43, 11.56s/it]\n",
      "50%|████▉     | 522/1054 [1:40:36<1:42:32, 11.56s/it]\n",
      "50%|████▉     | 523/1054 [1:40:48<1:42:20, 11.56s/it]\n",
      "50%|████▉     | 524/1054 [1:40:59<1:42:08, 11.56s/it]\n",
      "50%|████▉     | 525/1054 [1:41:11<1:41:57, 11.56s/it]\n",
      "50%|████▉     | 526/1054 [1:41:22<1:41:45, 11.56s/it]\n",
      "50%|█████     | 527/1054 [1:41:34<1:41:32, 11.56s/it]\n",
      "50%|█████     | 528/1054 [1:41:46<1:41:22, 11.56s/it]\n",
      "50%|█████     | 529/1054 [1:41:57<1:41:11, 11.56s/it]\n",
      "50%|█████     | 530/1054 [1:42:09<1:40:59, 11.56s/it]\n",
      "{'loss': 1.3721, 'learning_rate': 9.943074003795067e-05, 'epoch': 1.01}\n",
      "50%|█████     | 530/1054 [1:42:09<1:40:59, 11.56s/it]\n",
      "50%|█████     | 531/1054 [1:42:20<1:40:47, 11.56s/it]\n",
      "50%|█████     | 532/1054 [1:42:32<1:40:36, 11.56s/it]\n",
      "51%|█████     | 533/1054 [1:42:43<1:40:24, 11.56s/it]\n",
      "51%|█████     | 534/1054 [1:42:55<1:40:12, 11.56s/it]\n",
      "51%|█████     | 535/1054 [1:43:06<1:40:01, 11.56s/it]\n",
      "51%|█████     | 536/1054 [1:43:18<1:39:50, 11.56s/it]\n",
      "51%|█████     | 537/1054 [1:43:30<1:39:38, 11.56s/it]\n",
      "51%|█████     | 538/1054 [1:43:41<1:39:26, 11.56s/it]\n",
      "51%|█████     | 539/1054 [1:43:53<1:39:15, 11.56s/it]\n",
      "51%|█████     | 540/1054 [1:44:04<1:39:03, 11.56s/it]\n",
      "{'loss': 1.4077, 'learning_rate': 9.753320683111955e-05, 'epoch': 1.02}\n",
      "51%|█████     | 540/1054 [1:44:04<1:39:03, 11.56s/it]\n",
      "51%|█████▏    | 541/1054 [1:44:16<1:38:52, 11.56s/it]\n",
      "51%|█████▏    | 542/1054 [1:44:27<1:38:40, 11.56s/it]\n",
      "52%|█████▏    | 543/1054 [1:44:39<1:38:29, 11.56s/it]\n",
      "52%|█████▏    | 544/1054 [1:44:51<1:38:17, 11.56s/it]\n",
      "52%|█████▏    | 545/1054 [1:45:02<1:38:05, 11.56s/it]\n",
      "52%|█████▏    | 546/1054 [1:45:14<1:37:53, 11.56s/it]\n",
      "52%|█████▏    | 547/1054 [1:45:25<1:37:42, 11.56s/it]\n",
      "52%|█████▏    | 548/1054 [1:45:37<1:37:30, 11.56s/it]\n",
      "52%|█████▏    | 549/1054 [1:45:48<1:37:19, 11.56s/it]\n",
      "52%|█████▏    | 550/1054 [1:46:00<1:37:07, 11.56s/it]\n",
      "{'loss': 1.3077, 'learning_rate': 9.563567362428842e-05, 'epoch': 1.04}\n",
      "52%|█████▏    | 550/1054 [1:46:00<1:37:07, 11.56s/it]\n",
      "52%|█████▏    | 551/1054 [1:46:11<1:36:56, 11.56s/it]\n",
      "52%|█████▏    | 552/1054 [1:46:23<1:36:44, 11.56s/it]\n",
      "52%|█████▏    | 553/1054 [1:46:35<1:36:33, 11.56s/it]\n",
      "53%|█████▎    | 554/1054 [1:46:46<1:36:21, 11.56s/it]\n",
      "53%|█████▎    | 555/1054 [1:46:58<1:36:10, 11.56s/it]\n",
      "53%|█████▎    | 556/1054 [1:47:09<1:35:58, 11.56s/it]\n",
      "53%|█████▎    | 557/1054 [1:47:21<1:35:46, 11.56s/it]\n",
      "53%|█████▎    | 558/1054 [1:47:32<1:35:35, 11.56s/it]\n",
      "53%|█████▎    | 559/1054 [1:47:44<1:35:23, 11.56s/it]\n",
      "53%|█████▎    | 560/1054 [1:47:56<1:35:12, 11.56s/it]\n",
      "{'loss': 1.3177, 'learning_rate': 9.373814041745731e-05, 'epoch': 1.06}\n",
      "53%|█████▎    | 560/1054 [1:47:56<1:35:12, 11.56s/it]\n",
      "53%|█████▎    | 561/1054 [1:48:07<1:35:00, 11.56s/it]\n",
      "53%|█████▎    | 562/1054 [1:48:19<1:34:49, 11.56s/it]\n",
      "53%|█████▎    | 563/1054 [1:48:30<1:34:37, 11.56s/it]\n",
      "54%|█████▎    | 564/1054 [1:48:42<1:34:26, 11.56s/it]\n",
      "54%|█████▎    | 565/1054 [1:48:53<1:34:14, 11.56s/it]\n",
      "54%|█████▎    | 566/1054 [1:49:05<1:34:02, 11.56s/it]\n",
      "54%|█████▍    | 567/1054 [1:49:16<1:33:51, 11.56s/it]\n",
      "54%|█████▍    | 568/1054 [1:49:28<1:33:39, 11.56s/it]\n",
      "54%|█████▍    | 569/1054 [1:49:40<1:33:28, 11.56s/it]\n",
      "54%|█████▍    | 570/1054 [1:49:51<1:33:16, 11.56s/it]\n",
      "{'loss': 1.263, 'learning_rate': 9.18406072106262e-05, 'epoch': 1.08}\n",
      "54%|█████▍    | 570/1054 [1:49:51<1:33:16, 11.56s/it]\n",
      "54%|█████▍    | 571/1054 [1:50:03<1:33:05, 11.56s/it]\n",
      "54%|█████▍    | 572/1054 [1:50:14<1:32:53, 11.56s/it]\n",
      "54%|█████▍    | 573/1054 [1:50:26<1:32:41, 11.56s/it]\n",
      "54%|█████▍    | 574/1054 [1:50:37<1:32:30, 11.56s/it]\n",
      "55%|█████▍    | 575/1054 [1:50:49<1:32:18, 11.56s/it]\n",
      "55%|█████▍    | 576/1054 [1:51:01<1:32:07, 11.56s/it]\n",
      "55%|█████▍    | 577/1054 [1:51:12<1:31:55, 11.56s/it]\n",
      "55%|█████▍    | 578/1054 [1:51:24<1:31:44, 11.56s/it]\n",
      "55%|█████▍    | 579/1054 [1:51:35<1:31:32, 11.56s/it]\n",
      "55%|█████▌    | 580/1054 [1:51:47<1:31:21, 11.56s/it]\n",
      "{'loss': 1.2705, 'learning_rate': 8.994307400379507e-05, 'epoch': 1.1}\n",
      "55%|█████▌    | 580/1054 [1:51:47<1:31:21, 11.56s/it]\n",
      "55%|█████▌    | 581/1054 [1:51:58<1:31:09, 11.56s/it]\n",
      "55%|█████▌    | 582/1054 [1:52:10<1:30:58, 11.56s/it]\n",
      "55%|█████▌    | 583/1054 [1:52:22<1:30:46, 11.56s/it]\n",
      "55%|█████▌    | 584/1054 [1:52:33<1:30:34, 11.56s/it]\n",
      "56%|█████▌    | 585/1054 [1:52:45<1:30:23, 11.56s/it]\n",
      "56%|█████▌    | 586/1054 [1:52:56<1:30:11, 11.56s/it]\n",
      "56%|█████▌    | 587/1054 [1:53:08<1:29:59, 11.56s/it]\n",
      "56%|█████▌    | 588/1054 [1:53:19<1:29:48, 11.56s/it]\n",
      "56%|█████▌    | 589/1054 [1:53:31<1:29:36, 11.56s/it]\n",
      "56%|█████▌    | 590/1054 [1:53:42<1:29:25, 11.56s/it]\n",
      "{'loss': 1.3736, 'learning_rate': 8.804554079696396e-05, 'epoch': 1.12}\n",
      "56%|█████▌    | 590/1054 [1:53:42<1:29:25, 11.56s/it]\n",
      "56%|█████▌    | 591/1054 [1:53:54<1:29:13, 11.56s/it]\n",
      "56%|█████▌    | 592/1054 [1:54:06<1:29:02, 11.56s/it]\n",
      "56%|█████▋    | 593/1054 [1:54:17<1:28:50, 11.56s/it]\n",
      "56%|█████▋    | 594/1054 [1:54:29<1:28:39, 11.56s/it]\n",
      "56%|█████▋    | 595/1054 [1:54:40<1:28:27, 11.56s/it]\n",
      "57%|█████▋    | 596/1054 [1:54:52<1:28:16, 11.56s/it]\n",
      "57%|█████▋    | 597/1054 [1:55:03<1:28:04, 11.56s/it]\n",
      "57%|█████▋    | 598/1054 [1:55:15<1:27:52, 11.56s/it]\n",
      "57%|█████▋    | 599/1054 [1:55:27<1:27:41, 11.56s/it]\n",
      "57%|█████▋    | 600/1054 [1:55:38<1:27:29, 11.56s/it]\n",
      "{'loss': 1.3703, 'learning_rate': 8.614800759013283e-05, 'epoch': 1.14}\n",
      "57%|█████▋    | 600/1054 [1:55:38<1:27:29, 11.56s/it]\n",
      "57%|█████▋    | 601/1054 [1:55:50<1:27:18, 11.56s/it]\n",
      "57%|█████▋    | 602/1054 [1:56:01<1:27:06, 11.56s/it]\n",
      "57%|█████▋    | 603/1054 [1:56:13<1:26:55, 11.56s/it]\n",
      "57%|█████▋    | 604/1054 [1:56:24<1:26:43, 11.56s/it]\n",
      "57%|█████▋    | 605/1054 [1:56:36<1:26:31, 11.56s/it]\n",
      "57%|█████▋    | 606/1054 [1:56:47<1:26:20, 11.56s/it]\n",
      "58%|█████▊    | 607/1054 [1:56:59<1:26:08, 11.56s/it]\n",
      "58%|█████▊    | 608/1054 [1:57:11<1:25:56, 11.56s/it]\n",
      "58%|█████▊    | 609/1054 [1:57:22<1:25:45, 11.56s/it]\n",
      "58%|█████▊    | 610/1054 [1:57:34<1:25:33, 11.56s/it]\n",
      "{'loss': 1.3144, 'learning_rate': 8.425047438330171e-05, 'epoch': 1.16}\n",
      "58%|█████▊    | 610/1054 [1:57:34<1:25:33, 11.56s/it]\n",
      "58%|█████▊    | 611/1054 [1:57:45<1:25:22, 11.56s/it]\n",
      "58%|█████▊    | 612/1054 [1:57:57<1:25:11, 11.56s/it]\n",
      "58%|█████▊    | 613/1054 [1:58:08<1:24:59, 11.56s/it]\n",
      "58%|█████▊    | 614/1054 [1:58:20<1:24:47, 11.56s/it]\n",
      "58%|█████▊    | 615/1054 [1:58:32<1:24:36, 11.56s/it]\n",
      "58%|█████▊    | 616/1054 [1:58:43<1:24:24, 11.56s/it]\n",
      "59%|█████▊    | 617/1054 [1:58:55<1:24:13, 11.56s/it]\n",
      "59%|█████▊    | 618/1054 [1:59:06<1:24:01, 11.56s/it]\n",
      "59%|█████▊    | 619/1054 [1:59:18<1:23:50, 11.56s/it]\n",
      "59%|█████▉    | 620/1054 [1:59:29<1:23:38, 11.56s/it]\n",
      "{'loss': 1.3136, 'learning_rate': 8.23529411764706e-05, 'epoch': 1.18}\n",
      "59%|█████▉    | 620/1054 [1:59:29<1:23:38, 11.56s/it]\n",
      "59%|█████▉    | 621/1054 [1:59:41<1:23:27, 11.56s/it]\n",
      "59%|█████▉    | 622/1054 [1:59:52<1:23:15, 11.56s/it]\n",
      "59%|█████▉    | 623/1054 [2:00:04<1:23:03, 11.56s/it]\n",
      "59%|█████▉    | 624/1054 [2:00:16<1:22:52, 11.56s/it]\n",
      "59%|█████▉    | 625/1054 [2:00:27<1:22:40, 11.56s/it]\n",
      "59%|█████▉    | 626/1054 [2:00:39<1:22:29, 11.56s/it]\n",
      "59%|█████▉    | 627/1054 [2:00:50<1:22:17, 11.56s/it]\n",
      "60%|█████▉    | 628/1054 [2:01:02<1:22:06, 11.56s/it]\n",
      "60%|█████▉    | 629/1054 [2:01:13<1:21:54, 11.56s/it]\n",
      "60%|█████▉    | 630/1054 [2:01:25<1:21:43, 11.56s/it]\n",
      "{'loss': 1.3312, 'learning_rate': 8.045540796963948e-05, 'epoch': 1.2}\n",
      "60%|█████▉    | 630/1054 [2:01:25<1:21:43, 11.56s/it]\n",
      "60%|█████▉    | 631/1054 [2:01:37<1:21:31, 11.56s/it]\n",
      "60%|█████▉    | 632/1054 [2:01:48<1:21:20, 11.56s/it]\n",
      "60%|██████    | 633/1054 [2:02:00<1:21:08, 11.56s/it]\n",
      "60%|██████    | 634/1054 [2:02:11<1:20:57, 11.56s/it]\n",
      "60%|██████    | 635/1054 [2:02:23<1:20:45, 11.56s/it]\n",
      "60%|██████    | 636/1054 [2:02:34<1:20:33, 11.56s/it]\n",
      "60%|██████    | 637/1054 [2:02:46<1:20:22, 11.56s/it]\n",
      "61%|██████    | 638/1054 [2:02:57<1:20:10, 11.56s/it]\n",
      "61%|██████    | 639/1054 [2:03:09<1:19:59, 11.56s/it]\n",
      "61%|██████    | 640/1054 [2:03:21<1:19:47, 11.56s/it]\n",
      "{'loss': 1.2032, 'learning_rate': 7.855787476280835e-05, 'epoch': 1.21}\n",
      "61%|██████    | 640/1054 [2:03:21<1:19:47, 11.56s/it]\n",
      "61%|██████    | 641/1054 [2:03:32<1:19:35, 11.56s/it]\n",
      "61%|██████    | 642/1054 [2:03:44<1:19:24, 11.56s/it]\n",
      "61%|██████    | 643/1054 [2:03:55<1:19:12, 11.56s/it]\n",
      "61%|██████    | 644/1054 [2:04:07<1:19:00, 11.56s/it]\n",
      "61%|██████    | 645/1054 [2:04:18<1:18:49, 11.56s/it]\n",
      "61%|██████▏   | 646/1054 [2:04:30<1:18:37, 11.56s/it]\n",
      "61%|██████▏   | 647/1054 [2:04:42<1:18:26, 11.56s/it]\n",
      "61%|██████▏   | 648/1054 [2:04:53<1:18:14, 11.56s/it]\n",
      "62%|██████▏   | 649/1054 [2:05:05<1:18:03, 11.56s/it]\n",
      "62%|██████▏   | 650/1054 [2:05:16<1:17:51, 11.56s/it]\n",
      "{'loss': 1.2522, 'learning_rate': 7.666034155597723e-05, 'epoch': 1.23}\n",
      "62%|██████▏   | 650/1054 [2:05:16<1:17:51, 11.56s/it]\n",
      "62%|██████▏   | 651/1054 [2:05:28<1:17:40, 11.56s/it]\n",
      "62%|██████▏   | 652/1054 [2:05:39<1:17:28, 11.56s/it]\n",
      "62%|██████▏   | 653/1054 [2:05:51<1:17:16, 11.56s/it]\n",
      "62%|██████▏   | 654/1054 [2:06:03<1:17:05, 11.56s/it]\n",
      "62%|██████▏   | 655/1054 [2:06:14<1:16:53, 11.56s/it]\n",
      "62%|██████▏   | 656/1054 [2:06:26<1:16:42, 11.56s/it]\n",
      "62%|██████▏   | 657/1054 [2:06:37<1:16:30, 11.56s/it]\n",
      "62%|██████▏   | 658/1054 [2:06:49<1:16:19, 11.56s/it]\n",
      "63%|██████▎   | 659/1054 [2:07:00<1:16:07, 11.56s/it]\n",
      "63%|██████▎   | 660/1054 [2:07:12<1:15:56, 11.56s/it]\n",
      "{'loss': 1.318, 'learning_rate': 7.476280834914612e-05, 'epoch': 1.25}\n",
      "63%|██████▎   | 660/1054 [2:07:12<1:15:56, 11.56s/it]\n",
      "63%|██████▎   | 661/1054 [2:07:23<1:15:44, 11.56s/it]\n",
      "63%|██████▎   | 662/1054 [2:07:35<1:15:32, 11.56s/it]\n",
      "63%|██████▎   | 663/1054 [2:07:47<1:15:21, 11.56s/it]\n",
      "63%|██████▎   | 664/1054 [2:07:58<1:15:10, 11.56s/it]\n",
      "63%|██████▎   | 665/1054 [2:08:10<1:14:58, 11.56s/it]\n",
      "63%|██████▎   | 666/1054 [2:08:21<1:14:46, 11.56s/it]\n",
      "63%|██████▎   | 667/1054 [2:08:33<1:14:34, 11.56s/it]\n",
      "63%|██████▎   | 668/1054 [2:08:44<1:14:23, 11.56s/it]\n",
      "63%|██████▎   | 669/1054 [2:08:56<1:14:11, 11.56s/it]\n",
      "64%|██████▎   | 670/1054 [2:09:08<1:14:00, 11.56s/it]\n",
      "{'loss': 1.3001, 'learning_rate': 7.286527514231499e-05, 'epoch': 1.27}\n",
      "64%|██████▎   | 670/1054 [2:09:08<1:14:00, 11.56s/it]\n",
      "64%|██████▎   | 671/1054 [2:09:19<1:13:49, 11.56s/it]\n",
      "64%|██████▍   | 672/1054 [2:09:31<1:13:37, 11.56s/it]\n",
      "64%|██████▍   | 673/1054 [2:09:42<1:13:25, 11.56s/it]\n",
      "64%|██████▍   | 674/1054 [2:09:54<1:13:14, 11.56s/it]\n",
      "64%|██████▍   | 675/1054 [2:10:05<1:13:02, 11.56s/it]\n",
      "64%|██████▍   | 676/1054 [2:10:17<1:12:50, 11.56s/it]\n",
      "64%|██████▍   | 677/1054 [2:10:28<1:12:39, 11.56s/it]\n",
      "64%|██████▍   | 678/1054 [2:10:40<1:12:28, 11.56s/it]\n",
      "64%|██████▍   | 679/1054 [2:10:52<1:12:16, 11.56s/it]\n",
      "65%|██████▍   | 680/1054 [2:11:03<1:12:04, 11.56s/it]\n",
      "{'loss': 1.3716, 'learning_rate': 7.096774193548388e-05, 'epoch': 1.29}\n",
      "65%|██████▍   | 680/1054 [2:11:03<1:12:04, 11.56s/it]\n",
      "65%|██████▍   | 681/1054 [2:11:15<1:11:53, 11.56s/it]\n",
      "65%|██████▍   | 682/1054 [2:11:26<1:11:41, 11.56s/it]\n",
      "65%|██████▍   | 683/1054 [2:11:38<1:11:29, 11.56s/it]\n",
      "65%|██████▍   | 684/1054 [2:11:49<1:11:18, 11.56s/it]\n",
      "65%|██████▍   | 685/1054 [2:12:01<1:11:06, 11.56s/it]\n",
      "65%|██████▌   | 686/1054 [2:12:13<1:10:55, 11.56s/it]\n",
      "65%|██████▌   | 687/1054 [2:12:24<1:10:43, 11.56s/it]\n",
      "65%|██████▌   | 688/1054 [2:12:36<1:10:32, 11.56s/it]\n",
      "65%|██████▌   | 689/1054 [2:12:47<1:10:20, 11.56s/it]\n",
      "65%|██████▌   | 690/1054 [2:12:59<1:10:09, 11.56s/it]\n",
      "{'loss': 1.3118, 'learning_rate': 6.907020872865276e-05, 'epoch': 1.31}\n",
      "65%|██████▌   | 690/1054 [2:12:59<1:10:09, 11.56s/it]\n",
      "66%|██████▌   | 691/1054 [2:13:10<1:09:57, 11.56s/it]\n",
      "66%|██████▌   | 692/1054 [2:13:22<1:09:46, 11.56s/it]\n",
      "66%|██████▌   | 693/1054 [2:13:33<1:09:34, 11.56s/it]\n",
      "66%|██████▌   | 694/1054 [2:13:45<1:09:22, 11.56s/it]\n",
      "66%|██████▌   | 695/1054 [2:13:57<1:09:11, 11.56s/it]\n",
      "66%|██████▌   | 696/1054 [2:14:08<1:08:59, 11.56s/it]\n",
      "66%|██████▌   | 697/1054 [2:14:20<1:08:48, 11.56s/it]\n",
      "66%|██████▌   | 698/1054 [2:14:31<1:08:36, 11.56s/it]\n",
      "66%|██████▋   | 699/1054 [2:14:43<1:08:24, 11.56s/it]\n",
      "66%|██████▋   | 700/1054 [2:14:54<1:08:13, 11.56s/it]\n",
      "{'loss': 1.2465, 'learning_rate': 6.717267552182164e-05, 'epoch': 1.33}\n",
      "66%|██████▋   | 700/1054 [2:14:54<1:08:13, 11.56s/it]\n",
      "67%|██████▋   | 701/1054 [2:15:06<1:08:01, 11.56s/it]\n",
      "67%|██████▋   | 702/1054 [2:15:18<1:07:50, 11.56s/it]\n",
      "67%|██████▋   | 703/1054 [2:15:29<1:07:38, 11.56s/it]\n",
      "67%|██████▋   | 704/1054 [2:15:41<1:07:27, 11.56s/it]\n",
      "67%|██████▋   | 705/1054 [2:15:52<1:07:15, 11.56s/it]\n",
      "67%|██████▋   | 706/1054 [2:16:04<1:07:03, 11.56s/it]\n",
      "67%|██████▋   | 707/1054 [2:16:15<1:06:52, 11.56s/it]\n",
      "67%|██████▋   | 708/1054 [2:16:27<1:06:40, 11.56s/it]\n",
      "67%|██████▋   | 709/1054 [2:16:39<1:06:29, 11.56s/it]\n",
      "67%|██████▋   | 710/1054 [2:16:50<1:06:17, 11.56s/it]\n",
      "{'loss': 1.3278, 'learning_rate': 6.527514231499051e-05, 'epoch': 1.35}\n",
      "67%|██████▋   | 710/1054 [2:16:50<1:06:17, 11.56s/it]\n",
      "67%|██████▋   | 711/1054 [2:17:02<1:06:06, 11.56s/it]\n",
      "68%|██████▊   | 712/1054 [2:17:13<1:05:54, 11.56s/it]\n",
      "68%|██████▊   | 713/1054 [2:17:25<1:05:43, 11.56s/it]\n",
      "68%|██████▊   | 714/1054 [2:17:36<1:05:31, 11.56s/it]\n",
      "68%|██████▊   | 715/1054 [2:17:48<1:05:19, 11.56s/it]\n",
      "68%|██████▊   | 716/1054 [2:17:59<1:05:08, 11.56s/it]\n",
      "68%|██████▊   | 717/1054 [2:18:11<1:04:56, 11.56s/it]\n",
      "68%|██████▊   | 718/1054 [2:18:23<1:04:45, 11.56s/it]\n",
      "68%|██████▊   | 719/1054 [2:18:34<1:04:33, 11.56s/it]\n",
      "68%|██████▊   | 720/1054 [2:18:46<1:04:22, 11.56s/it]\n",
      "{'loss': 1.2868, 'learning_rate': 6.337760910815939e-05, 'epoch': 1.37}\n",
      "68%|██████▊   | 720/1054 [2:18:46<1:04:22, 11.56s/it]\n",
      "68%|██████▊   | 721/1054 [2:18:57<1:04:10, 11.56s/it]\n",
      "69%|██████▊   | 722/1054 [2:19:09<1:03:58, 11.56s/it]\n",
      "69%|██████▊   | 723/1054 [2:19:20<1:03:47, 11.56s/it]\n",
      "69%|██████▊   | 724/1054 [2:19:32<1:03:35, 11.56s/it]\n",
      "69%|██████▉   | 725/1054 [2:19:44<1:03:24, 11.56s/it]\n",
      "69%|██████▉   | 726/1054 [2:19:55<1:03:12, 11.56s/it]\n",
      "69%|██████▉   | 727/1054 [2:20:07<1:03:01, 11.56s/it]\n",
      "69%|██████▉   | 728/1054 [2:20:18<1:02:49, 11.56s/it]\n",
      "69%|██████▉   | 729/1054 [2:20:30<1:02:38, 11.56s/it]\n",
      "69%|██████▉   | 730/1054 [2:20:41<1:02:26, 11.56s/it]\n",
      "{'loss': 1.3472, 'learning_rate': 6.148007590132827e-05, 'epoch': 1.39}\n",
      "69%|██████▉   | 730/1054 [2:20:41<1:02:26, 11.56s/it]\n",
      "69%|██████▉   | 731/1054 [2:20:53<1:02:15, 11.56s/it]\n",
      "69%|██████▉   | 732/1054 [2:21:04<1:02:03, 11.56s/it]\n",
      "70%|██████▉   | 733/1054 [2:21:16<1:01:51, 11.56s/it]\n",
      "70%|██████▉   | 734/1054 [2:21:28<1:01:40, 11.56s/it]\n",
      "70%|██████▉   | 735/1054 [2:21:39<1:01:28, 11.56s/it]\n",
      "70%|██████▉   | 736/1054 [2:21:51<1:01:17, 11.56s/it]\n",
      "70%|██████▉   | 737/1054 [2:22:02<1:01:05, 11.56s/it]\n",
      "70%|███████   | 738/1054 [2:22:14<1:00:53, 11.56s/it]\n",
      "70%|███████   | 739/1054 [2:22:25<1:00:42, 11.56s/it]\n",
      "70%|███████   | 740/1054 [2:22:37<1:00:30, 11.56s/it]\n",
      "{'loss': 1.2177, 'learning_rate': 5.9582542694497156e-05, 'epoch': 1.4}\n",
      "70%|███████   | 740/1054 [2:22:37<1:00:30, 11.56s/it]\n",
      "70%|███████   | 741/1054 [2:22:49<1:00:19, 11.56s/it]\n",
      "70%|███████   | 742/1054 [2:23:00<1:00:07, 11.56s/it]\n",
      "70%|███████   | 743/1054 [2:23:12<59:56, 11.56s/it]\n",
      "71%|███████   | 744/1054 [2:23:23<59:44, 11.56s/it]\n",
      "71%|███████   | 745/1054 [2:23:35<59:33, 11.56s/it]\n",
      "71%|███████   | 746/1054 [2:23:46<59:21, 11.56s/it]\n",
      "71%|███████   | 747/1054 [2:23:58<59:09, 11.56s/it]\n",
      "71%|███████   | 748/1054 [2:24:09<58:58, 11.56s/it]\n",
      "71%|███████   | 749/1054 [2:24:21<58:46, 11.56s/it]\n",
      "71%|███████   | 750/1054 [2:24:33<58:35, 11.56s/it]\n",
      "{'loss': 1.2992, 'learning_rate': 5.7685009487666045e-05, 'epoch': 1.42}\n",
      "71%|███████   | 750/1054 [2:24:33<58:35, 11.56s/it]\n",
      "71%|███████▏  | 751/1054 [2:24:44<58:23, 11.56s/it]\n",
      "71%|███████▏  | 752/1054 [2:24:56<58:12, 11.56s/it]\n",
      "71%|███████▏  | 753/1054 [2:25:07<58:00, 11.56s/it]\n",
      "72%|███████▏  | 754/1054 [2:25:19<57:48, 11.56s/it]\n",
      "72%|███████▏  | 755/1054 [2:25:30<57:37, 11.56s/it]\n",
      "72%|███████▏  | 756/1054 [2:25:42<57:25, 11.56s/it]\n",
      "72%|███████▏  | 757/1054 [2:25:54<57:14, 11.56s/it]\n",
      "72%|███████▏  | 758/1054 [2:26:05<57:02, 11.56s/it]\n",
      "72%|███████▏  | 759/1054 [2:26:17<56:51, 11.56s/it]\n",
      "72%|███████▏  | 760/1054 [2:26:28<56:39, 11.56s/it]\n",
      "{'loss': 1.342, 'learning_rate': 5.5787476280834914e-05, 'epoch': 1.44}\n",
      "72%|███████▏  | 760/1054 [2:26:28<56:39, 11.56s/it]\n",
      "72%|███████▏  | 761/1054 [2:26:40<56:28, 11.56s/it]\n",
      "72%|███████▏  | 762/1054 [2:26:51<56:16, 11.56s/it]\n",
      "72%|███████▏  | 763/1054 [2:27:03<56:04, 11.56s/it]\n",
      "72%|███████▏  | 764/1054 [2:27:14<55:53, 11.56s/it]\n",
      "73%|███████▎  | 765/1054 [2:27:26<55:41, 11.56s/it]\n",
      "73%|███████▎  | 766/1054 [2:27:38<55:30, 11.56s/it]\n",
      "73%|███████▎  | 767/1054 [2:27:49<55:18, 11.56s/it]\n",
      "73%|███████▎  | 768/1054 [2:28:01<55:07, 11.56s/it]\n",
      "73%|███████▎  | 769/1054 [2:28:12<54:55, 11.56s/it]\n",
      "73%|███████▎  | 770/1054 [2:28:24<54:43, 11.56s/it]\n",
      "{'loss': 1.2955, 'learning_rate': 5.38899430740038e-05, 'epoch': 1.46}\n",
      "73%|███████▎  | 770/1054 [2:28:24<54:43, 11.56s/it]\n",
      "73%|███████▎  | 771/1054 [2:28:35<54:32, 11.56s/it]\n",
      "73%|███████▎  | 772/1054 [2:28:47<54:20, 11.56s/it]\n",
      "73%|███████▎  | 773/1054 [2:28:59<54:09, 11.56s/it]\n",
      "73%|███████▎  | 774/1054 [2:29:10<53:57, 11.56s/it]\n",
      "74%|███████▎  | 775/1054 [2:29:22<53:46, 11.56s/it]\n",
      "74%|███████▎  | 776/1054 [2:29:33<53:34, 11.56s/it]\n",
      "74%|███████▎  | 777/1054 [2:29:45<53:22, 11.56s/it]\n",
      "74%|███████▍  | 778/1054 [2:29:56<53:11, 11.56s/it]\n",
      "74%|███████▍  | 779/1054 [2:30:08<53:00, 11.56s/it]\n",
      "74%|███████▍  | 780/1054 [2:30:20<52:48, 11.56s/it]\n",
      "{'loss': 1.2563, 'learning_rate': 5.199240986717268e-05, 'epoch': 1.48}\n",
      "74%|███████▍  | 780/1054 [2:30:20<52:48, 11.56s/it]\n",
      "74%|███████▍  | 781/1054 [2:30:31<52:36, 11.56s/it]\n",
      "74%|███████▍  | 782/1054 [2:30:43<52:25, 11.56s/it]\n",
      "74%|███████▍  | 783/1054 [2:30:54<52:13, 11.56s/it]\n",
      "74%|███████▍  | 784/1054 [2:31:06<52:01, 11.56s/it]\n",
      "74%|███████▍  | 785/1054 [2:31:17<51:50, 11.56s/it]\n",
      "75%|███████▍  | 786/1054 [2:31:29<51:38, 11.56s/it]\n",
      "75%|███████▍  | 787/1054 [2:31:40<51:27, 11.56s/it]\n",
      "75%|███████▍  | 788/1054 [2:31:52<51:15, 11.56s/it]\n",
      "75%|███████▍  | 789/1054 [2:32:04<51:04, 11.56s/it]\n",
      "75%|███████▍  | 790/1054 [2:32:15<50:52, 11.56s/it]\n",
      "{'loss': 1.3268, 'learning_rate': 5.009487666034156e-05, 'epoch': 1.5}\n",
      "75%|███████▍  | 790/1054 [2:32:15<50:52, 11.56s/it]\n",
      "75%|███████▌  | 791/1054 [2:32:27<50:41, 11.56s/it]\n",
      "75%|███████▌  | 792/1054 [2:32:38<50:29, 11.56s/it]\n",
      "75%|███████▌  | 793/1054 [2:32:50<50:17, 11.56s/it]\n",
      "75%|███████▌  | 794/1054 [2:33:01<50:06, 11.56s/it]\n",
      "75%|███████▌  | 795/1054 [2:33:13<49:54, 11.56s/it]\n",
      "76%|███████▌  | 796/1054 [2:33:25<49:43, 11.56s/it]\n",
      "76%|███████▌  | 797/1054 [2:33:36<49:31, 11.56s/it]\n",
      "76%|███████▌  | 798/1054 [2:33:48<49:20, 11.56s/it]\n",
      "76%|███████▌  | 799/1054 [2:33:59<49:08, 11.56s/it]\n",
      "76%|███████▌  | 800/1054 [2:34:11<48:57, 11.56s/it]\n",
      "{'loss': 1.3122, 'learning_rate': 4.819734345351044e-05, 'epoch': 1.52}\n",
      "76%|███████▌  | 800/1054 [2:34:11<48:57, 11.56s/it]\n",
      "76%|███████▌  | 801/1054 [2:34:22<48:45, 11.56s/it]\n",
      "76%|███████▌  | 802/1054 [2:34:34<48:33, 11.56s/it]\n",
      "76%|███████▌  | 803/1054 [2:34:45<48:22, 11.56s/it]\n",
      "76%|███████▋  | 804/1054 [2:34:57<48:10, 11.56s/it]\n",
      "76%|███████▋  | 805/1054 [2:35:09<47:59, 11.56s/it]\n",
      "76%|███████▋  | 806/1054 [2:35:20<47:47, 11.56s/it]\n",
      "77%|███████▋  | 807/1054 [2:35:32<47:36, 11.56s/it]\n",
      "77%|███████▋  | 808/1054 [2:35:43<47:24, 11.56s/it]\n",
      "77%|███████▋  | 809/1054 [2:35:55<47:13, 11.56s/it]\n",
      "77%|███████▋  | 810/1054 [2:36:06<47:01, 11.56s/it]\n",
      "{'loss': 1.2723, 'learning_rate': 4.629981024667932e-05, 'epoch': 1.54}\n",
      "77%|███████▋  | 810/1054 [2:36:06<47:01, 11.56s/it]\n",
      "77%|███████▋  | 811/1054 [2:36:18<46:49, 11.56s/it]\n",
      "77%|███████▋  | 812/1054 [2:36:30<46:38, 11.56s/it]\n",
      "77%|███████▋  | 813/1054 [2:36:41<46:26, 11.56s/it]\n",
      "77%|███████▋  | 814/1054 [2:36:53<46:15, 11.56s/it]\n",
      "77%|███████▋  | 815/1054 [2:37:04<46:03, 11.56s/it]\n",
      "77%|███████▋  | 816/1054 [2:37:16<45:52, 11.56s/it]\n",
      "78%|███████▊  | 817/1054 [2:37:27<45:40, 11.56s/it]\n",
      "78%|███████▊  | 818/1054 [2:37:39<45:28, 11.56s/it]\n",
      "78%|███████▊  | 819/1054 [2:37:50<45:17, 11.56s/it]\n",
      "78%|███████▊  | 820/1054 [2:38:02<45:05, 11.56s/it]\n",
      "{'loss': 1.3194, 'learning_rate': 4.44022770398482e-05, 'epoch': 1.56}\n",
      "78%|███████▊  | 820/1054 [2:38:02<45:05, 11.56s/it]\n",
      "78%|███████▊  | 821/1054 [2:38:14<44:54, 11.56s/it]\n",
      "78%|███████▊  | 822/1054 [2:38:25<44:42, 11.56s/it]\n",
      "78%|███████▊  | 823/1054 [2:38:37<44:31, 11.56s/it]\n",
      "78%|███████▊  | 824/1054 [2:38:48<44:19, 11.56s/it]\n",
      "78%|███████▊  | 825/1054 [2:39:00<44:08, 11.56s/it]\n",
      "78%|███████▊  | 826/1054 [2:39:11<43:56, 11.56s/it]\n",
      "78%|███████▊  | 827/1054 [2:39:23<43:44, 11.56s/it]\n",
      "79%|███████▊  | 828/1054 [2:39:35<43:33, 11.56s/it]\n",
      "79%|███████▊  | 829/1054 [2:39:46<43:21, 11.56s/it]\n",
      "79%|███████▊  | 830/1054 [2:39:58<43:10, 11.56s/it]\n",
      "{'loss': 1.3545, 'learning_rate': 4.250474383301708e-05, 'epoch': 1.57}\n",
      "79%|███████▊  | 830/1054 [2:39:58<43:10, 11.56s/it]\n",
      "79%|███████▉  | 831/1054 [2:40:09<42:58, 11.56s/it]\n",
      "79%|███████▉  | 832/1054 [2:40:21<42:47, 11.56s/it]\n",
      "79%|███████▉  | 833/1054 [2:40:32<42:35, 11.56s/it]\n",
      "79%|███████▉  | 834/1054 [2:40:44<42:24, 11.56s/it]\n",
      "79%|███████▉  | 835/1054 [2:40:55<42:12, 11.56s/it]\n",
      "79%|███████▉  | 836/1054 [2:41:07<42:00, 11.56s/it]\n",
      "79%|███████▉  | 837/1054 [2:41:19<41:49, 11.56s/it]\n",
      "80%|███████▉  | 838/1054 [2:41:30<41:37, 11.56s/it]\n",
      "80%|███████▉  | 839/1054 [2:41:42<41:26, 11.56s/it]\n",
      "80%|███████▉  | 840/1054 [2:41:53<41:14, 11.56s/it]\n",
      "{'loss': 1.3182, 'learning_rate': 4.060721062618596e-05, 'epoch': 1.59}\n",
      "80%|███████▉  | 840/1054 [2:41:53<41:14, 11.56s/it]\n",
      "80%|███████▉  | 841/1054 [2:42:05<41:03, 11.56s/it]\n",
      "80%|███████▉  | 842/1054 [2:42:16<40:51, 11.56s/it]\n",
      "80%|███████▉  | 843/1054 [2:42:28<40:39, 11.56s/it]\n",
      "80%|████████  | 844/1054 [2:42:40<40:28, 11.56s/it]\n",
      "80%|████████  | 845/1054 [2:42:51<40:16, 11.56s/it]\n",
      "80%|████████  | 846/1054 [2:43:03<40:05, 11.56s/it]\n",
      "80%|████████  | 847/1054 [2:43:14<39:53, 11.56s/it]\n",
      "80%|████████  | 848/1054 [2:43:26<39:42, 11.56s/it]\n",
      "81%|████████  | 849/1054 [2:43:37<39:30, 11.56s/it]\n",
      "81%|████████  | 850/1054 [2:43:49<39:19, 11.56s/it]\n",
      "{'loss': 1.2659, 'learning_rate': 3.870967741935484e-05, 'epoch': 1.61}\n",
      "81%|████████  | 850/1054 [2:43:49<39:19, 11.56s/it]\n",
      "81%|████████  | 851/1054 [2:44:01<39:07, 11.56s/it]\n",
      "81%|████████  | 852/1054 [2:44:12<38:55, 11.56s/it]\n",
      "81%|████████  | 853/1054 [2:44:24<38:44, 11.56s/it]\n",
      "81%|████████  | 854/1054 [2:44:35<38:32, 11.56s/it]\n",
      "81%|████████  | 855/1054 [2:44:47<38:21, 11.56s/it]\n",
      "81%|████████  | 856/1054 [2:44:58<38:09, 11.56s/it]\n",
      "81%|████████▏ | 857/1054 [2:45:10<37:57, 11.56s/it]\n",
      "81%|████████▏ | 858/1054 [2:45:21<37:46, 11.56s/it]\n",
      "81%|████████▏ | 859/1054 [2:45:33<37:34, 11.56s/it]\n",
      "82%|████████▏ | 860/1054 [2:45:45<37:23, 11.56s/it]\n",
      "{'loss': 1.3135, 'learning_rate': 3.681214421252372e-05, 'epoch': 1.63}\n",
      "82%|████████▏ | 860/1054 [2:45:45<37:23, 11.56s/it]\n",
      "82%|████████▏ | 861/1054 [2:45:56<37:11, 11.56s/it]\n",
      "82%|████████▏ | 862/1054 [2:46:08<37:00, 11.56s/it]\n",
      "82%|████████▏ | 863/1054 [2:46:19<36:48, 11.56s/it]\n",
      "82%|████████▏ | 864/1054 [2:46:31<36:37, 11.56s/it]\n",
      "82%|████████▏ | 865/1054 [2:46:42<36:25, 11.56s/it]\n",
      "82%|████████▏ | 866/1054 [2:46:54<36:13, 11.56s/it]\n",
      "82%|████████▏ | 867/1054 [2:47:06<36:02, 11.56s/it]\n",
      "82%|████████▏ | 868/1054 [2:47:17<35:50, 11.56s/it]\n",
      "82%|████████▏ | 869/1054 [2:47:29<35:39, 11.56s/it]\n",
      "83%|████████▎ | 870/1054 [2:47:40<35:27, 11.56s/it]\n",
      "{'loss': 1.2577, 'learning_rate': 3.49146110056926e-05, 'epoch': 1.65}\n",
      "83%|████████▎ | 870/1054 [2:47:40<35:27, 11.56s/it]\n",
      "83%|████████▎ | 871/1054 [2:47:52<35:16, 11.56s/it]\n",
      "83%|████████▎ | 872/1054 [2:48:03<35:04, 11.56s/it]\n",
      "83%|████████▎ | 873/1054 [2:48:15<34:53, 11.56s/it]\n",
      "83%|████████▎ | 874/1054 [2:48:26<34:41, 11.56s/it]\n",
      "83%|████████▎ | 875/1054 [2:48:38<34:29, 11.56s/it]\n",
      "83%|████████▎ | 876/1054 [2:48:50<34:18, 11.56s/it]\n",
      "83%|████████▎ | 877/1054 [2:49:01<34:06, 11.56s/it]\n",
      "83%|████████▎ | 878/1054 [2:49:13<33:55, 11.56s/it]\n",
      "83%|████████▎ | 879/1054 [2:49:24<33:43, 11.56s/it]\n",
      "83%|████████▎ | 880/1054 [2:49:36<33:32, 11.56s/it]\n",
      "{'loss': 1.3685, 'learning_rate': 3.301707779886148e-05, 'epoch': 1.67}\n",
      "83%|████████▎ | 880/1054 [2:49:36<33:32, 11.56s/it]\n",
      "84%|████████▎ | 881/1054 [2:49:47<33:20, 11.56s/it]\n",
      "84%|████████▎ | 882/1054 [2:49:59<33:09, 11.56s/it]\n",
      "84%|████████▍ | 883/1054 [2:50:11<32:57, 11.56s/it]\n",
      "84%|████████▍ | 884/1054 [2:50:22<32:45, 11.56s/it]\n",
      "84%|████████▍ | 885/1054 [2:50:34<32:34, 11.56s/it]\n",
      "84%|████████▍ | 886/1054 [2:50:45<32:22, 11.56s/it]\n",
      "84%|████████▍ | 887/1054 [2:50:57<32:11, 11.56s/it]\n",
      "84%|████████▍ | 888/1054 [2:51:08<31:59, 11.56s/it]\n",
      "84%|████████▍ | 889/1054 [2:51:20<31:48, 11.56s/it]\n",
      "84%|████████▍ | 890/1054 [2:51:31<31:36, 11.56s/it]\n",
      "{'loss': 1.2111, 'learning_rate': 3.1119544592030364e-05, 'epoch': 1.69}\n",
      "84%|████████▍ | 890/1054 [2:51:31<31:36, 11.56s/it]\n",
      "85%|████████▍ | 891/1054 [2:51:43<31:24, 11.56s/it]\n",
      "85%|████████▍ | 892/1054 [2:51:55<31:13, 11.56s/it]\n",
      "85%|████████▍ | 893/1054 [2:52:06<31:01, 11.56s/it]\n",
      "85%|████████▍ | 894/1054 [2:52:18<30:50, 11.56s/it]\n",
      "85%|████████▍ | 895/1054 [2:52:29<30:38, 11.56s/it]\n",
      "85%|████████▌ | 896/1054 [2:52:41<30:27, 11.56s/it]\n",
      "85%|████████▌ | 897/1054 [2:52:52<30:15, 11.56s/it]\n",
      "85%|████████▌ | 898/1054 [2:53:04<30:03, 11.56s/it]\n",
      "85%|████████▌ | 899/1054 [2:53:16<29:52, 11.56s/it]\n",
      "85%|████████▌ | 900/1054 [2:53:27<29:40, 11.56s/it]\n",
      "{'loss': 1.2778, 'learning_rate': 2.9222011385199243e-05, 'epoch': 1.71}\n",
      "85%|████████▌ | 900/1054 [2:53:27<29:40, 11.56s/it]\n",
      "85%|████████▌ | 901/1054 [2:53:39<29:29, 11.56s/it]\n",
      "86%|████████▌ | 902/1054 [2:53:50<29:17, 11.56s/it]\n",
      "86%|████████▌ | 903/1054 [2:54:02<29:06, 11.56s/it]\n",
      "86%|████████▌ | 904/1054 [2:54:13<28:54, 11.56s/it]\n",
      "86%|████████▌ | 905/1054 [2:54:25<28:43, 11.56s/it]\n",
      "86%|████████▌ | 906/1054 [2:54:37<28:31, 11.56s/it]\n",
      "86%|████████▌ | 907/1054 [2:54:48<28:19, 11.56s/it]\n",
      "86%|████████▌ | 908/1054 [2:55:00<28:08, 11.56s/it]\n",
      "86%|████████▌ | 909/1054 [2:55:11<27:56, 11.56s/it]\n",
      "86%|████████▋ | 910/1054 [2:55:23<27:45, 11.56s/it]\n",
      "{'loss': 1.2405, 'learning_rate': 2.7324478178368122e-05, 'epoch': 1.73}\n",
      "86%|████████▋ | 910/1054 [2:55:23<27:45, 11.56s/it]\n",
      "86%|████████▋ | 911/1054 [2:55:34<27:33, 11.56s/it]\n",
      "87%|████████▋ | 912/1054 [2:55:46<27:22, 11.56s/it]\n",
      "87%|████████▋ | 913/1054 [2:55:57<27:10, 11.56s/it]\n",
      "87%|████████▋ | 914/1054 [2:56:09<26:58, 11.56s/it]\n",
      "87%|████████▋ | 915/1054 [2:56:21<26:47, 11.56s/it]\n",
      "87%|████████▋ | 916/1054 [2:56:32<26:35, 11.56s/it]\n",
      "87%|████████▋ | 917/1054 [2:56:44<26:24, 11.56s/it]\n",
      "87%|████████▋ | 918/1054 [2:56:55<26:12, 11.56s/it]\n",
      "87%|████████▋ | 919/1054 [2:57:07<26:01, 11.56s/it]\n",
      "87%|████████▋ | 920/1054 [2:57:18<25:49, 11.56s/it]\n",
      "{'loss': 1.2643, 'learning_rate': 2.5426944971537005e-05, 'epoch': 1.75}\n",
      "87%|████████▋ | 920/1054 [2:57:18<25:49, 11.56s/it]\n",
      "87%|████████▋ | 921/1054 [2:57:30<25:37, 11.56s/it]\n",
      "87%|████████▋ | 922/1054 [2:57:42<25:26, 11.56s/it]\n",
      "88%|████████▊ | 923/1054 [2:57:53<25:14, 11.56s/it]\n",
      "88%|████████▊ | 924/1054 [2:58:05<25:03, 11.56s/it]\n",
      "88%|████████▊ | 925/1054 [2:58:16<24:51, 11.56s/it]\n",
      "88%|████████▊ | 926/1054 [2:58:28<24:40, 11.56s/it]\n",
      "88%|████████▊ | 927/1054 [2:58:39<24:28, 11.56s/it]\n",
      "88%|████████▊ | 928/1054 [2:58:51<24:17, 11.56s/it]\n",
      "88%|████████▊ | 929/1054 [2:59:02<24:05, 11.56s/it]\n",
      "88%|████████▊ | 930/1054 [2:59:14<23:53, 11.56s/it]\n",
      "{'loss': 1.3203, 'learning_rate': 2.3529411764705884e-05, 'epoch': 1.76}\n",
      "88%|████████▊ | 930/1054 [2:59:14<23:53, 11.56s/it]\n",
      "88%|████████▊ | 931/1054 [2:59:26<23:42, 11.56s/it]\n",
      "88%|████████▊ | 932/1054 [2:59:37<23:30, 11.56s/it]\n",
      "89%|████████▊ | 933/1054 [2:59:49<23:19, 11.56s/it]\n",
      "89%|████████▊ | 934/1054 [3:00:00<23:07, 11.56s/it]\n",
      "89%|████████▊ | 935/1054 [3:00:12<22:56, 11.56s/it]\n",
      "89%|████████▉ | 936/1054 [3:00:23<22:44, 11.56s/it]\n",
      "89%|████████▉ | 937/1054 [3:00:35<22:32, 11.56s/it]\n",
      "89%|████████▉ | 938/1054 [3:00:47<22:21, 11.56s/it]\n",
      "89%|████████▉ | 939/1054 [3:00:58<22:09, 11.56s/it]\n",
      "89%|████████▉ | 940/1054 [3:01:10<21:58, 11.56s/it]\n",
      "{'loss': 1.2667, 'learning_rate': 2.1631878557874763e-05, 'epoch': 1.78}\n",
      "89%|████████▉ | 940/1054 [3:01:10<21:58, 11.56s/it]\n",
      "89%|████████▉ | 941/1054 [3:01:21<21:46, 11.56s/it]\n",
      "89%|████████▉ | 942/1054 [3:01:33<21:35, 11.56s/it]\n",
      "89%|████████▉ | 943/1054 [3:01:44<21:23, 11.56s/it]\n",
      "90%|████████▉ | 944/1054 [3:01:56<21:12, 11.56s/it]\n",
      "90%|████████▉ | 945/1054 [3:02:08<21:00, 11.56s/it]\n",
      "90%|████████▉ | 946/1054 [3:02:19<20:48, 11.56s/it]\n",
      "90%|████████▉ | 947/1054 [3:02:31<20:37, 11.56s/it]\n",
      "90%|████████▉ | 948/1054 [3:02:42<20:25, 11.56s/it]\n",
      "90%|█████████ | 949/1054 [3:02:54<20:14, 11.56s/it]\n",
      "90%|█████████ | 950/1054 [3:03:05<20:02, 11.56s/it]\n",
      "{'loss': 1.2781, 'learning_rate': 1.9734345351043645e-05, 'epoch': 1.8}\n",
      "90%|█████████ | 950/1054 [3:03:05<20:02, 11.56s/it]\n",
      "90%|█████████ | 951/1054 [3:03:17<19:51, 11.56s/it]\n",
      "90%|█████████ | 952/1054 [3:03:28<19:39, 11.56s/it]\n",
      "90%|█████████ | 953/1054 [3:03:40<19:27, 11.56s/it]\n",
      "91%|█████████ | 954/1054 [3:03:52<19:16, 11.56s/it]\n",
      "91%|█████████ | 955/1054 [3:04:03<19:04, 11.56s/it]\n",
      "91%|█████████ | 956/1054 [3:04:15<18:53, 11.56s/it]\n",
      "91%|█████████ | 957/1054 [3:04:26<18:41, 11.56s/it]\n",
      "91%|█████████ | 958/1054 [3:04:38<18:30, 11.56s/it]\n",
      "91%|█████████ | 959/1054 [3:04:49<18:18, 11.56s/it]\n",
      "91%|█████████ | 960/1054 [3:05:01<18:07, 11.56s/it]\n",
      "{'loss': 1.2076, 'learning_rate': 1.7836812144212524e-05, 'epoch': 1.82}\n",
      "91%|█████████ | 960/1054 [3:05:01<18:07, 11.56s/it]\n",
      "91%|█████████ | 961/1054 [3:05:13<17:55, 11.56s/it]\n",
      "91%|█████████▏| 962/1054 [3:05:24<17:43, 11.56s/it]\n",
      "91%|█████████▏| 963/1054 [3:05:36<17:32, 11.56s/it]\n",
      "91%|█████████▏| 964/1054 [3:05:47<17:20, 11.56s/it]\n",
      "92%|█████████▏| 965/1054 [3:05:59<17:09, 11.56s/it]\n",
      "92%|█████████▏| 966/1054 [3:06:10<16:57, 11.56s/it]\n",
      "92%|█████████▏| 967/1054 [3:06:22<16:46, 11.56s/it]\n",
      "92%|█████████▏| 968/1054 [3:06:33<16:34, 11.56s/it]\n",
      "92%|█████████▏| 969/1054 [3:06:45<16:22, 11.56s/it]\n",
      "92%|█████████▏| 970/1054 [3:06:57<16:11, 11.56s/it]\n",
      "{'loss': 1.2958, 'learning_rate': 1.5939278937381406e-05, 'epoch': 1.84}\n",
      "92%|█████████▏| 970/1054 [3:06:57<16:11, 11.56s/it]\n",
      "92%|█████████▏| 971/1054 [3:07:08<15:59, 11.56s/it]\n",
      "92%|█████████▏| 972/1054 [3:07:20<15:48, 11.56s/it]\n",
      "92%|█████████▏| 973/1054 [3:07:31<15:36, 11.56s/it]\n",
      "92%|█████████▏| 974/1054 [3:07:43<15:25, 11.56s/it]\n",
      "93%|█████████▎| 975/1054 [3:07:54<15:13, 11.56s/it]\n",
      "93%|█████████▎| 976/1054 [3:08:06<15:01, 11.56s/it]\n",
      "93%|█████████▎| 977/1054 [3:08:18<14:50, 11.56s/it]\n",
      "93%|█████████▎| 978/1054 [3:08:29<14:38, 11.56s/it]\n",
      "93%|█████████▎| 979/1054 [3:08:41<14:27, 11.56s/it]\n",
      "93%|█████████▎| 980/1054 [3:08:52<14:15, 11.56s/it]\n",
      "{'loss': 1.3517, 'learning_rate': 1.4041745730550285e-05, 'epoch': 1.86}\n",
      "93%|█████████▎| 980/1054 [3:08:52<14:15, 11.56s/it]\n",
      "93%|█████████▎| 981/1054 [3:09:04<14:04, 11.56s/it]\n",
      "93%|█████████▎| 982/1054 [3:09:15<13:52, 11.56s/it]\n",
      "93%|█████████▎| 983/1054 [3:09:27<13:41, 11.56s/it]\n",
      "93%|█████████▎| 984/1054 [3:09:38<13:29, 11.56s/it]\n",
      "93%|█████████▎| 985/1054 [3:09:50<13:17, 11.56s/it]\n",
      "94%|█████████▎| 986/1054 [3:10:02<13:06, 11.56s/it]\n",
      "94%|█████████▎| 987/1054 [3:10:13<12:54, 11.56s/it]\n",
      "94%|█████████▎| 988/1054 [3:10:25<12:43, 11.56s/it]\n",
      "94%|█████████▍| 989/1054 [3:10:36<12:31, 11.56s/it]\n",
      "94%|█████████▍| 990/1054 [3:10:48<12:20, 11.56s/it]\n",
      "{'loss': 1.2981, 'learning_rate': 1.2144212523719166e-05, 'epoch': 1.88}\n",
      "94%|█████████▍| 990/1054 [3:10:48<12:20, 11.56s/it]\n",
      "94%|█████████▍| 991/1054 [3:10:59<12:08, 11.56s/it]\n",
      "94%|█████████▍| 992/1054 [3:11:11<11:56, 11.56s/it]\n",
      "94%|█████████▍| 993/1054 [3:11:23<11:45, 11.56s/it]\n",
      "94%|█████████▍| 994/1054 [3:11:34<11:33, 11.56s/it]\n",
      "94%|█████████▍| 995/1054 [3:11:46<11:22, 11.56s/it]\n",
      "94%|█████████▍| 996/1054 [3:11:57<11:10, 11.56s/it]\n"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    " \n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def some_func(x,y):\n",
    "    return x+y\n",
    "\n",
    "g = partial(some_func, 1)\n",
    "g(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,2,3,4,5]\n",
    "y = [6,7]\n",
    "\n",
    "list(chain(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
